[
    {
        "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
        "summary": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems.",
        "entry_id": "http://arxiv.org/abs/2511.04541v1",
        "pub_date": "2025-11-06",
        "translated_summary": "跨领域建模用户偏好始终是板面推荐（即推荐有序项目序列）研究中的核心挑战。我们探索了大型语言模型如何通过板面对比推理，有效构建用户偏好的世界模型。我们在涵盖三个不同数据集的实验任务中，对多种大语言模型进行了实证研究。结果表明，任务表现与大语言模型所捕捉的偏好函数特性存在关联性，这为改进方向提供了线索，同时彰显了大语言模型作为推荐系统世界模型的潜力。"
    },
    {
        "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables",
        "summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research.",
        "entry_id": "http://arxiv.org/abs/2511.04491v1",
        "pub_date": "2025-11-06",
        "translated_summary": "现有的表格推理基准大多基于小型统一表格测试模型，未能充分体现现实数据的复杂性，导致对大型语言模型推理能力的评估存在局限。真实场景中的表格往往具有长篇幅、异构性和领域专有特征，既包含结构化字段又混合自由文本，需要模型在数千个标记范围内进行多跳推理。为弥补这一空白，我们推出RUST-BENCH基准数据集，涵盖2031个真实世界表格中的7966个问题，涉及两大领域：i) RB-Science（美国国家科学基金会资助记录）和ii) RB-Sports（NBA统计数据）。与先前研究不同，RUST-BENCH从数据规模、异构性、领域专有性和推理复杂性四个维度对LLMs进行联合评估。开源与商业模型的实验表明，当前LLMs在处理异构表格结构和复杂多跳推理时存在明显困难，暴露出架构设计和提示策略的持续缺陷。RUST-BENCH为推进表格推理研究建立了一个具有挑战性的新测试平台。"
    },
    {
        "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs",
        "summary": "Retrieval of information from graph-structured knowledge bases represents a\npromising direction for improving the factuality of LLMs. While various\nsolutions have been proposed, a comparison of methods is difficult due to the\nlack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a framework for generating high-quality synthetic\nKnowledge Graph Question Answering datasets from any Knowledge Graph, providing\nthe full set of ground-truth facts in the KG to reason over each question. We\nshow how, in addition to enabling more informative benchmarking of KG\nretrievers, the data produced with SynthKGQA also allows us to train better\nmodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset\ndesigned to test zero-shot generalization abilities of KG retrievers with\nrespect to unseen graph structures and relation types, and benchmark popular\nsolutions for KG-augmented LLMs on it.",
        "entry_id": "http://arxiv.org/abs/2511.04473v1",
        "pub_date": "2025-11-06",
        "translated_summary": "从图结构知识库中检索信息是提升大语言模型事实准确性的重要方向。虽然已有多种解决方案被提出，但由于缺乏具有图检索真值标注的挑战性问答数据集，不同方法间的比较仍存在困难。我们推出了SynthKGQA框架，该框架能够基于任意知识图谱生成高质量的知识图谱问答合成数据集，并为每个问题提供知识图谱中完整的真值事实链以供推理验证。研究表明，通过SynthKGQA生成的数据不仅能实现更具信息量的知识图谱检索器基准测试，还能训练出更优质的模型。我们将该框架应用于维基数据知识图谱，构建了GTSQA数据集——该数据集专门用于测试知识图谱检索器在面对未见图结构和关系类型时的零样本泛化能力，并在此数据集上对当前主流的知识图谱增强型大语言模型解决方案进行了基准评估。"
    },
    {
        "title": "On the Brittleness of CLIP Text Encoders",
        "summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy.",
        "entry_id": "http://arxiv.org/abs/2511.04247v1",
        "pub_date": "2025-11-06",
        "translated_summary": "近年来，多模态协同嵌入模型（特别是CLIP）通过将图像与文本在共享表征空间中对齐，推动了零样本分类和多媒体信息检索领域的发展。然而，基于对比对齐训练的此类模型对输入微小扰动缺乏稳定性。尤其在处理人工表述的查询时，查询语句的细微变化可能导致最佳匹配结果的排序产生显著差异。本文系统分析了多媒体信息检索场景中多类非语义查询扰动的影响，基于TRECVID特设视频搜索查询和V3C1视频数据集，对多种CLIP变体进行了词汇、句法和语义层面的扰动测试。研究发现：所有模型均对句法和语义扰动最为敏感，而鲁棒性薄弱环节主要集中在标点符号、大小写等表层文本编辑。这一结果表明，在基准准确率之外，鲁棒性应成为评估视觉语言模型的关键维度。"
    },
    {
        "title": "Denoised Recommendation Model with Collaborative Signal Decoupling",
        "summary": "Although the collaborative filtering (CF) algorithm has achieved remarkable\nperformance in recommendation systems, it suffers from suboptimal\nrecommendation performance due to noise in the user-item interaction matrix.\nNumerous noise-removal studies have improved recommendation models, but most\nexisting approaches conduct denoising on a single graph. This may cause\nattenuation of collaborative signals: removing edges between two nodes can\ninterrupt paths between other nodes, weakening path-dependent collaborative\ninformation. To address these limitations, this study proposes a novel\nGNN-based CF model called DRCSD for denoising unstable interactions. DRCSD\nincludes two core modules: a collaborative signal decoupling module (decomposes\nsignals into distinct orders by structural characteristics) and an order-wise\ndenoising module (performs targeted denoising on each order). Additionally, the\ninformation aggregation mechanism of traditional GNN-based CF models is\nmodified to avoid cross-order signal interference until the final pooling\noperation. Extensive experiments on three public real-world datasets show that\nDRCSD has superior robustness against unstable interactions and achieves\nstatistically significant performance improvements in recommendation accuracy\nmetrics compared to state-of-the-art baseline models.",
        "entry_id": "http://arxiv.org/abs/2511.04237v1",
        "pub_date": "2025-11-06",
        "translated_summary": "尽管协同过滤算法在推荐系统中取得了显著成效，但由于用户-物品交互矩阵中的噪声干扰，其推荐性能仍存在优化空间。现有大量去噪研究通过提升推荐模型性能，但多数方法仅在单一图结构上进行去噪操作，这可能导致协同信号衰减：移除两个节点间的边会中断其他节点间的路径，从而削弱依赖路径的协同信息。针对这些局限性，本研究提出名为DRCSD的新型图神经网络协同过滤模型，专门用于处理不稳定交互的噪声问题。该模型包含两个核心模块：协同信号解耦模块（根据结构特征将信号分解为不同阶数）和分阶去噪模块（对各阶信号进行针对性去噪）。此外，本研究改进了传统基于图神经网络的协同过滤模型的信息聚合机制，避免跨阶信号干扰直至最终池化操作。在三个真实公开数据集上的大量实验表明，DRCSD对不稳定交互具有卓越的鲁棒性，且在推荐准确性指标上相比现有最优基线模型实现了统计学意义上的显著提升。"
    },
    {
        "title": "Coordination-Free Lane Partitioning for Convergent ANN Search",
        "summary": "Production vector search systems often fan out each query across parallel\nlanes (threads, replicas, or shards) to meet latency service-level objectives\n(SLOs). In practice, these lanes rediscover the same candidates, so extra\ncompute does not increase coverage. We present a coordination-free lane\npartitioner that turns duplication into complementary work at the same cost and\ndeadline. For each query we (1) build a deterministic candidate pool sized to\nthe total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)\nassign each lane a disjoint slice of positions. Lanes then return different\nresults by construction, with no runtime coordination.\n  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT\nfeature vectors) with Hierarchical Navigable Small World graphs (HNSW)\nrecall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%\nto 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to\n0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted\nfile (IVF) indexes we see smaller but consistent gains (for example, +11% on MS\nMARCO) by de-duplicating list routing. A microbenchmark shows planner overhead\nof ~37 microseconds per query (mean at the main setting) with linear growth in\nthe number of merged candidates.\n  These results yield a simple operational guideline: size the per-query pool\nto the total budget, deterministically partition positions across lanes, and\nturn redundant fan-out into complementary coverage without changing budget or\ndeadline.",
        "entry_id": "http://arxiv.org/abs/2511.04221v1",
        "pub_date": "2025-11-06",
        "translated_summary": "生产环境中的向量检索系统通常会将每个查询并行分发至多个执行通道（线程、副本或分片）以满足延迟服务等级目标。实践中这些通道往往会重复发现相同候选结果，导致额外计算资源无法提升结果覆盖度。我们提出一种无协调机制的通道分区策略，在保持相同成本与截止时间的前提下，将重复计算转化为互补性工作。针对每个查询，我们：（1）构建确定性的候选池，其容量与总top-k预算相匹配；（2）施加每查询专属的伪随机排列；（3）为每个通道分配互不重叠的位置区间。通过这种构造方式，各通道无需运行时协调即可返回差异化结果。\n\n在四通道配置（总候选预算64）的同等成本下，基于分层导航小世界图的SIFT1M数据集（100万SIFT特征向量）实验中，召回率@10从0.249提升至0.999，同时通道重叠率从近100%降至0%。在MS MARCO数据集（880万文本段）的HNSW实验中，命中率@10从0.200提升至0.601，平均倒数排名@10从0.133提升至0.330。对于倒排文件索引，通过列表路由去重实现了较小但稳定的增益（如在MS MARCO上提升11%）。微基准测试显示，规划器每查询开销约37微秒（主要配置下的均值），且随合并候选数线性增长。\n\n这些成果衍生出简明运维指南：将每查询候选池容量设置为总预算规模，通过确定性位置分区分配至各通道，即可在不改变预算与截止时间的前提下，将冗余分发转化为互补性覆盖。"
    },
    {
        "title": "Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance",
        "summary": "University students face immense challenges during their undergraduate lives,\noften being deprived of personalized on-demand guidance that mentors fail to\nprovide at scale. Digital tools exist, but there is a serious lack of\ncustomized coaching for newcomers. This paper presents an AI-powered chatbot\nthat will serve as a mentor for the students of BRAC University. The main\ncomponent is a data ingestion pipeline that efficiently processes and updates\ninformation from diverse sources, such as CSV files and university webpages.\nThe chatbot retrieves information through a hybrid approach, combining BM25\nlexical ranking with ChromaDB semantic retrieval, and uses a Large Language\nModel, LLaMA-3.3-70B, to generate conversational responses. The generated text\nwas found to be semantically highly relevant, with a BERTScore of 0.831 and a\nMETEOR score of 0.809. The data pipeline was also very efficient, taking 106.82\nseconds for updates, compared to 368.62 seconds for new data. This chatbot will\nbe able to help students by responding to their queries, helping them to get a\nbetter understanding of university life, and assisting them to plan better\nroutines for their semester in the open-credit university.",
        "entry_id": "http://arxiv.org/abs/2511.04172v1",
        "pub_date": "2025-11-06",
        "translated_summary": "大学生在本科阶段面临诸多挑战，往往难以获得导师大规模提供的个性化即时指导。虽然现有数字化工具，但针对新生的定制化辅导严重匮乏。本文提出一款人工智能聊天机器人，将为BRAC大学学生提供导师式服务。该系统的核心组件是数据摄取管道，能高效处理并实时更新来自CSV文件和大学网页等多源信息。通过结合BM25词汇排序与ChromaDB语义检索的混合检索机制，并基于LLaMA-3.3-70B大语言模型生成对话响应，该聊天机器人展现出优异的语义相关性——BERTScore达0.831，METEOR评分达0.809。数据管道处理效率显著，更新现有数据仅需106.82秒，而处理新数据也仅需368.62秒。这款智能助手将有效解答学生疑问，帮助其深入理解大学生活，并在开放学分制下规划更合理的学期安排。"
    },
    {
        "title": "E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce",
        "summary": "Finding relevant products given a user query plays a pivotal role in an\ne-commerce platform, as it can spark shopping behaviors and result in revenue\ngains. The challenge lies in accurately predicting the correlation between\nqueries and products. Recently, mining the cross-features between queries and\nproducts based on the commonsense reasoning capacity of Large Language Models\n(LLMs) has shown promising performance. However, such methods suffer from high\ncosts due to intensive real-time LLM inference during serving, as well as human\nannotations and potential Supervised Fine Tuning (SFT). To boost efficiency\nwhile leveraging the commonsense reasoning capacity of LLMs for various\ne-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation\nEnhancer (E-CARE). During inference, models augmented with E-CARE can access\ncommonsense reasoning with only a single LLM forward pass per query by\nutilizing a commonsense reasoning factor graph that encodes most of the\nreasoning schema from powerful LLMs. The experiments on 2 downstream tasks show\nan improvement of up to 12.1% on precision@5.",
        "entry_id": "http://arxiv.org/abs/2511.04087v1",
        "pub_date": "2025-11-06",
        "translated_summary": "在电子商务平台中，根据用户查询推荐相关商品具有关键作用，它能有效激发购物行为并带来收入增长。核心挑战在于如何准确预测查询与商品之间的关联性。近期研究显示，基于大语言模型的常识推理能力挖掘查询与商品间的交叉特征已展现出显著效果。然而，这类方法因需在服务期间进行密集的实时大语言模型推理，同时依赖人工标注及潜在的监督微调，导致成本高昂。为在提升效率的同时充分利用大语言模型的常识推理能力应对各类电商任务，我们提出高效常识增强推荐框架E-CARE。该框架通过构建常识推理因子图，将大部分来自强大语言模型的推理模式进行编码，使得增强后的模型在推理时仅需对每个查询执行单次大语言模型前向传播即可获得常识推理能力。在两项下游任务上的实验表明，该框架使精确率@5指标最高提升12.1%。"
    },
    {
        "title": "Publication Trend in DESIDOC Journal of Library and Information Technology during 2013-2017: A Scientometric Approach",
        "summary": "DESIDOC Journal of Library & Information Technology (DJLIT) formerly known as\nDESIDOC Bulletin of Information Technology is a peer-reviewed, open access,\nbimonthly journal. This paper presents a Scientometric analysis of the DESIDOC\nJournal. The paper analyses the pattern of growth of the research output\npublished in the journal, pattern of authorship, author productivity, and,\nsubjects covered to the papers over the period (2013-2017). It is found that\n227 papers were published during the period of study (2001-2012). The maximum\nnumbers of articles were collaborative in nature. The subject concentration of\nthe journal noted is Scientometrics. The maximum numbers of articles (65%) have\nranged their thought contents between 6 and 10 pages. The study applied\nstandard formula and statistical tools to bring out the factual result.",
        "entry_id": "http://arxiv.org/abs/2511.04082v1",
        "pub_date": "2025-11-06",
        "translated_summary": "《DESIDOC图书馆与信息科技杂志》（DJLIT）前身为《DESIDOC信息科技通报》，是一本经过同行评审的开放获取双月刊。本文对该期刊进行了科学计量学分析，重点研究了2013-2017年间刊载研究成果的产出增长模式、作者合作模式、作者生产力及论文主题分布。研究发现，在观测周期（2001-2012年）内共发表227篇论文，其中大多数论文为合作研究成果。该期刊的核心主题领域为科学计量学，65%的论文篇幅集中在6-10页范围内。本研究通过标准公式与统计工具得出了客观结论。"
    },
    {
        "title": "Caption Injection for Optimization in Generative Search Engine",
        "summary": "Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation\n(RAG) techniques and Large Language Models (LLMs) to integrate multi-source\ninformation and provide users with accurate and comprehensive responses. Unlike\ntraditional search engines that present results in ranked lists, GSEs shift\nusers' attention from sequential browsing to content-driven subjective\nperception, driving a paradigm shift in information retrieval. In this context,\nenhancing the subjective visibility of content through Generative Search Engine\nOptimization (G-SEO) methods has emerged as a new research focus. With the\nrapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)\ntechniques, GSEs can now efficiently integrate text, images, audio, and video,\nproducing richer responses that better satisfy complex information needs.\nExisting G-SEO methods, however, remain limited to text-based optimization and\nfail to fully exploit multimodal data. To address this gap, we propose Caption\nInjection, the first multimodal G-SEO approach, which extracts captions from\nimages and injects them into textual content, integrating visual semantics to\nenhance the subjective visibility of content in generative search scenarios. We\nsystematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under\nboth unimodal and multimodal settings. Experimental results show that Caption\nInjection significantly outperforms text-only G-SEO baselines under the G-Eval\nmetric, demonstrating the necessity and effectiveness of multimodal integration\nin G-SEO to improve user-perceived content visibility.",
        "entry_id": "http://arxiv.org/abs/2511.04080v1",
        "pub_date": "2025-11-06",
        "translated_summary": "生成式搜索引擎（GSEs）通过检索增强生成技术与大语言模型整合多源信息，为用户提供精准全面的回答。与传统搜索引擎呈现排序列表的方式不同，GSEs将用户注意力从顺序浏览转向内容驱动的主观感知，推动信息检索范式变革。在此背景下，如何通过生成式搜索引擎优化方法提升内容的主观可见性成为新兴研究热点。随着多模态检索增强生成技术的快速发展，GSEs已能高效整合文本、图像、音频和视频，生成更丰富的响应以满足复杂信息需求。然而现有G-SEO方法仍局限于文本优化，未能充分利用多模态数据。为此，我们提出首项多模态G-SEO方法——字幕注入，通过提取图像描述文本并将其注入文本内容，融合视觉语义以增强生成式搜索场景下内容的主观可见性。我们在多模态检索增强生成基准MRAMG上系统评估了该方法在单模态与多模态设置下的表现。实验结果表明，在G-Eval指标下，字幕注入方法显著优于纯文本G-SEO基线，验证了多模态整合在提升用户感知内容可见性方面的必要性与有效性。"
    },
    {
        "title": "Two Decades of Research at the University of Lagos (2004-2023): A Scientometric Analysis of Productivity, Collaboration, and Impact",
        "summary": "This paper presents a scientometric analysis of research output from the\nUniversity of Lagos, focusing on the two decades spanning 2004 to 2023. Using\nbibliometric data retrieved from the Web of Science, we examine trends in\npublication volume, collaboration patterns, citation impact, and the most\nprolific authors, departments, and research domains at the university. The\nstudy reveals a consistent increase in research productivity, with the highest\npublication output recorded in 2023. Health Sciences, Engineering, and Social\nSciences are identified as dominant fields, reflecting the university's\ninterdisciplinary research strengths. Collaborative efforts, both locally and\ninternationally, show a positive correlation with higher citation impact, with\nthe United States and the United Kingdom being the leading international\ncollaborators. Notably, open-access publications account for a significant\nportion of the university's research output, enhancing visibility and citation\nrates. The findings offer valuable insights into the university's research\nperformance over the past two decades, providing a foundation for strategic\nplanning and policy formulation to foster research excellence and global\nimpact.",
        "entry_id": "http://arxiv.org/abs/2511.04075v1",
        "pub_date": "2025-11-06",
        "translated_summary": "本文对拉各斯大学2004至2023二十年间的研究产出展开科学计量分析。基于Web of Science检索的文献计量数据，我们系统考察了该校的论文发表趋势、合作模式、引用影响力以及核心作者、院系和研究领域。研究表明：该校科研生产力持续增长，2023年达到发文峰值；健康科学、工程学与社会科学构成三大优势学科，彰显跨学科研究实力；本地与国际合作均与高引用影响力呈正相关，其中美国与英国为主要国际合作对象。值得关注的是，开放获取论文在全校研究成果中占比显著，有效提升了学术可见度与引用率。这些发现为评估该校近二十年的科研绩效提供了重要依据，可为优化学术发展战略与政策制定提供参考，从而进一步提升研究质量与全球影响力。"
    },
    {
        "title": "Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters",
        "summary": "Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest\nvectors for a query vector from a dataset. It enforces that a specified set of\ndiscrete labels $S$ for the query must be included in the labels of each\nretrieved vector. Existing graph-based methods typically incorporate filter\nawareness by assigning fixed penalties or prioritizing nodes based on filter\nsatisfaction. However, since these methods use fixed, data in- dependent\npenalties, they often fail to generalize across datasets with diverse label and\nvector distributions. In this work, we propose a principled alternative that\nlearns the optimal trade-off between vector distance and filter match directly\nfrom the data, rather than relying on fixed penalties. We formulate this as a\nconstrained linear optimization problem, deriving weights that better reflect\nthe underlying filter distribution and more effectively address the filtered\nANN search problem. These learned weights guide both the search process and\nindex construction, leading to graph structures that more effectively capture\nthe underlying filter distribution and filter semantics. Our experiments\ndemonstrate that adapting the distance function to the data significantly im-\nproves accuracy by 5-10% over fixed-penalty methods, providing a more flexible\nand generalizable framework for the filtered ANN search problem.",
        "entry_id": "http://arxiv.org/abs/2511.04073v1",
        "pub_date": "2025-11-06",
        "translated_summary": "过滤式近似最近邻搜索能够从数据集中为查询向量检索最接近的向量，其核心要求是查询向量指定的离散标签集$S$必须包含于每个被检索向量的标签集合中。现有基于图的方法通常通过固定惩罚值或基于过滤条件满足程度的节点优先级机制来实现过滤感知。然而，由于这些方法采用固定且与数据无关的惩罚机制，往往难以在具有不同标签和向量分布的数据集上保持泛化能力。本研究提出一种理论严谨的替代方案：直接从数据中学习向量距离与过滤匹配之间的最优权衡，而非依赖固定惩罚机制。我们将其构建为约束线性优化问题，推导出的权重能更好反映底层过滤分布，从而更有效解决过滤式近似最近邻搜索问题。这些学习得到的权重同时指导搜索过程和索引构建，形成的图结构能更有效地捕捉底层过滤分布与过滤语义。实验表明，相较于固定惩罚方法，这种根据数据自适应调整距离函数的方式将准确率显著提升5-10%，为过滤式近似最近邻搜索问题提供了更灵活且可泛化的解决方案。"
    },
    {
        "title": "KnowThyself: An Agentic Assistant for LLM Interpretability",
        "summary": "We develop KnowThyself, an agentic assistant that advances large language\nmodel (LLM) interpretability. Existing tools provide useful insights but remain\nfragmented and code-intensive. KnowThyself consolidates these capabilities into\na chat-based interface, where users can upload models, pose natural language\nquestions, and obtain interactive visualizations with guided explanations. At\nits core, an orchestrator LLM first reformulates user queries, an agent router\nfurther directs them to specialized modules, and the outputs are finally\ncontextualized into coherent explanations. This design lowers technical\nbarriers and provides an extensible platform for LLM inspection. By embedding\nthe whole process into a conversational workflow, KnowThyself offers a robust\nfoundation for accessible LLM interpretability.",
        "entry_id": "http://arxiv.org/abs/2511.03878v1",
        "pub_date": "2025-11-05",
        "translated_summary": "我们开发了KnowThyself智能助手，这是一个推动大语言模型可解释性研究的智能辅助系统。现有工具虽能提供有用见解，但存在功能碎片化和代码依赖性强的问题。KnowThyself通过基于聊天的交互界面整合这些能力，用户可上传模型、用自然语言提问，并获得带引导说明的交互式可视化结果。其核心架构包含三层处理：协调器大模型首先重构用户查询，智能路由代理进一步将问题分发至专业模块，最终输出结果会被整合成连贯的阐释。这种设计既降低了技术门槛，又构建了可扩展的大语言模型检测平台。通过将全流程嵌入对话式工作流，KnowThyself为普及大语言模型可解释性提供了坚实基础。"
    },
    {
        "title": "CLAX: Fast and Flexible Neural Click Models in JAX",
        "summary": "CLAX is a JAX-based library that implements classic click models using modern\ngradient-based optimization. While neural click models have emerged over the\npast decade, complex click models based on probabilistic graphical models\n(PGMs) have not systematically adopted gradient-based optimization, preventing\npractitioners from leveraging modern deep learning frameworks while preserving\nthe interpretability of classic models. CLAX addresses this gap by replacing\nEM-based optimization with direct gradient-based optimization in a numerically\nstable manner. The framework's modular design enables the integration of any\ncomponent, from embeddings and deep networks to custom modules, into classic\nclick models for end-to-end optimization. We demonstrate CLAX's efficiency by\nrunning experiments on the full Baidu-ULTR dataset comprising over a billion\nuser sessions in $\\approx$ 2 hours on a single GPU, orders of magnitude faster\nthan traditional EM approaches. CLAX implements ten classic click models,\nserving both industry practitioners seeking to understand user behavior and\nimprove ranking performance at scale and researchers developing new click\nmodels. CLAX is available at: https://github.com/philipphager/clax",
        "entry_id": "http://arxiv.org/abs/2511.03620v1",
        "pub_date": "2025-11-05",
        "translated_summary": "CLAX是一个基于JAX的库，它通过现代梯度优化方法实现了经典点击模型。尽管神经点击模型在过去十年中不断涌现，但基于概率图模型的复杂点击模型尚未系统性地采用梯度优化方法，这导致从业者无法在保持经典模型可解释性的同时利用现代深度学习框架。CLAX通过以数值稳定的方式将基于EM的优化替换为直接梯度优化，成功解决了这一局限。该框架采用模块化设计，允许将嵌入层、深度网络乃至自定义模块等任何组件集成到经典点击模型中，实现端到端优化。我们在包含超十亿用户会话的完整Baidu-ULTR数据集上验证了CLAX的效率，单GPU仅需约2小时即可完成实验，比传统EM方法快数个数量级。CLAX实现了十种经典点击模型，既服务于需要理解用户行为并提升大规模排序性能的行业从业者，也助力于开发新型点击模型的研究人员。项目地址：https://github.com/philipphager/clax"
    },
    {
        "title": "A Semantic Encoding of Object Centric Event Data",
        "summary": "The Object-Centric Event Data (OCED) is a novel meta-model aimed at providing\na common ground for process data records centered around events and objects.\nOne of its objectives is to foster interoperability and process information\nexchange. In this context, the integration of data from different providers,\nthe combination of multiple processes, and the enhancement of knowledge\ninference are novel challenges. Semantic Web technologies can enable the\ncreation of a machine-readable OCED description enriched through ontology-based\nrelationships and entity categorization. In this paper, we introduce an\napproach built upon Semantic Web technologies for the realization of\nsemantic-enhanced OCED, with the aim to strengthen process data reasoning,\ninterconnect information sources, and boost expressiveness.",
        "entry_id": "http://arxiv.org/abs/2511.03351v1",
        "pub_date": "2025-11-05",
        "translated_summary": "以对象为中心的事件数据是一种新型元模型，旨在为围绕事件和对象构建的流程数据记录提供统一标准。该模型的目标之一是促进互操作性与流程信息交换。在此背景下，如何整合多源数据、融合多重流程以及增强知识推理能力成为全新挑战。语义网技术能够通过基于本体的关系定义与实体分类，构建机器可读的增强型事件数据描述。本文提出一种基于语义网技术的实现方法，旨在构建语义增强的以对象为中心的事件数据模型，从而强化流程数据推理能力，打通信息源之间的连接通道，提升系统表达能力。"
    },
    {
        "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning",
        "summary": "The rapid growth of open-access (OA) publications has intensified the\nchallenge of identifying relevant scientific papers. Due to privacy constraints\nand limited access to user interaction data, recent efforts have shifted toward\ncontent-based recommendation, which relies solely on textual information.\nHowever, existing models typically treat papers as unstructured text,\nneglecting their discourse organization and thereby limiting semantic\ncompleteness and interpretability. To address these limitations, we propose\nOMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective,\nMethod, Result, Conclusion) summarization, multi-level contrastive learning,\nand structure-aware re-ranking for scholarly recommendation. The QA-style\nsummarization module converts raw papers into structured and\ndiscourse-consistent representations, while multi-level contrastive objectives\nalign semantic representations across metadata, section, and document levels.\nThe final re-ranking stage further refines retrieval precision through\ncontextual similarity calibration. Experiments on DBLP, S2ORC, and the newly\nconstructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses\nstate-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in\nPrecision@10 and Recall@10, respectively. Additional evaluations confirm that\nQA-style summarization produces more coherent and factually complete\nrepresentations. Overall, OMRC-MR provides a unified and interpretable\ncontent-based paradigm for scientific paper recommendation, advancing\ntrustworthy and privacy-aware scholarly information retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.03330v1",
        "pub_date": "2025-11-05",
        "translated_summary": "开放获取（OA）文献的快速增长加剧了科学论文精准筛选的挑战。由于用户交互数据存在隐私限制与获取壁垒，近期研究重点已转向仅依赖文本信息的内容推荐方法。然而现有模型通常将论文视为非结构化文本，忽略了其论述结构，导致语义完整性受限且可解释性不足。为此，我们提出OMRC-MR分层框架，该框架融合了问答式OMRC（目标、方法、结果、结论）摘要生成、多层级对比学习与结构感知重排序技术，用于学术论文推荐。问答式摘要模块将原始论文转化为结构化的论述一致性表征，而多层级对比学习目标则在元数据、章节和文档层面实现语义表征对齐。最终的重排序阶段通过上下文相似度校准进一步提升检索精度。在DBLP、S2ORC及新建的Sci-OMRC数据集上的实验表明，OMRC-MR始终优于现有最优基线模型，在Precision@10和Recall@10指标上分别最高提升7.2%和3.8%。额外评估证实问答式摘要能生成更具连贯性与事实完整性的表征。总体而言，OMRC-MR为科学论文推荐提供了统一且可解释的内容驱动范式，推动了可信赖且保护隐私的学术信息检索发展。"
    },
    {
        "title": "Two thousand years of the oracle problem. Insights from Ancient Delphi on the future of blockchain oracles",
        "summary": "The oracle problem refers to the inability of an agent to know if the\ninformation coming from an oracle is authentic and unbiased. In ancient times,\nphilosophers and historians debated on how to evaluate, increase, and secure\nthe reliability of oracle predictions, particularly those from Delphi, which\npertained to matters of state. Today, we refer to data carriers for automatic\nmachines as oracles, but establishing a secure channel between these oracles\nand the real world still represents a challenge. Despite numerous efforts, this\nproblem remains mostly unsolved, and the recent advent of blockchain oracles\nhas added a layer of complexity because of the decentralization of blockchains.\nThis paper conceptually connects Delphic and modern blockchain oracles,\ndeveloping a comparative framework. Leveraging blockchain oracle taxonomy,\nlexical analysis is also performed on 167 Delphic queries to shed light on the\nrelationship between oracle answer quality and question type. The presented\nframework aims first at revealing commonalities between classical and\ncomputational oracles and then at enriching the oracle analysis within each\nfield. This study contributes to the computer science literature by proposing\nstrategies to improve the reliability of blockchain oracles based on insights\nfrom Delphi and to classical literature by introducing a framework that can\nalso be applied to interpret and classify other ancient oracular mechanisms.",
        "entry_id": "http://arxiv.org/abs/2511.03319v1",
        "pub_date": "2025-11-05",
        "translated_summary": "神谕问题是指行为体无法判断来自神谕的信息是否真实无偏。古代哲学家与历史学家曾就如何评估、提升并确保神谕预测（尤其是涉及国家事务的德尔斐神谕）的可靠性展开辩论。如今，我们将自动机器的数据载体称为预言机，但在这些预言机与现实世界之间建立安全通道仍具挑战。尽管付出诸多努力，该问题仍悬而未决，而近期区块链预言机的出现更因区块链的去中心化特性增添了复杂性。本文从概念层面将德尔斐神谕与现代区块链预言机相联系，构建出比较研究框架。基于区块链预言机分类法，我们对167条德尔斐神谕问询进行词法分析，以揭示神谕应答质量与问题类型之间的关联。该框架旨在首先揭示古典神谕与计算型预言机之间的共性，进而深化各自领域内的神谕分析。本研究通过借鉴德尔斐经验提出提升区块链预言机可靠性的策略，为计算机科学文献作出贡献；同时通过引入可适用于其他古代神谕机制解读与分类的分析框架，为古典文献研究提供了新视角。"
    },
    {
        "title": "KScaNN: Scalable Approximate Nearest Neighbor Search on Kunpeng",
        "summary": "Approximate Nearest Neighbor Search (ANNS) is a cornerstone algorithm for\ninformation retrieval, recommendation systems, and machine learning\napplications. While x86-based architectures have historically dominated this\ndomain, the increasing adoption of ARM-based servers in industry presents a\ncritical need for ANNS solutions optimized on ARM architectures. A naive port\nof existing x86 ANNS algorithms to ARM platforms results in a substantial\nperformance deficit, failing to leverage the unique capabilities of the\nunderlying hardware. To address this challenge, we introduce KScaNN, a novel\nANNS algorithm co-designed for the Kunpeng 920 ARM architecture. KScaNN\nembodies a holistic approach that synergizes sophisticated, data aware\nalgorithmic refinements with carefully-designed hardware specific\noptimizations. Its core contributions include: 1) novel algorithmic techniques,\nincluding a hybrid intra-cluster search strategy and an improved PQ residual\ncalculation method, which optimize the search process at a higher level; 2) an\nML-driven adaptive search module that provides adaptive, per-query tuning of\nsearch parameters, eliminating the inefficiencies of static configurations; and\n3) highly-optimized SIMD kernels for ARM that maximize hardware utilization for\nthe critical distance computation workloads. The experimental results\ndemonstrate that KScaNN not only closes the performance gap but establishes a\nnew standard, achieving up to a 1.63x speedup over the fastest x86-based\nsolution. This work provides a definitive blueprint for achieving\nleadership-class performance for vector search on modern ARM architectures and\nunderscores",
        "entry_id": "http://arxiv.org/abs/2511.03298v1",
        "pub_date": "2025-11-05",
        "translated_summary": "近似最近邻搜索（ANNS）是信息检索、推荐系统和机器学习应用的基础算法。虽然基于x86的架构历来主导该领域，但工业界对ARM服务器日益广泛的应用，亟需针对ARM架构优化的ANNS解决方案。将现有x86平台ANNS算法简单移植到ARM平台会导致显著性能损失，无法充分利用底层硬件的独特能力。为此，我们提出KScaNN——专为鲲鹏920 ARM架构协同设计的新型ANNS算法。该算法采用整体优化思路，将精密的数据感知算法改进与精心设计的硬件专用优化技术相融合，其核心创新包括：1）新型算法技术，包含混合式簇内搜索策略与改进的PQ残差计算方法，从更高维度优化搜索流程；2）基于机器学习的自适应搜索模块，实现按查询动态调整搜索参数，消除静态配置的效率瓶颈；3）针对ARM架构深度优化的SIMD内核，最大限度提升关键距离计算任务的硬件利用率。实验结果表明，KScaNN不仅弥补了性能差距，更树立了新的性能标杆，相较最快的x86解决方案实现最高1.63倍加速比。本研究为在现代ARM架构上实现领先级向量搜索性能提供了完整技术蓝图，同时印证了算法-硬件协同设计在下一代检索系统中的关键价值。"
    },
    {
        "title": "Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval",
        "summary": "Machine Translation for English Retrieval of Information in Any Language\n(MATERIAL) is an IARPA initiative targeted to advance the state of\ncross-lingual information retrieval (CLIR). This report provides a detailed\ndescription of Information Sciences Institute's (ISI's) Summarization and\ndomain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.\nSpecifically, we outline our team's novel approach to handle CLIR with emphasis\nin developing an approach amenable to retrieve a query-relevant document\n\\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3\nevaluations, SARAL exceeded the performance of other teams in five out of six\nevaluation conditions spanning three different languages (Farsi, Kazakh, and\nGeorgian).",
        "entry_id": "http://arxiv.org/abs/2511.03228v1",
        "pub_date": "2025-11-05",
        "translated_summary": "机器翻译英语跨语言信息检索项目（简称MATERIAL）是美国情报高级研究计划署推动跨语言信息检索技术发展的专项计划。本报告详细介绍了信息科学研究所SARAL团队在该项目中的研究成果，重点阐述了团队在跨语言检索方面的创新方法——突破传统排序文档列表模式，构建了一套能够检索查询相关文档集的解决方案。在MATERIAL第三阶段评估中，SARAL团队在涉及波斯语、哈萨克语和格鲁吉亚语三种语言的六项评测任务中，有五项性能表现超越其他参评团队。"
    },
    {
        "title": "Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification",
        "summary": "Large language models (LLMs) excel in generating fluent utterances but can\nlack reliable grounding in verified information. At the same time,\nknowledge-graph-based fact-checkers deliver precise and interpretable evidence,\nyet suffer from limited coverage or latency. By integrating LLMs with knowledge\ngraphs and real-time search agents, we introduce a hybrid fact-checking\napproach that leverages the individual strengths of each component. Our system\ncomprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid\none-hop lookups in DBpedia, 2) an LM-based classification guided by a\ntask-specific labeling prompt, producing outputs with internal rule-based\nlogic, and 3) a Web Search Agent invoked only when KG coverage is insufficient.\nOur pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the\nSupported/Refuted split without task-specific fine-tuning. To address Not\nenough information cases, we conduct a targeted reannotation study showing that\nour approach frequently uncovers valid evidence for claims originally labeled\nas Not Enough Information (NEI), as confirmed by both expert annotators and LLM\nreviewers. With this paper, we present a modular, opensource fact-checking\npipeline with fallback strategies and generalization across datasets.",
        "entry_id": "http://arxiv.org/abs/2511.03217v1",
        "pub_date": "2025-11-05",
        "translated_summary": "大型语言模型（LLM）在生成流畅文本方面表现出色，但可能缺乏对已验证信息的可靠依据。与此同时，基于知识图谱的事实核查系统虽能提供精确且可解释的证据，却存在覆盖范围有限或响应延迟的问题。通过将LLM与知识图谱及实时搜索智能体相结合，我们提出了一种混合式事实核查方法，充分发挥各组件的独特优势。该系统包含三个自动化步骤：1）知识图谱检索模块，用于在DBpedia中快速执行单跳查询；2）基于语言模型的分类器，通过任务特定的标注提示生成具有内部规则逻辑的输出；3）当知识图谱覆盖不足时触发的网络搜索代理。在FEVER基准测试的“支持/反驳”数据子集上，我们的流水线在不进行任务特定微调的情况下取得了0.93的F1分数。针对“信息不足”案例，我们开展了专项重标注研究，结果表明该方法能频繁为原标记为“信息不足”的声明找到有效证据，这一结论同时获得了专业标注者和LLM评审的确认。本文提出的模块化开源事实核查流水线具备故障应对策略，并展现出跨数据集的泛化能力。"
    },
    {
        "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework",
        "summary": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
        "entry_id": "http://arxiv.org/abs/2511.05385v1",
        "pub_date": "2025-11-07",
        "translated_summary": "检索增强生成（RAG）通过引入外部知识来提升大语言模型的可靠性。为实现灵活检索，智能体化RAG采用自主多轮检索与推理机制处理查询。尽管当前基于强化学习的智能体化RAG性能有所提升，但其搜索与推理过程常伴随显著的令牌消耗，形成以效率换取准确性的权衡。为此，本文提出TeaRAG——一个能同时压缩检索内容与推理步骤的高效令牌智能体化RAG框架：1）在检索内容压缩方面，通过在图检索中引入简洁三元组增强基于语块的语义检索，构建融合语义关联与共现关系的知识图谱，进而利用个性化PageRank算法聚焦核心知识，降低单次检索的令牌数；2）在推理步骤优化方面，提出迭代过程感知直接偏好优化（IP-DPO），其奖励函数通过知识匹配机制评估知识完备性，同时对冗余推理步骤施加惩罚。该设计可生成高质量偏好对数据集，支撑迭代式DPO训练以提升推理简洁性。在六个数据集上的实验表明，在Llama3-8B-Instruct和Qwen2.5-14B-Instruct模型上，TeaRAG将精确匹配率平均提升4%与2%，同时分别减少61%和59%的输出令牌数。代码已开源：https://github.com/Applied-Machine-Learning-Lab/TeaRAG。"
    },
    {
        "title": "QUESTER: Query Specification for Generative Retrieval",
        "summary": "Generative Retrieval (GR) differs from the traditional index-then-retrieve\npipeline by storing relevance in model parameters and directly generating\ndocument identifiers. However, GR often struggles to generalize and is costly\nto scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),\nwhich reframes GR as query specification generation - in this work, a simple\nkeyword query handled by BM25 - using a (small) LLM. The policy is trained\nusing reinforcement learning techniques (GRPO). Across in- and out-of-domain\nevaluations, we show that our model is more effective than BM25, and\ncompetitive with neural IR models, while maintaining a good efficiency",
        "entry_id": "http://arxiv.org/abs/2511.05301v1",
        "pub_date": "2025-11-07",
        "translated_summary": "生成式检索与传统“先索引后检索”流程不同，其将相关性信息存储于模型参数中，并直接生成文档标识符。然而该方法常面临泛化能力不足与扩展成本高昂的问题。我们提出QUESTER框架（查询规约生成式检索），通过（轻量级）大语言模型将生成式检索重构为查询规约生成任务——本研究中体现为可由BM25处理的简易关键词查询。该策略采用强化学习技术（GRPO）进行训练。在领域内外多项评估中，我们的模型不仅效果优于BM25，更能与神经信息检索模型保持竞争力，同时维持良好的检索效率。"
    },
    {
        "title": "Mapping Research Productivity of BRICS Countries with Special Reference to Coronary Artery Disease (CAD): A Scientometric Study",
        "summary": "This study presents a comprehensive scientometric analysis of research\nproductivity on Coronary Artery Disease (CAD) among the BRICS countries,\nBrazil, Russia, India, China, and South Africa, using data retrieved from the\nWeb of Science database for the period 1990 to 2019. A total of 50,036 records\nwere analyzed to assess publication growth trends, authorship patterns,\ncollaboration levels, and citation impact. The findings reveal a steady\nincrease in CAD-related publications, with China emerging as the leading\ncontributor, followed by Brazil, Russia, India, and South Africa. English\ndominated as the primary language of communication, accounting for over 93% of\npublications. Authorship and collaboration analysis indicate a high degree of\njoint research, with 97.91% of studies being co-authored and a degree of\ncollaboration of 0.98, underscoring the collective nature of scientific inquiry\nin this domain. The study validates the applicability of Lotkas Law for author\nproductivity, Bradfords Law for journal distribution, and Zipfs Law for keyword\nfrequency, while the Price Square Root Law was found inapplicable. The\npredominant publication format was journal articles (79.7%), and Kardiologiya\n(Russia) emerged as the most prolific journal. The results demonstrate\nsignificant growth in CAD research output and collaboration within BRICS,\nthough notable disparities persist among member nations. The study recommends\nenhancing individual author productivity, expanding international\ncollaboration, and supporting CAD research through strategic institutional and\ngovernmental initiatives. These findings provide valuable insights for\npolicymakers, funding agencies, and the academic community to strengthen\ncardiovascular research capacity within developing economies.",
        "entry_id": "http://arxiv.org/abs/2511.05211v1",
        "pub_date": "2025-11-07",
        "translated_summary": "本研究通过科学计量学方法，对1990-2019年间金砖国家（巴西、俄罗斯、印度、中国、南非）在冠状动脉疾病（CAD）领域的研究产出进行全面分析。基于Web of Science数据库提取的50,036条文献记录，系统评估了论文增长趋势、作者模式、合作水平及引用影响力。研究发现：CAD相关出版物呈稳定增长态势，中国位居发文量首位，其后依次为巴西、俄罗斯、印度和南非；英文为主要交流语言（占比93%以上）；作者与合作分析显示该领域具有高度协同性——合著率达97.91%，合作强度指数达0.98，凸显科研合作的集体性特征。研究验证了洛特卡定律（作者产出分布）、布拉德福定律（期刊分布）与齐普夫定律（关键词频次）的适用性，但普赖斯平方根定律在此不适用。期刊论文是主要成果形式（79.7%），其中俄罗斯《Kardiologiya》为最高产期刊。结果表明金砖国家CAD研究产出与合作显著增长，但成员国间仍存在明显差异。建议通过机构与政府的战略性支持，提升个体作者产出、拓展国际合作网络。本研究为政策制定者、资助机构及学术界加强发展中国家心血管研究能力建设提供了重要参考依据。"
    },
    {
        "title": "Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR",
        "summary": "In this paper, we present a novel series of Russian information retrieval\ndatasets constructed from the \"Did you know...\" section of Russian Wikipedia.\nOur datasets support a range of retrieval tasks, including fact-checking,\nretrieval-augmented generation, and full-document retrieval, by leveraging\ninteresting facts and their referenced Wikipedia articles annotated at the\nsentence level with graded relevance. We describe the methodology for dataset\ncreation that enables the expansion of existing Russian Information Retrieval\n(IR) resources. Through extensive experiments, we extend the RusBEIR research\nby comparing lexical retrieval models, such as BM25, with state-of-the-art\nneural architectures fine-tuned for Russian, as well as multilingual models.\nResults of our experiments show that lexical methods tend to outperform neural\nmodels on full-document retrieval, while neural approaches better capture\nlexical semantics in shorter texts, such as in fact-checking or fine-grained\nretrieval. Using our newly created datasets, we also analyze the impact of\ndocument length on retrieval performance and demonstrate that combining\nretrieval with neural reranking consistently improves results. Our contribution\nexpands the resources available for Russian information retrieval research and\nhighlights the importance of accurate evaluation of retrieval models to achieve\noptimal performance. All datasets are publicly available at HuggingFace. To\nfacilitate reproducibility and future research, we also release the full\nimplementation on GitHub.",
        "entry_id": "http://arxiv.org/abs/2511.05079v1",
        "pub_date": "2025-11-07",
        "translated_summary": "本文基于俄语维基百科\"你知道吗…\"栏目构建了一套新颖的俄语信息检索数据集。该数据集通过利用趣味性事实及其引用的维基百科文章（附带句子级分级相关性标注），支持事实核查、检索增强生成和全文档检索等多重任务。我们详细阐述了扩展现有俄语信息检索资源的数据集构建方法，并通过系列实验拓展了RusBEIR研究框架，系统比较了BM25等词汇检索模型与针对俄语优化的前沿神经架构及多语言模型。实验结果表明：在全文档检索任务中，词汇检索方法普遍优于神经模型，而在事实核查或细粒度检索等短文本场景中，神经方法更能有效捕捉词汇语义。基于新建数据集，我们进一步分析了文档长度对检索性能的影响，论证了神经重排序与检索结合的持续增效作用。本研究成果拓展了俄语信息检索的研究资源，强调精准评估检索模型对实现最优性能的重要性。所有数据集已发布于HuggingFace平台，并为确保可复现性及后续研究，我们在GitHub同步开放完整实现代码。"
    },
    {
        "title": "The use of social media among library professionals and patrons: A review of literature",
        "summary": "This paper focused on the utilization of social media by library\nprofessionals and library users. It provides an understanding of social media,\nthe most popular social media platforms utilized in the libraries. It also\nmentions the reasons for the adoption of social media in libraries be it\nacademic, public, school libraries and other types of libraries. This is a\nreview paper on the use of social media among library professionals and\npatrons. The findings reveal the contributions of social media to the\nlibraries. Social media makes things easy for library professionals and library\nusers. It enables them to connect, create awareness to new information,\ndisseminate information instantly, and helps to market the library resources\nand services. Therefore, it is recommended amongst others that the library\nmanagement board should encourage the use of social media in libraries.",
        "entry_id": "http://arxiv.org/abs/2511.05051v1",
        "pub_date": "2025-11-07",
        "translated_summary": "本文聚焦图书馆从业人员与用户对社交媒体的应用实践，系统阐释了社交媒体的概念内涵、图书馆界使用最普遍的社交平台类型，并剖析了高校图书馆、公共图书馆、中小学图书馆及各类专业图书馆采纳社交媒体技术的动因。作为针对图书馆从业者与使用者社交媒体应用的综述性研究，本文发现社交媒体为图书馆事业带来多重赋能：它显著提升馆员工作效率与用户服务体验，构建馆群与用户间的即时连接渠道，助推最新资讯的传播触达，实现馆藏资源与服务的精准推广。基于研究结论，建议图书馆管理委员会将社交媒体应用纳入发展战略，积极推动各类图书馆部署社会化媒体工具。"
    },
    {
        "title": "Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval",
        "summary": "As financial applications of large language models (LLMs) gain attention,\naccurate Information Retrieval (IR) remains crucial for reliable AI services.\nHowever, existing benchmarks fail to capture the complex and domain-specific\ninformation needs of real-world banking scenarios. Building domain-specific IR\nbenchmarks is costly and constrained by legal restrictions on using real\ncustomer data. To address these challenges, we propose a systematic methodology\nfor constructing domain-specific IR benchmarks through LLM-based query\ngeneration. As a concrete implementation of this methodology, our pipeline\ncombines single and multi-document query generation with an enhanced and\nreasoning-augmented answerability assessment method, achieving stronger\nalignment with human judgments than prior approaches. Using this methodology,\nwe construct KoBankIR, comprising 815 queries derived from 204 official banking\ndocuments. Our experiments show that existing retrieval models struggle with\nthe complex multi-document queries in KoBankIR, demonstrating the value of our\nsystematic approach for domain-specific benchmark construction and underscoring\nthe need for improved retrieval techniques in financial domains.",
        "entry_id": "http://arxiv.org/abs/2511.05000v1",
        "pub_date": "2025-11-07",
        "translated_summary": "随着大语言模型在金融领域的应用日益受到关注，精准的信息检索技术仍是确保人工智能服务可靠性的关键。然而，现有基准测试难以全面反映真实银行场景中复杂且具有领域特性的信息需求。构建领域专用信息检索基准不仅成本高昂，还受到使用真实客户数据的法律限制。为应对这些挑战，我们提出一种基于大语言模型的系统性领域检索基准构建方法。作为该方法的具体实践，我们的流程将单文档与多文档查询生成相结合，并引入增强型推理辅助可答性评估机制，相比现有方法更能契合人类判断标准。基于该方法构建的KoBankIR基准库包含从204份官方银行文件衍生的815条查询指令。实验表明，现有检索模型在应对KoBankIR中复杂的多文档查询时表现不佳，这既印证了我们系统性构建方法的有效性，也揭示了金融领域检索技术亟待提升的现实需求。"
    },
    {
        "title": "Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG",
        "summary": "Retrieval systems are essential to contemporary AI pipelines, although most\nconfuse two separate processes: finding relevant information and giving enough\ncontext for reasoning. We introduce the Search-Is-Not-Retrieve (SINR)\nframework, a dual-layer architecture that distinguishes between fine-grained\nsearch representations and coarse-grained retrieval contexts. SINR enhances the\ncomposability, scalability, and context fidelity of retrieval systems by\ndirectly connecting small, semantically accurate search chunks to larger,\ncontextually complete retrieve chunks, all without incurring extra processing\ncosts. This design changes retrieval from a passive step to an active one,\nmaking the system architecture more like how people process information. We\ndiscuss the SINR framework's conceptual foundation, formal structure,\nimplementation issues, and qualitative outcomes. This provides a practical\nfoundation for the next generation of AI systems that use retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.04939v1",
        "pub_date": "2025-11-07",
        "translated_summary": "检索系统是现代人工智能流程的核心组件，但多数系统混淆了两个独立过程：寻找相关信息与提供充分推理语境。我们提出\"搜索非检索\"（SINR）框架，该双层级架构明确区分细粒度搜索表征与粗粒度检索语境。通过将语义精准的小型搜索块与语境完整的宏观检索块直接关联，SINR在无需额外处理成本的前提下，显著提升了检索系统的可组合性、扩展性与语境保真度。这一设计使检索从被动步骤转变为主动过程，令系统架构更贴近人类信息处理模式。我们将深入探讨SINR框架的理论基础、形式化结构、实施要点与质性成效，为下一代基于检索的人工智能系统奠定实践基础。"
    },
    {
        "title": "Association via Entropy Reduction",
        "summary": "Prior to recent successes using neural networks, term frequency-inverse\ndocument frequency (tf-idf) was clearly regarded as the best choice for\nidentifying documents related to a query. We provide a different score, aver,\nand observe, on a dataset with ground truth marking for association, that aver\ndoes do better at finding assciated pairs than tf-idf. This example involves\nfinding associated vertices in a large graph and that may be an area where\nneural networks are not currently an obvious best choice. Beyond this one\nanecdote, we observe that (1) aver has a natural threshold for declaring pairs\nas unassociated while tf-idf does not, (2) aver can distinguish between pairs\nof documents for which tf-idf gives a score of 1.0, (3) aver can be applied to\nlarger collections of documents than pairs while tf-idf cannot, and (4) that\naver is derived from entropy under a simple statistical model while tf-idf is a\nconstruction designed to achieve a certain goal and hence aver may be more\n\"natural.\" To be fair, we also observe that (1) writing down and computing the\naver score for a pair is more complex than for tf-idf and (2) that the fact\nthat the aver score is naturally scale-free makes it more complicated to\ninterpret aver scores.",
        "entry_id": "http://arxiv.org/abs/2511.04901v1",
        "pub_date": "2025-11-07",
        "translated_summary": "在神经网络取得近期成功之前，词频-逆文档频率（tf-idf）被公认为检索关联文档的最佳方法。我们提出了一种新型评分指标aver，并在具有真实关联标注的数据集上验证了其在发现关联文档对方面确实优于tf-idf。本案例涉及大型图中的关联顶点发现，这或许是神经网络目前尚未显现明显优势的领域。除该实例外，我们还发现：（1）aver具有判定非关联对的天然阈值而tf-idf不具备；（2）对于tf-idf评分均为1.0的文档对，aver能有效区分其关联强度；（3）aver可扩展应用于多文档集而tf-idf仅适用于文档对；（4）aver基于简单统计模型中的熵推导得出，而tf-idf是为实现特定目标构建的指标，因此aver可能更具“自然性”。公允而言，我们也注意到：（1）aver的计算公式与过程较tf-idf更为复杂；（2）aver天然具备无量纲特性，这使其得分解读更具挑战性。"
    },
    {
        "title": "EMO100DB: An Open Dataset of Improvised Songs with Emotion Data",
        "summary": "In this study, we introduce Emo100DB: a dataset consisting of improvised\nsongs that were recorded and transcribed with emotion data based on Russell's\ncircumplex model of emotion. The dataset was developed by collecting improvised\nsongs that consist of melody, lyrics, and an instrumental accompaniment played,\nsung, and recorded by 20 young adults. Before recording each song, the\nparticipants were asked to report their emotional state, with the axes\nrepresenting arousal and valence based on Russell's circumplex model of\nemotions. The dataset is organized into four emotion quadrants, and it includes\nthe lyrics text and MIDI file of the melody extracted from the participant\nrecordings, along with the original audio in WAV format. By providing an\nintegrated composition of data and analysis, this study aims to offer a\ncomprehensive dataset that allows for a diverse exploration of the relationship\nbetween music and emotion.",
        "entry_id": "http://arxiv.org/abs/2511.04755v1",
        "pub_date": "2025-11-06",
        "translated_summary": "本研究推出Emo100DB数据集——一个收录即兴演唱歌曲的数据库，所有曲目均基于罗素情感环状模型进行录音、文本转写及情感标注。该数据集通过采集20位青年创作者即兴创作的歌曲构建而成，每首作品包含由参与者演奏录制的旋律、人声歌词与器乐伴奏。在录制前，参与者需根据罗素情感环状模型的双轴维度（唤醒度与效价）自评当前情感状态。数据集按情感象限分为四类，除原始WAV格式音频外，还提供从录音中提取的歌词文本与旋律MIDI文件。通过整合数据资源与分析维度，本研究旨在构建一个能够支持多角度探索音乐与情感关联的综合性数据集。"
    },
    {
        "title": "On the Brittleness of CLIP Text Encoders",
        "summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy.",
        "entry_id": "http://arxiv.org/abs/2511.04247v2",
        "pub_date": "2025-11-06",
        "translated_summary": "近年来，多模态协同嵌入模型（特别是CLIP）通过将图像与文本对齐至共享表征空间，在零样本分类和多媒体信息检索领域实现了重大突破。然而，基于对比对齐训练的此类模型对输入微小扰动缺乏稳定性。尤其在处理人工表述的查询时，查询语句的细微变化可能导致最佳匹配结果的排序产生显著差异。本文系统分析了多媒体信息检索场景中多类非语义查询扰动的影响，基于TRECVID Ad-Hoc视频搜索查询和V3C1视频数据集，对多种CLIP变体进行了词汇、句法和语义层面的扰动测试。研究发现：所有模型均对句法与语义扰动最为敏感，而鲁棒性薄弱环节集中在标点符号、大小写等表层文本编辑。这一结果表明，在基准准确率之外，鲁棒性应成为评估视觉语言模型的关键维度。"
    },
    {
        "title": "Collaborative residual learners for automatic icd10 prediction using prescribed medications",
        "summary": "Clinical coding is an administrative process that involves the translation of diagnostic data from episodes of care into a standard code format such as ICD10. It has many critical applications such as billing and aetiology research. The automation of clinical coding is very challenging due to data sparsity, low interoperability of digital health systems, complexity of real-life diagnosis coupled with the huge size of ICD10 code space. Related work suffer from low applicability due to reliance on many data sources, inefficient modelling and less generalizable solutions. We propose a novel collaborative residual learning based model to automatically predict ICD10 codes employing only prescriptions data. Extensive experiments were performed on two real-world clinical datasets (outpatient & inpatient) from Maharaj Nakorn Chiang Mai Hospital with real case-mix distributions. We obtain multi-label classification accuracy of 0.71 and 0.57 of average precision, 0.57 and 0.38 of F1-score and 0.73 and 0.44 of accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.",
        "entry_id": "http://arxiv.org/abs/2012.11327v1",
        "pub_date": "2020-12-16",
        "translated_summary": "临床编码是将诊疗过程中的诊断数据转换为ICD10等标准编码格式的行政流程，在医疗计费和病因学研究等领域具有重要应用。由于数据稀疏性、数字健康系统互操作性低、真实诊断复杂性及ICD10编码体系庞大，临床编码自动化面临巨大挑战。现有研究因依赖多数据源、建模效率低及解决方案普适性差，存在适用性不足的问题。本文提出一种基于协同残差学习的新型模型，仅通过处方数据即可实现ICD10编码的自动预测。我们在清迈大学马哈拉吉医院包含真实病例组合的门诊与住院数据集上开展了大量实验，结果显示：针对住院和门诊数据集，多标签分类的平均精确度分别达到0.71和0.57，F1分数分别为0.57和0.38，主要诊断预测准确率分别达到0.73和0.44。"
    },
    {
        "title": "Ensemble model for pre-discharge icd10 coding prediction",
        "summary": "The translation of medical diagnosis to clinical coding has wide range of applications in billing, aetiology analysis, and auditing. Currently, coding is a manual effort while the automation of such task is not straight forward. Among the challenges are the messy and noisy clinical records, case complexities, along with the huge ICD10 code space. Previous work mainly relied on discharge notes for prediction and was applied to a very limited data scale. We propose an ensemble model incorporating multiple clinical data sources for accurate code predictions. We further propose an assessment mechanism to provide confidence rates in predicted outcomes. Extensive experiments were performed on two new real-world clinical datasets (inpatient & outpatient) with unaltered case-mix distributions from Maharaj Nakorn Chiang Mai Hospital. We obtain multi-label classification accuracies of 0.73 and 0.58 for average precision, 0.56 and 0.35 for F1-scores and 0.71 and 0.4 accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.",
        "entry_id": "http://arxiv.org/abs/2012.11333v1",
        "pub_date": "2020-12-16",
        "translated_summary": "将医疗诊断转化为临床编码在医疗账单管理、病因分析和审计等领域具有广泛应用。当前编码工作主要依赖人工完成，而实现该任务的自动化面临诸多挑战：临床记录杂乱且存在噪声、病例复杂度高、ICD10编码体系庞大。既往研究主要依赖出院小结进行预测，且仅在极有限的数据规模上实施。我们提出一种融合多源临床数据的集成模型，以实现精准的编码预测，并建立评估机制为预测结果提供置信度评级。基于玛哈叻清迈医院未经筛选的真实临床数据集（住院与门诊），我们开展了大规模实验验证。实验结果显示：在住院与门诊数据集上，多标签分类的平均精确度分别达到0.73和0.58，F1分数分别为0.56和0.35，主要诊断预测准确率分别达到0.71和0.4。"
    },
    {
        "title": "Should I visit this place? Inclusion and Exclusion Phrase Mining from Reviews",
        "summary": "Although several automatic itinerary generation services have made travel planning easy, often times travellers find themselves in unique situations where they cannot make the best out of their trip. Visitors differ in terms of many factors such as suffering from a disability, being of a particular dietary preference, travelling with a toddler, etc. While most tourist spots are universal, others may not be inclusive for all. In this paper, we focus on the problem of mining inclusion and exclusion phrases associated with 11 such factors, from reviews related to a tourist spot. While existing work on tourism data mining mainly focuses on structured extraction of trip related information, personalized sentiment analysis, and automatic itinerary generation, to the best of our knowledge this is the first work on inclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000 reviews related to 1000 tourist spots, our broad level classifier provides a binary overlap F1 of $\\sim$80 and $\\sim$82 to classify a phrase as inclusion or exclusion respectively. Further, our inclusion/exclusion classifier provides an F1 of $\\sim$98 and $\\sim$97 for 11-class inclusion and exclusion classification respectively. We believe that our work can significantly improve the quality of an automatic itinerary generation service.",
        "entry_id": "http://arxiv.org/abs/2012.10226v1",
        "pub_date": "2020-12-18",
        "translated_summary": "尽管多项自动化行程生成服务已使旅行规划变得便捷，但游客在特殊情境下仍难以充分享受旅程。游客个体差异显著——或身患残疾、或有特殊饮食偏好、或需携带幼童同行等。虽然多数旅游景点具有普适性，但部分场所却无法满足所有人群需求。本文重点研究从旅游景点相关评论中挖掘涉及11类特殊因素的包容性与排斥性短语。现有旅游数据挖掘研究主要集中于行程信息的结构化提取、个性化情感分析及自动化行程生成，而本研究首次针对旅游评论中的包容/排斥短语进行挖掘。基于涵盖1000个旅游景点的2000条评论数据集，我们的广义分类器在判断短语属于包容类或排斥类时，分别获得约80和约82的二元重叠F1值。进一步地，针对11类细分场景的包容与排斥分类，我们的专用分类器分别取得了约98和约97的F1值。我们相信这项研究将显著提升自动化行程生成服务的质量。"
    },
    {
        "title": "Intelligent Vector-based Customer Segmentation in the Banking Industry",
        "summary": "Customer Segmentation is the process of dividing customers into groups based on common characteristics. An intelligent Customer Segmentation will not only enable an organization to effectively allocate marketing resources (e.g., Recommender Systems in the Banking sector) but also it will enable identifying the customer cohorts that are most likely to benefit from a specific policy (e.g., to discover diverse patient groups in the Health sector). While there has been a significant improvement in approaches to Customer Segmentation, the main challenge remains to be the understanding of the reasons behind the segmentation need. This task is challenging as it is subjective and depends on the goal of segmentation as well as the analyst's perspective. To address this challenge, in this paper, we present an intelligent vector-based customer segmentation approach. The proposed approach will leverage feature engineering to enable analysts to identify important features (from a pool of features such as demographics, geography, psychographics, behavioral, and more) and feed them into a neural embedding framework named Customer2Vec. The Customer2Vec combines the neural network classification and clustering methods as supervised and unsupervised learning techniques to embed the customer vector. We adopt a typical scenario in the Banking Sector to highlight how Customer2Vec significantly improves the quality of the segmentation and detecting customer similarities.",
        "entry_id": "http://arxiv.org/abs/2012.11876v1",
        "pub_date": "2020-12-22",
        "translated_summary": "客户细分是根据共同特征将客户划分为不同群体的过程。智能化的客户细分不仅能够帮助机构有效配置营销资源（例如银行业的推荐系统），还能识别最可能从特定政策中受益的客户群体（例如医疗领域中发现不同类型的患者群体）。尽管客户细分方法已取得显著进展，但核心挑战仍在于理解细分需求背后的动因。由于该任务具有主观性，且取决于细分目标和分析师视角，因此极具挑战性。为解决这一难题，本文提出了一种基于向量的智能客户细分方法。该方法通过特征工程，使分析师能够从人口统计、地理、心理特征、行为特征等特征池中识别重要特征，并将其输入名为Customer2Vec的神经嵌入框架。该框架结合神经网络分类（监督学习）与聚类（无监督学习）技术，构建客户向量嵌入模型。我们通过银行业典型场景验证了Customer2Vec能显著提升细分质量与客户相似度检测效果。"
    },
    {
        "title": "Dynamic-K Recommendation with Personalized Decision Boundary",
        "summary": "In this paper, we investigate the recommendation task in the most common scenario with implicit feedback (e.g., clicks, purchases). State-of-the-art methods in this direction usually cast the problem as to learn a personalized ranking on a set of items (e.g., webpages, products). The top-N results are then provided to users as recommendations, where the N is usually a fixed number pre-defined by the system according to some heuristic criteria (e.g., page size, screen size). There is one major assumption underlying this fixed-number recommendation scheme, i.e., there are always sufficient relevant items to users' preferences. Unfortunately, this assumption may not always hold in real-world scenarios. In some applications, there might be very limited candidate items to recommend, and some users may have very high relevance requirement in recommendation. In this way, even the top-1 ranked item may not be relevant to a user's preference. Therefore, we argue that it is critical to provide a dynamic-K recommendation, where the K should be different with respect to the candidate item set and the target user. We formulate this dynamic-K recommendation task as a joint learning problem with both ranking and classification objectives. The ranking objective is the same as existing methods, i.e., to create a ranking list of items according to users' interests. The classification objective is unique in this work, which aims to learn a personalized decision boundary to differentiate the relevant items from irrelevant items. Based on these ideas, we extend two state-of-the-art ranking-based recommendation methods, i.e., BPRMF and HRM, to the corresponding dynamic-K versions, namely DK-BPRMF and DK-HRM. Our experimental results on two datasets show that the dynamic-K models are more effective than the original fixed-N recommendation methods.",
        "entry_id": "http://arxiv.org/abs/2012.13569v1",
        "pub_date": "2020-12-25",
        "translated_summary": "本文研究基于隐式反馈（如点击、购买行为）的推荐任务。该领域的先进方法通常将问题转化为对项目集合（如网页、商品）进行个性化排序学习，并将前N个结果作为推荐内容提供给用户。其中N值通常是根据启发式标准（如页面尺寸、屏幕大小）预设的固定数值。这种固定数量推荐方案存在一个关键前提假设：系统总能找到足够多的符合用户偏好的相关项目。然而在实际场景中，该假设未必始终成立。某些应用场景中可推荐候选项目非常有限，部分用户对推荐内容的相关性要求极高，此时即使排名首位的项目也可能与用户偏好不匹配。因此我们提出动态K值推荐机制，其核心在于根据候选项目集和目标用户特性动态调整K值。我们将该任务形式化为包含排序与分类目标的联合学习问题：排序目标与现有方法一致，即根据用户兴趣生成项目排序列表；分类目标则是本研究的创新点，旨在通过学习个性化决策边界来区分相关与无关项目。基于此思路，我们拓展了两种先进排序推荐方法（BPRMF与HRM），提出对应的动态K版本DK-BPRMF和DK-HRM。在两个数据集上的实验结果表明，动态K模型较原始固定N值推荐方法具有显著优势。"
    },
    {
        "title": "Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision Approach",
        "summary": "The proliferation of massive open online courses (MOOCs) demands an effective way of course recommendation for jobs posted in recruitment websites, especially for the people who take MOOCs to find new jobs. Despite the advances of supervised ranking models, the lack of enough supervised signals prevents us from directly learning a supervised ranking model. This paper proposes a general automated weak supervision framework AutoWeakS via reinforcement learning to solve the problem. On the one hand, the framework enables training multiple supervised ranking models upon the pseudo labels produced by multiple unsupervised ranking models. On the other hand, the framework enables automatically searching the optimal combination of these supervised and unsupervised models. Systematically, we evaluate the proposed model on several datasets of jobs from different recruitment websites and courses from a MOOCs platform. Experiments show that our model significantly outperforms the classical unsupervised, supervised and weak supervision baselines.",
        "entry_id": "http://arxiv.org/abs/2012.14234v1",
        "pub_date": "2020-12-28",
        "translated_summary": "大规模开放在线课程(MOOC)的激增，要求招聘网站能为发布的职位提供有效的课程推荐服务，尤其对希望通过慕课求职的人群具有重要意义。尽管现有排序模型已取得长足发展，但监督信号的严重缺失制约了有监督排序模型的直接应用。为此，本文提出基于强化学习的通用自动化弱监督框架AutoWeakS：一方面通过无监督排序模型生成伪标签，进而训练多个有监督排序模型；另一方面通过强化学习自动搜索最优模型组合。我们系统化地在多个招聘网站职位数据集和慕课平台课程数据集上进行评估，实验表明该模型显著优于经典的无监督、有监督及弱监督基线方法。"
    },
    {
        "title": "Measuring University Impact: Wikipedia approach",
        "summary": "The impact of Universities on the social, economic and political landscape is one of the key directions in contemporary educational evaluation. In this paper, we discuss the new methodological technique that evaluates the impact of university based on popularity (number of page-views) of their alumni's pages on Wikipedia. It allows revealing the alumni popularity dynamics and tracking its state. Preliminary analysis shows that the number of page-views is higher for the contemporary persons that prove the perspectives of this approach. Then, universities were ranked based on the methodology and compared to the famous international university rankings ARWU and QS based only on alumni scales: for the top 10 universities, there is an intersection of two universities (Columbia University, Stanford University). The correlation coefficients between different university rankings are provided in the paper. Finally, the ranking based on the alumni popularity was compared with the ranking of universities based on the popularity of their webpages on Wikipedia: there is a strong connection between these indicators.",
        "entry_id": "http://arxiv.org/abs/2012.13980v1",
        "pub_date": "2020-12-27",
        "translated_summary": "高校对社会、经济及政治格局的影响是当代教育评估的核心方向之一。本文探讨了一种基于维基百科校友页面浏览量数据的高校影响力评估新方法。该方法能揭示校友知名度动态变化并追踪其状态。初步分析表明，当代人物的页面浏览量更高，这印证了该方法的应用前景。我们根据该方法对高校进行排名，并与仅基于校友规模的国际知名大学排名（ARWU和QS）进行比较：在前十名高校中，有两个大学重合（哥伦比亚大学、斯坦福大学）。文中提供了不同大学排名间的相关系数。最后，将基于校友知名度的排名与基于高校维基百科页面浏览量的排名进行对比，发现这两项指标之间存在显著关联。"
    },
    {
        "title": "Neural document expansion for ad-hoc information retrieval",
        "summary": "Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",
        "entry_id": "http://arxiv.org/abs/2012.14005v1",
        "pub_date": "2020-12-27",
        "translated_summary": "最近，Nogueira等人[2019]提出了一种基于神经序列到序列模型的新文档扩展方法，在短文本检索任务上展现出显著提升。然而，该方法需要大量领域内训练数据。本文证明，这种神经文档扩展方法能有效适配标准信息检索任务——这类任务通常面临标注稀缺且存在大量长文档的挑战。"
    },
    {
        "title": "Query Expansion for Cross-Language Question Re-Ranking",
        "summary": "Community question-answering (CQA) platforms have become very popular forums for asking and answering questions daily. While these forums are rich repositories of community knowledge, they present challenges for finding relevant answers and similar questions, due to the open-ended nature of informal discussions. Further, if the platform allows questions and answers in multiple languages, we are faced with the additional challenge of matching cross-lingual information. In this work, we focus on the cross-language question re-ranking shared task, which aims to find existing questions that may be written in different languages. Our contribution is an exploration of query expansion techniques for this problem. We investigate expansions based on Word Embeddings, DBpedia concepts linking, and Hypernym, and show that they outperform existing state-of-the-art methods.",
        "entry_id": "http://arxiv.org/abs/1904.07982v1",
        "pub_date": "2019-04-16",
        "translated_summary": "社区问答平台已成为日常提问与回答的热门论坛。尽管这些论坛是社区知识的丰富宝库，但由于开放式非正式讨论的特性，在寻找相关答案和类似问题时仍面临挑战。此外，若平台允许多语言提问与回答，我们还需应对跨语言信息匹配这一额外难题。本研究聚焦于跨语言问题重排序共享任务，旨在发现可能以不同语言表述的现存问题。我们的贡献在于针对该问题探索查询扩展技术，研究了基于词嵌入、DBpedia概念链接和上位词关系的扩展方法，并证明这些方法优于现有前沿技术。"
    },
    {
        "title": "How to define co-occurrence in different domains of study?",
        "summary": "This position paper presents a comparative study of co-occurrences. Some similarities and differences in the definition exist depending on the research domain (e.g. linguistics, NLP, computer science). This paper discusses these points, and deals with the methodological aspects in order to identify co-occurrences in a multidisciplinary paradigm.",
        "entry_id": "http://arxiv.org/abs/1904.08010v1",
        "pub_date": "2019-04-16",
        "translated_summary": "本立场文件对共现关系展开了一项比较研究。根据研究领域（如语言学、自然语言处理、计算机科学）的不同，其定义存在若干异同之处。本文通过多学科范式探讨这些要点，并论述识别共现关系的方法论层面。"
    },
    {
        "title": "Neural Message Passing for Multi-Label Classification",
        "summary": "Multi-label classification (MLC) is the task of assigning a set of target labels for a given sample. Modeling the combinatorial label interactions in MLC has been a long-haul challenge. We propose Label Message Passing (LaMP) Neural Networks to efficiently model the joint prediction of multiple labels. LaMP treats labels as nodes on a label-interaction graph and computes the hidden representation of each label node conditioned on the input using attention-based neural message passing. Attention enables LaMP to assign different importance to neighbor nodes per label, learning how labels interact (implicitly). The proposed models are simple, accurate, interpretable, structure-agnostic, and applicable for predicting dense labels since LaMP is incredibly parallelizable. We validate the benefits of LaMP on seven real-world MLC datasets, covering a broad spectrum of input/output types and outperforming the state-of-the-art results. Notably, LaMP enables intuitive interpretation of how classifying each label depends on the elements of a sample and at the same time rely on its interaction with other labels. We provide our code and datasets at https://github.com/QData/LaMP",
        "entry_id": "http://arxiv.org/abs/1904.08049v1",
        "pub_date": "2019-04-17",
        "translated_summary": "多标签分类任务旨在为给定样本分配一组目标标签，而如何建模标签间的组合交互关系一直是该领域的长期挑战。我们提出标签消息传递神经网络，通过基于注意力机制的神经消息传递技术，将标签视为标签交互图中的节点，并基于输入计算每个标签节点的隐表示。注意力机制使模型能够为每个标签的相邻节点分配不同权重，从而隐式学习标签间的交互规律。该模型结构简洁、预测精准、可解释性强，且不依赖特定图结构——由于具备高度并行化特性，尤其适用于密集标签预测场景。我们在七个真实多标签数据集上验证了LaMP的优越性，这些数据集覆盖多种输入/输出类型，实验结果表明其性能超越现有最优方法。值得注意的是，LaMP能直观展示每个标签的分类决策如何依赖于样本特征元素，同时揭示其与其他标签的交互依赖关系。代码与数据集已开源：https://github.com/QData/LaMP"
    },
    {
        "title": "Emotional Contribution Analysis of Online Reviews",
        "summary": "In response to the constant increase in population and tourism worldwide, there is a need for the development of cross-language market research tools that are more cost and time effective than surveys or interviews. Focusing on the Chinese tourism boom and the hotel industry in Japan, we extracted the most influential keywords in emotional judgement from Chinese online reviews of Japanese hotels in the portal site Ctrip. Using an entropy based mathematical model and a machine learning algorithm, we determined the words that most closely represent the demands and emotions of this customer base.",
        "entry_id": "http://arxiv.org/abs/1905.00185v1",
        "pub_date": "2019-05-01",
        "translated_summary": "针对全球人口与旅游业的持续增长，亟需开发比传统问卷和访谈更具成本与时间效益的跨语言市场调研工具。本研究聚焦中国游客赴日旅游热潮及日本酒店业，通过携程网中文评论数据，运用基于信息熵的数学模型与机器学习算法，从中国游客对日本酒店的在线评价中提取情感判断最具影响力的关键词，精准识别该客群的核心需求与情感倾向。"
    },
    {
        "title": "FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance",
        "summary": "Frequently Asked Question (FAQ) retrieval is an important task where the objective is to retrieve an appropriate Question-Answer (QA) pair from a database based on a user's query. We propose a FAQ retrieval system that considers the similarity between a user's query and a question as well as the relevance between the query and an answer. Although a common approach to FAQ retrieval is to construct labeled data for training, it takes annotation costs. Therefore, we use a traditional unsupervised information retrieval system to calculate the similarity between the query and question. On the other hand, the relevance between the query and answer can be learned by using QA pairs in a FAQ database. The recently-proposed BERT model is used for the relevance calculation. Since the number of QA pairs in FAQ page is not enough to train a model, we cope with this issue by leveraging FAQ sets that are similar to the one in question. We evaluate our approach on two datasets. The first one is localgovFAQ, a dataset we construct in a Japanese administrative municipality domain. The second is StackExchange dataset, which is the public dataset in English. We demonstrate that our proposed method outperforms baseline methods on these datasets.",
        "entry_id": "http://arxiv.org/abs/1905.02851v2",
        "pub_date": "2019-05-08",
        "translated_summary": "常见问题解答检索是一项重要任务，其目标是根据用户查询从数据库中检索出相应的问题-答案对。我们提出了一种FAQ检索系统，该系统同时考虑用户查询与问题的相似度，以及查询与答案的相关性。尽管构建标注数据进行训练是FAQ检索的常用方法，但标注成本较高。因此，我们采用传统无监督信息检索系统来计算查询与问题的相似度。另一方面，查询与答案的相关性可以通过使用FAQ数据库中的问答对进行学习。我们采用最新提出的BERT模型进行相关性计算。由于FAQ页面中的问答对数量不足以训练模型，我们通过利用与目标FAQ集相似的FAQ集合来解决这一问题。我们在两个数据集上评估了该方法：第一个是本地政务FAQ数据集，这是我们在日本行政市政领域构建的数据集；第二个是StackExchange公共英文数据集。实验证明，我们提出的方法在这两个数据集上均优于基线方法。"
    },
    {
        "title": "Who wrote this book? A challenge for e-commerce",
        "summary": "Modern e-commerce catalogs contain millions of references, associated with textual and visual information that is of paramount importance for the products to be found via search or browsing. Of particular significance is the book category, where the author name(s) field poses a significant challenge. Indeed, books written by a given author (such as F. Scott Fitzgerald) might be listed with different authors' names in a catalog due to abbreviations and spelling variants and mistakes, among others. To solve this problem at scale, we design a composite system involving open data sources for books as well as machine learning components leveraging deep learning-based techniques for natural language processing. In particular, we use Siamese neural networks for an approximate match with known author names, and direct correction of the provided author's name using sequence-to-sequence learning with neural networks. We evaluate this approach on product data from the e-commerce website Rakuten France, and find that the top proposal of the system is the normalized author name with 72% accuracy.",
        "entry_id": "http://arxiv.org/abs/1905.01973v1",
        "pub_date": "2019-04-19",
        "translated_summary": "现代电子商务目录包含数百万种商品，其关联的文本与视觉信息对于用户通过搜索或浏览找到产品至关重要。其中图书类目的作者名字段尤为特殊——同一作者（如F·斯科特·菲茨杰拉德）的著作可能因缩写、拼写变体或错误等原因在目录中呈现不同作者名称。为大规模解决该问题，我们设计了一套复合系统：既整合图书开放数据源，又采用基于深度学习的自然语言处理技术。具体通过孪生神经网络实现与已知作者名的近似匹配，并利用神经网络序列到序列学习直接校正现有作者名。基于法国乐天电商平台产品数据的测试显示，该系统首选建议的标准化作者名准确率达72%。"
    },
    {
        "title": "A Content-Based Approach to Email Triage Action Prediction: Exploration and Evaluation",
        "summary": "Email has remained a principal form of communication among people, both in enterprise and social settings. With a deluge of emails crowding our mailboxes daily, there is a dire need of smart email systems that can recover important emails and make personalized recommendations. In this work, we study the problem of predicting user triage actions to incoming emails where we take the reply prediction as a working example. Different from existing methods, we formulate the triage action prediction as a recommendation problem and focus on the content-based approach, where the users are represented using the content of current and past emails. We also introduce additional similarity features to further explore the affinities between users and emails. Experiments on the publicly available Avocado email collection demonstrate the advantages of our proposed recommendation framework and our method is able to achieve better performance compared to the state-of-the-art deep recommendation methods. More importantly, we provide valuable insight into the effectiveness of different textual and user representations and show that traditional bag-of-words approaches, with the help from the similarity features, compete favorably with the more advanced neural embedding methods.",
        "entry_id": "http://arxiv.org/abs/1905.01991v1",
        "pub_date": "2019-04-30",
        "translated_summary": "电子邮件始终是企业和社交场景中人们沟通的主要方式。面对每日涌入收件箱的海量邮件，智能邮件系统亟需实现重要邮件恢复与个性化推荐功能。本文以回复预测为例，研究用户对接收邮件的分类行为预测问题。与现有方法不同，我们将分类行为预测构建为推荐问题，聚焦于基于内容的研究方法——通过当前及历史邮件内容构建用户画像。通过引入额外相似性特征，进一步挖掘用户与邮件之间的关联性。在公开的Avocado邮件数据集上的实验表明，我们提出的推荐框架具有显著优势，相较当前最先进的深度推荐方法能获得更优性能。更重要的是，我们揭示了不同文本表征和用户表征方法的有效性，并证明传统词袋模型在相似性特征辅助下，可与更先进的神经嵌入方法相媲美。"
    },
    {
        "title": "Deep Landscape Forecasting for Real-time Bidding Advertising",
        "summary": "The emergence of real-time auction in online advertising has drawn huge attention of modeling the market competition, i.e., bid landscape forecasting. The problem is formulated as to forecast the probability distribution of market price for each ad auction. With the consideration of the censorship issue which is caused by the second-price auction mechanism, many researchers have devoted their efforts on bid landscape forecasting by incorporating survival analysis from medical research field. However, most existing solutions mainly focus on either counting-based statistics of the segmented sample clusters, or learning a parameterized model based on some heuristic assumptions of distribution forms. Moreover, they neither consider the sequential patterns of the feature over the price space. In order to capture more sophisticated yet flexible patterns at fine-grained level of the data, we propose a Deep Landscape Forecasting (DLF) model which combines deep learning for probability distribution forecasting and survival analysis for censorship handling. Specifically, we utilize a recurrent neural network to flexibly model the conditional winning probability w.r.t. each bid price. Then we conduct the bid landscape forecasting through probability chain rule with strict mathematical derivations. And, in an end-to-end manner, we optimize the model by minimizing two negative likelihood losses with comprehensive motivations. Without any specific assumption for the distribution form of bid landscape, our model shows great advantages over previous works on fitting various sophisticated market price distributions. In the experiments over two large-scale real-world datasets, our model significantly outperforms the state-of-the-art solutions under various metrics.",
        "entry_id": "http://arxiv.org/abs/1905.03028v2",
        "pub_date": "2019-05-07",
        "translated_summary": "在线广告实时竞价的出现，使得市场竞争建模（即竞价环境预测）受到广泛关注。该问题可表述为预测每次广告竞价市场价格的概率分布。针对第二价格拍卖机制导致的数据截断问题，众多研究者借鉴医学领域的生存分析方法开展竞价环境预测研究。然而现有解决方案大多聚焦于分段样本群的计数统计，或基于分布形式的启发式假设学习参数化模型，且均未考虑特征在价格空间上的序列模式。为在细粒度数据层面捕捉更复杂灵活的模式，我们提出深度融合竞价环境预测模型，将深度学习与生存分析相结合进行概率分布预测与截断数据处理。具体而言，我们利用循环神经网络灵活建模各出价价格对应的条件获胜概率，继而通过概率链式法则进行严格数学推导来实现竞价环境预测。以端到端方式，我们通过最小化两个具有综合动机的负似然损失函数来优化模型。该模型无需对竞价环境分布形式做特定假设，在拟合各类复杂市场价格分布方面较已有工作展现出显著优势。基于两个大规模真实数据集的实验表明，我们的模型在多项指标上均显著优于现有最优解决方案。"
    },
    {
        "title": "A Novel Fuzzy Search Approach over Encrypted Data with Improved Accuracy and Efficiency",
        "summary": "As cloud computing becomes prevalent in recent years, more and more enterprises and individuals outsource their data to cloud servers. To avoid privacy leaks, outsourced data usually is encrypted before being sent to cloud servers, which disables traditional search schemes for plain text. To meet both end of security and searchability, search-supported encryption is proposed. However, many previous schemes suffer severe vulnerability when typos and semantic diversity exist in query requests. To overcome such flaw, higher error-tolerance is always expected for search-supported encryption design, sometimes defined as 'fuzzy search'. In this paper, we propose a new scheme of multi-keyword fuzzy search over encrypted and outsourced data. Our approach introduces a new mechanism to map a natural language expression into a word-vector space. Compared with previous approaches, our design shows higher robustness when multiple kinds of typos are involved. Besides, our approach is enhanced with novel data structures to improve search efficiency. These two innovations can work well for both accuracy and efficiency. Moreover, these designs will not hurt the fundamental security. Experiments on a real-world dataset demonstrate the effectiveness of our proposed approach, which outperforms currently popular approaches focusing on similar tasks.",
        "entry_id": "http://arxiv.org/abs/1904.12111v2",
        "pub_date": "2019-04-27",
        "translated_summary": "近年来，随着云计算的普及，越来越多的企业和个人将数据外包至云服务器。为防止隐私泄露，外包数据通常会在上传至云端前进行加密处理，但这使得传统明文搜索方案无法适用。为实现安全性与可搜索性的统一，可搜索加密技术应运而生。然而，现有方案在查询请求存在拼写错误或语义多样性时存在明显缺陷。为克服这一不足，可搜索加密设计需要具备更高的容错能力，即实现\"模糊搜索\"。本文提出一种支持多关键词模糊搜索的加密外包数据查询方案。该方案创新性地通过词向量空间映射自然语言表达，相较于现有方案，在应对多种拼写错误时展现出更强的鲁棒性。同时，我们采用新型数据结构提升搜索效率，这两项创新在保证准确率的同时显著提升性能，且不会损害基础安全性。在真实数据集上的实验表明，本方案在同等任务中的表现优于当前主流方案。"
    },
    {
        "title": "Topic Classification Method for Analyzing Effect of eWOM on Consumer Game Sales",
        "summary": "Electronic word-of-mouth (eWOM) has become an important resource for the analysis of marketing research. In this study, in order to analyze user needs for consumer game software, we focus on tweet data. And we proposed topic extraction method using entropy-based feature selection based feature expansion. We also applied it to the classification of the data extracted from tweet data by using SVM. As a result, we achieved a 0.63 F-measure.",
        "entry_id": "http://arxiv.org/abs/1904.13213v1",
        "pub_date": "2019-04-23",
        "translated_summary": "电子口碑已成为营销调研分析的重要资源。为探究消费者对游戏软件的需求特性，本研究以推文数据为分析对象，提出基于熵特征选择与特征扩展的主题挖掘方法，并采用支持向量机对推文数据进行分类处理。实验结果显示，该方法的F值评估指标达到0.63。"
    },
    {
        "title": "Advanced Customer Activity Prediction based on Deep Hierarchic Encoder-Decoders",
        "summary": "Product recommender systems and customer profiling techniques have always been a priority in online retail. Recent machine learning research advances and also wide availability of massive parallel numerical computing has enabled various approaches and directions of recommender systems advancement. Worth to mention is the fact that in past years multiple traditional \"offline\" retail business are gearing more and more towards employing inferential and even predictive analytics both to stock-related problems such as predictive replenishment but also to enrich customer interaction experience. One of the most important areas of recommender systems research and development is that of Deep Learning based models which employ representational learning to model consumer behavioral patterns. Current state of the art in Deep Learning based recommender systems uses multiple approaches ranging from already classical methods such as the ones based on learning product representation vector, to recurrent analysis of customer transactional time-series and up to generative models based on adversarial training. Each of these methods has multiple advantages and inherent weaknesses such as inability of understanding the actual user-journey, ability to propose only single product recommendation or top-k product recommendations without prediction of actual next-best-offer. In our work we will present a new and innovative architectural approach of applying state-of-the-art hierarchical multi-module encoder-decoder architecture in order to solve several of current state-of-the-art recommender systems issues. Our approach will also produce by-products such as product need-based segmentation and customer behavioral segmentation - all in an end-to-end trainable approach. Finally, we will present a couple methods that solve known retail & distribution pain-points based on the proposed architecture.",
        "entry_id": "http://arxiv.org/abs/1904.07687v4",
        "pub_date": "2019-04-11",
        "translated_summary": "商品推荐系统与用户画像技术始终是在线零售领域的关注焦点。随着机器学习研究的最新进展以及大规模并行数值计算的广泛普及，推荐系统的发展呈现出多元化趋势。值得注意的是，近年来众多传统线下零售企业正越来越多地运用推断性甚至预测性分析技术，不仅将其应用于库存管理（如预测性补货），更致力于提升客户交互体验。基于深度学习的推荐模型通过表征学习来构建消费者行为模式，已成为该领域的重要研究方向。当前最先进的深度学习推荐系统融合了多种技术路径：既包含基于商品表征向量学习的经典方法，也涵盖客户交易时间序列的循环分析，更延伸至基于对抗训练的生成模型。这些方法虽各具优势，却也存在固有缺陷——例如无法真正理解用户行为路径、仅能推荐单一商品或Top-K商品列表而无法预测真正意义上的\"下一个最佳优惠\"。本研究提出了一种创新的层次化多模块编码器-解码器架构，旨在解决现有推荐系统的若干痛点。该端到端可训练架构不仅能实现核心推荐功能，还将自然衍生出基于需求的产品细分和客户行为细分等副产品。最后，我们将基于该架构提出若干解决零售分销领域典型痛点的方法论。"
    },
    {
        "title": "Short Text Topic Modeling Techniques, Applications, and Performance: A Survey",
        "summary": "Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.",
        "entry_id": "http://arxiv.org/abs/1904.07695v1",
        "pub_date": "2019-04-13",
        "translated_summary": "短文本分析旨在推断出具有判别力且连贯的潜在主题，这是一项关键的基础性任务，因为众多实际应用都需要对短文本进行语义理解。基于词语共现的传统长文本主题建模算法（如PLSA和LDA）难以有效解决该问题，因为短文本中可用的词语共现信息极其有限。因此，旨在克服短文本稀疏性问题的主题建模技术近年来备受机器学习研究界关注。本文系统综述了文献中提出的各类短文本主题建模方法，将其划分为基于狄利克雷多项混合、全局词语共现和自聚合三大类方法，通过典型算法示例分析其在各任务中的性能表现。我们开发了首个综合性开源工具包STTM（基于Java语言），该工具集成了所有综述算法与基准数据集，采用统一接口以促进该研究领域新方法的拓展。最后，我们在多个真实数据集上评估了这些前沿方法，通过纵向对比与长文本主题建模算法的横向比较，全面验证其性能表现。"
    },
    {
        "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
        "summary": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
        "entry_id": "http://arxiv.org/abs/2511.10492v1",
        "pub_date": "2025-11-13",
        "translated_summary": "在推荐系统中，除了准确性之外，针对多样性、新颖性和个性化等目标的优化对长期用户满意度至关重要。工业实践者已积累了大量结构化领域知识，我们将其称为\"人类先验\"（如物品分类体系、时序模式）。这类知识通常通过排名阶段或后排名阶段的后期调整来应用，但这种方法始终与核心模型学习相分离——随着行业向端到端生成式推荐基础模型转型，这种分离尤为不利。另一方面，许多针对超准确性目标的方法往往需要针对特定架构进行修改，并以完全无监督的方式学习用户意图，从而丢弃了这些宝贵的人类先验。\n\n我们提出了一种与主干模型无关的框架，将多年实践积累的人类先验直接整合到生成式推荐器的端到端训练中，而非抛弃这些知识。受高效大语言模型解码策略启发，我们通过轻量级的先验条件适配头，引导模型沿着人类可理解的维度（如交互类型、长短期兴趣）解耦用户意图。同时引入了分层组合策略来建模不同先验类型间的复杂交互。在三个大规模数据集上的实验表明，我们的方法显著提升了准确性及超准确性目标。研究还证实，人类先验能使主干模型更有效地利用更长上下文和更大模型规模。"
    },
    {
        "title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.",
        "entry_id": "http://arxiv.org/abs/2511.10277v1",
        "pub_date": "2025-11-13",
        "translated_summary": "大型语言模型在生成类人文本方面展现出卓越能力，但在计算机游戏对话系统中的应用仍存在局限。这主要源于其较高的硬件需求、延迟限制，以及游戏场景中需保持明确知识边界的必要性。本文提出一种模块化非玩家角色对话系统，通过微调小型语言模型来编码特定角色特征，并与可实时切换的记忆模块相结合。这些记忆模块能够保存角色专属的对话上下文和世界知识，无需在游戏过程中重新训练或加载模型，即可实现富有表现力的交互和长期记忆功能。我们使用三种开源小型语言模型进行系统评估：DistilGPT-2、TinyLlama-1.1B-Chat和Mistral-7B-Instruct，这些模型基于合成的角色对齐数据训练，并在消费级硬件上进行基准测试。虽然该研究受游戏应用驱动，但其模块化设计和角色驱动的记忆架构，对于需要表现力丰富、可扩展且具备深度记忆的对话代理场景具有广泛适用潜力，例如虚拟助手、客服机器人或交互式教育系统等领域。"
    },
    {
        "title": "GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation",
        "summary": "As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR.",
        "entry_id": "http://arxiv.org/abs/2511.10138v1",
        "pub_date": "2025-11-13",
        "translated_summary": "作为连接用户与商业内容的智能枢纽，广告推荐系统在数字经济的信息流通与价值创造中占据核心地位。然而现有多阶段广告推荐系统存在目标错位与误差累积问题，难以实现全局最优；而统一生成式推荐模型在实际工业应用中仍面临诸多挑战。为此，我们提出GPR（生成式预训练推荐系统），首次通过单模型框架将广告推荐重新定义为端到端生成任务，以统一生成范式替代传统级联架构。为实现该框架，我们在统一表征、网络架构和训练策略三大维度实现创新突破：首先设计面向广告场景的统一输入范式与令牌化方法，将广告与自然内容映射至共享的多层级语义ID空间，增强异构数据的语义对齐与建模一致性；其次构建异质层级解码器（HHD），通过双流解码架构分离用户意图建模与广告生成路径，在保持强大建模能力的同时实现训练效率与推理灵活性的平衡；最终提出融合多令牌预测（MTP）、价值感知微调与层级增强策略优化（HEPO）算法的多阶段联合训练策略，形成统一兴趣建模、价值对齐与策略优化的完整生成式推荐链路。GPR已在腾讯微信视频号广告系统全面部署，在GMV、CTCVR等关键业务指标上取得显著提升。"
    },
    {
        "title": "Practical RAG Evaluation: A Rarity-Aware Set-Based Metric and Cost-Latency-Quality Trade-offs",
        "summary": "This paper addresses the guessing game in building production RAG. Classical rank-centric IR metrics (nDCG/MAP/MRR) are a poor fit for RAG, where LLMs consume a set of passages rather than a browsed list; position discounts and prevalence-blind aggregation miss what matters: whether the prompt at cutoff K contains the decisive evidence. Second, there is no standardized, reproducible way to build and audit golden sets. Third, leaderboards exist but lack end-to-end, on-corpus benchmarking that reflects production trade-offs. Fourth, how state-of-the-art embedding models handle proper-name identity signals and conversational noise remains opaque. To address these, we contribute: (1) RA-nWG@K, a rarity-aware, per-query-normalized set score, and operational ceilings via the pool-restricted oracle ceiling (PROC) and the percentage of PROC (%PROC) to separate retrieval from ordering headroom within a Cost-Latency-Quality (CLQ) lens; (2) rag-gs (MIT), a lean golden-set pipeline with Plackett-Luce listwise refinement whose iterative updates outperform single-shot LLM ranking; (3) a comprehensive benchmark on a production RAG (scientific-papers corpus) spanning dense retrieval, hybrid dense+BM25, embedding models and dimensions, cross-encoder rerankers, ANN (HNSW), and quantization; and (4) targeted diagnostics that quantify proper-name identity signal and conversational-noise sensitivity via identity-destroying and formatting ablations. Together, these components provide practitioner Pareto guidance and auditable guardrails to support reproducible, budget/SLA-aware decisions.",
        "entry_id": "http://arxiv.org/abs/2511.09545v1",
        "pub_date": "2025-11-12",
        "translated_summary": "本文针对生产环境RAG构建中的评估难题展开研究。传统以排序为核心的信息检索指标（nDCG/MAP/MRR）与RAG场景存在根本性错配——大语言模型处理的是段落集合而非浏览列表，位置衰减和忽略证据分布的聚合方式无法捕捉核心问题：截断点K处的提示是否包含决定性证据。其次，当前缺乏标准化、可复现的黄金集构建与审计方法。再者，现有排行榜缺少反映生产环境权衡的端到端全库基准。最后，前沿嵌入模型如何处理专有名词标识信号与会话噪声仍不透明。为此我们提出：(1) RA-nWG@K——一种稀有度感知的查询归一化集合评分指标，通过池限制理论上限(PROC)及其百分比(%PROC)在成本-延迟-质量(CLQ)框架下区分检索能力与排序潜力；(2) rag-gs(MIT授权)——采用Plackett-Luce列表优化的轻量级黄金集流程，其迭代更新优于单次LLM排序；(3) 基于科学论文库的生产级RAG综合基准，覆盖稠密检索、混合检索、嵌入模型与维度、交叉编码器重排序、近似最近邻(HNSW)及量化技术；(4) 通过身份标识破坏与格式消融实验，量化专有名词标识信号处理能力与会话噪声敏感性。这些组件共同为实践者提供帕累托决策指导与可审计防护，支持符合预算与服务等级协议的可复现决策。"
    },
    {
        "title": "Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction",
        "summary": "Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR community. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simulation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query predictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench.",
        "entry_id": "http://arxiv.org/abs/2511.09329v1",
        "pub_date": "2025-11-12",
        "translated_summary": "由于缺乏成熟的衡量标准与基准，用户模拟验证始终是一项困难的任务，这使得评估模拟器是否准确反映真实用户行为充满挑战。作为SIGIR 2025 Sim4IA研讨会微共享任务环节的一部分，我们推出首个信息检索领域针对下一查询与对话预测的仿真基准套件Sim4IA-Bench。该套件中的数据集包含来自CORE搜索引擎的160个真实搜索会话，其中70个会话提供多达62次模拟运行数据，分为任务A与任务B，分别对应不同方法对用户后续搜索查询或对话的预测。Sim4IA-Bench为评估比较用户模拟方法及开发新型模拟器效度指标奠定了基础。尽管规模适中，但这是首个公开连接真实搜索会话与模拟下一查询预测的基准套件。除作为下一查询预测测试平台外，它还可支持查询重构行为、意图漂移及交互感知检索评估的探索性研究。我们还为此任务引入新的下一查询预测评估指标。通过开源此套件，我们旨在推动可复现研究，并促进面向信息访问的逼真可解释用户模拟工作：https://github.com/irgroup/Sim4IA-Bench。"
    },
    {
        "title": "NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning",
        "summary": "Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality-level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.",
        "entry_id": "http://arxiv.org/abs/2511.09250v1",
        "pub_date": "2025-11-12",
        "translated_summary": "受脑启发人工智能领域的最新进展试图利用CLIP等多模态模型将神经信号与视觉语义对齐。然而现有方法通常将CLIP视为静态特征提取器，忽视了其对神经表征的适应性以及EEG-图像对齐中固有的生理-符号鸿沟。为解决这些挑战，我们提出NeuroCLIP——一个专为EEG-图像对比学习设计的提示调优框架。该框架包含三项核心创新：(1) 设计双流视觉嵌入管道，结合动态过滤与令牌级融合生成实例级自适应提示，通过图像内容指导图像块嵌入令牌的调整，实现神经约束下的视觉表征细粒度调制；(2) 首次将视觉提示令牌引入EEG-图像对齐，作为全局模态级提示与实例级调整协同工作，这些令牌被嵌入Transformer架构以促进神经感知的全局适应与参数优化；(3) 受人类视觉编码神经科学原理启发，提出改进的对比损失函数，更精准建模EEG信号中的语义歧义与跨模态噪声。在THINGS-EEG2数据集上，NeuroCLIP在零样本图像检索任务中达到63.2%的Top-1准确率，较此前最佳方法提升12.3%，并在跨被试条件下展现出强大泛化能力（Top-1提升4.6%），彰显了生理感知提示调优在连接大脑信号与视觉语义方面的巨大潜力。"
    },
    {
        "title": "Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning",
        "summary": "Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.",
        "entry_id": "http://arxiv.org/abs/2511.09109v2",
        "pub_date": "2025-11-12",
        "translated_summary": "检索增强生成技术虽已被证实能有效缓解大语言模型的幻觉问题，但在复杂多步推理场景中的效能仍显不足。近期研究将基于搜索的交互机制融入RAG，实现了实时检索的迭代推理。然而现有方法多依赖结果监督，未能对中间步骤提供明确指导，易导致奖励破解和回答质量下降。我们提出双向检索增强推理框架Bi-RAR，通过前向与后向联合评估每个推理步骤。为衡量各步骤的信息完备性，基于柯氏复杂度构建了双向信息距离指标，并借助语言模型生成概率进行近似计算。该量化方法既能评估当前推理与答案的距离，又可衡量其对问题的解答程度。在此双向信号优化方面，采用具有级联奖励结构的多目标强化学习框架，重点强化早期轨迹对齐。在七个问答基准测试中的实证结果表明，Bi-RAR不仅超越现有方法，还能在训练与推理过程中实现与搜索引擎的高效交互推理。"
    },
    {
        "title": "Efficient Model-Agnostic Continual Learning for Next POI Recommendation",
        "summary": "Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption.",
        "entry_id": "http://arxiv.org/abs/2511.08941v1",
        "pub_date": "2025-11-12",
        "translated_summary": "下一兴趣点推荐通过用户历史签到记录预测其下一个目的地，从而提升个性化基于位置的服务质量。然而现有方法大多依赖静态数据集和固定模型，难以适应用户行为随时间的动态变化。为突破这一局限，我们探索名为\"持续下一兴趣点推荐\"的新任务，使模型能通过持续更新机制动态适应用户兴趣的演变。该任务面临双重挑战：既要捕捉用户行为的动态变化，又需保留已学知识。同时，在实际部署中还需确保更新时效与内存使用效率。为此，我们提出GIRAM框架——基于生成式关键字的兴趣检索与自适应建模，这个高效且模型无关的框架将情境感知的持续兴趣与近期兴趣相融合。GIRAM包含四个核心组件：(1)用于保存历史偏好的兴趣记忆库；(2)实现统一兴趣关键字表征的情境感知编码模块；(3)基于生成式关键字的检索模块，用于识别多样化相关持续兴趣；(4)自适应兴趣更新与融合模块，负责更新兴趣记忆库并平衡持续兴趣与近期兴趣。该框架可与现有下一兴趣点推荐模型无缝集成。在三个真实数据集上的实验表明，GIRAM在保持高效更新时间与内存消耗的同时，持续优于现有最优方法。"
    },
    {
        "title": "Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding",
        "summary": "Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.",
        "entry_id": "http://arxiv.org/abs/2511.08480v1",
        "pub_date": "2025-11-11",
        "translated_summary": "视觉语言模型通过获取可迁移的语义嵌入推动多模态表征学习，从而显著提升跨模态检索、聚类与分类等视觉语言任务的性能。理想的嵌入向量应全面保留输入语义内容，同时突出下游任务所需的判别性特征。最新研究表明，通过大规模对比学习可将VLMs转化为具有竞争力的嵌入模型，实现两个互补目标的同步优化。我们认为上述两个目标可解耦：对输入的全面理解有助于嵌入模型通过对比学习在下游任务中获得更优表现。本文提出压缩式预训练阶段CoMa，作为对比学习的热身阶段。实验表明，仅需少量预训练数据即可将VLM转化为具有竞争力的嵌入模型。CoMa在MMEB基准测试中实现了同体量VLMs的最优性能，在效率与效果上达成双重突破。"
    },
    {
        "title": "Advancing Scientific Knowledge Retrieval and Reuse with a Novel Digital Library for Machine-Readable Knowledge",
        "summary": "Digital libraries for research, such as the ACM Digital Library or Semantic Scholar, do not enable the machine-supported, efficient reuse of scientific knowledge (e.g., in synthesis research). This is because these libraries are based on document-centric models with narrative text knowledge expressions that require manual or semi-automated knowledge extraction, structuring, and organization. We present ORKG reborn, an emerging digital library that supports finding, accessing, and reusing accurate, fine-grained, and reproducible machine-readable expressions of scientific knowledge that relate scientific statements and their supporting evidence in terms of data and code. The rich expressions of scientific knowledge are published as reborn (born-reusable) articles and provide novel possibilities for scientific knowledge retrieval, for instance by statistical methods, software packages, variables, or data matching specific constraints. We describe the proposed system and demonstrate its practical viability and potential for information retrieval in contrast to state-of-the-art digital libraries and document-centric scholarly communication using several published articles in research fields ranging from computer science to soil science. Our work underscores the enormous potential of scientific knowledge databases and a viable approach to their construction.",
        "entry_id": "http://arxiv.org/abs/2511.08476v1",
        "pub_date": "2025-11-11",
        "translated_summary": "诸如ACM数字图书馆或语义学者（Semantic Scholar）等研究型数字图书馆，尚无法实现科学知识的机器支持高效复用（例如在综述研究中）。这是因为这些图书馆基于以文档为中心的模型，其叙述性文本的知识表达方式需要人工或半自动化的知识提取、结构化与组织。我们推出“重生版开放研究知识图谱”（ORKG reborn），这一新兴数字图书馆支持查找、访问并复用精确、细粒度、可重现的机器可读科学知识表达，这些表达通过数据与代码将科学论断及其支撑证据相互关联。这种丰富的科学知识表达以“重生”（即可重复使用）论文的形式发布，为科学知识检索提供了全新可能，例如通过统计方法、软件包、变量或符合特定约束条件的数据进行检索。我们详细描述了该系统的设计，并通过计算机科学到土壤科学等多个研究领域已发表论文的实例，对比现有顶尖数字图书馆和以文档为中心的学术交流模式，论证了该系统在信息检索方面的实际可行性与潜力。我们的工作彰显了科学知识数据库的巨大潜力，并为其构建提供了可行路径。"
    },
    {
        "title": "Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents",
        "summary": "Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a \"see-saw\" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \\textbf{HID} (\\textbf{H}ybrid \\textbf{I}ntent-based \\textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional \"see-saw\" into \"win-win\" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \\textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \\textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \\textit{diversity} and \\textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems.",
        "entry_id": "http://arxiv.org/abs/2511.08378v1",
        "pub_date": "2025-11-11",
        "translated_summary": "基于会话的推荐旨在根据匿名用户的交互会话预测其下一次交互行为。在实际推荐场景中，低曝光商品构成了交互行为的主体，这种长尾分布严重影响了推荐多样性。现有方法试图通过提升尾部商品曝光来解决该问题，却导致推荐准确率下降，呈现出长尾性能与准确率之间的“跷跷板效应”。我们认为这一矛盾源于尾部商品中存在的会话无关噪声，而现有长尾处理方法未能有效识别和约束此类噪声。为解决这一根本矛盾，我们提出\\textbf{HID}框架——一种即插即用的混合意图双约束框架，通过引入面向长尾性能和准确率的混合意图双约束，将传统的“跷跷板”关系转化为“共赢”关系。该框架包含两大核心创新：(一) \\textit{混合意图学习}，通过采用属性感知谱聚类重构商品-意图映射关系，重新构建意图提取策略。此外，通过为每个会话分配目标意图和噪声意图，实现会话无关噪声的甄别；(二) \\textit{意图约束损失函数}，融合了面向\\textit{多样性}和\\textit{准确率}的两个创新约束范式，用以规范商品和会话的表示学习过程。通过严格的理论推导，这两个目标被统一到单个训练损失函数中。在多个SBR模型和数据集上的大量实验表明，HID能同时提升长尾性能和推荐准确率，在长尾推荐系统中确立了新的性能标杆。"
    },
    {
        "title": "TurkEmbed: Turkish Embedding Model on NLI & STS Tasks",
        "summary": "This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.",
        "entry_id": "http://arxiv.org/abs/2511.08376v1",
        "pub_date": "2025-11-11",
        "translated_summary": "本文介绍了TurkEmbed——一种新型土耳其语嵌入模型，其设计目标是在自然语言推理（NLI）和语义文本相似度（STS）任务中超越现有模型。当前土耳其语嵌入模型普遍依赖机器翻译数据集，这可能限制其准确性和语义理解能力。TurkEmbed通过融合多样化数据集与先进训练技术（包括套娃表示学习），实现了更鲁棒精准的嵌入表示。该方法使模型能适配各类资源受限环境，并提供更快速的编码能力。我们在土耳其语STS-b-TR数据集上采用皮尔逊与斯皮尔曼相关指标进行评估，结果显示该模型在语义相似度任务中取得显著提升。此外，TurkEmbed在All-NLI-TR和STS-b-TR基准测试中超越当前最优模型Emrecan，实现了1-4%的性能提升。TurkEmbed有望通过提供更精细的语言理解能力并推动下游应用发展，从而增强土耳其语自然语言处理生态系统。"
    },
    {
        "title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress",
        "summary": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.",
        "entry_id": "http://arxiv.org/abs/2511.08325v1",
        "pub_date": "2025-11-11",
        "translated_summary": "尽管大型语言模型（LLM）发展迅速，但在网络购物、浏览器导航等多轮决策任务（即智能体任务）中仍面临挑战。这类任务要求根据环境反馈进行一系列智能决策。以往针对LLM智能体的研究通常依赖精心设计的提示工程或通过专家轨迹进行微调来提升性能。本研究另辟蹊径：探索构建过程奖励模型（PRM）来评估每个决策步骤并指导智能体的决策过程。与LLM推理中每一步都基于正确性评分不同，智能体任务中的行为没有绝对的正确性标准，而应通过其与目标的接近程度及已取得的进展来评估。基于这一洞见，我们提出了面向智能体任务的重新定义的过程奖励模型AgentPRM，该模型能同时捕捉序列决策间的相互关联及其对最终目标的贡献，从而实现更优的进度跟踪与探索-利用平衡。为高效获取训练AgentPRM所需的标注数据，我们采用基于时序差分（TD）的估计方法结合广义优势估计（GAE），该方法被证明比现有方法更具样本效率。跨多个智能体任务的广泛实验表明，AgentPRM的计算效率较基线方法提升超过8倍，且在扩展测试时计算资源时展现出稳健的性能提升。此外，我们通过详细分析揭示了该方法的作用机制，并提供了更多洞见，例如将AgentPRM应用于LLM智能体的强化学习。"
    },
    {
        "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System",
        "summary": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag",
        "entry_id": "http://arxiv.org/abs/2511.08181v1",
        "pub_date": "2025-11-11",
        "translated_summary": "当前，推荐系统领域正致力于通过利用模态信息或引入基于大语言模型卓越推理能力的智能体概念，来缓解冷启动场景下的局限性。与此同时，由于餐饮领域独特的数据属性与关系特征，传统餐饮推荐系统多采用知识图谱与本体论概念。在此背景下，我们提出MARC——一个基于智能体检索增强生成的多模态多任务鸡尾酒推荐系统，该系统在冷启动条件下利用图数据库实现推荐。该体系通过任务识别路由器和反思机制两大核心流程，生成高质量且符合情境的推荐结果。我们通过处理Kaggle平台的鸡尾酒数据构建图数据库，并采用200道人工编制的问题进行效果评估。评估过程综合运用大语言模型即评判与人工评估两种方式，结果表明基于图数据库生成的答案质量显著优于简单向量数据库方案。代码已开源：https://github.com/diddbwls/cocktail_rec_agentrag"
    },
    {
        "title": "DiffuGR: Generative Document Retrieval with Diffusion Language Models",
        "summary": "Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.08150v1",
        "pub_date": "2025-11-11",
        "translated_summary": "生成式检索将文档检索重新定义为基于序列的文档标识符生成任务，通过模型参数记忆文档，实现无需显式索引的端到端检索。现有生成式检索方法均基于自回归生成模型，即从左到右顺序生成标识符。然而这类方法存在两大局限：（1）文档标识符生成与自然语言生成存在本质差异，早期生成的错误标识符会导致完全错误的检索结果；（2）无法动态平衡检索效率与准确率之间的权衡关系，而这在实际应用中至关重要。为克服这些局限，我们提出基于扩散语言模型的生成式检索方法DiffuGR。该方法将文档标识符生成建模为离散扩散过程：训练阶段通过随机掩码破坏文档标识符，并学习扩散语言模型在检索感知目标下恢复被破坏的标识符；推理阶段则尝试并行生成标识符，并通过可控的去噪步骤进行优化。相较于传统的自回归解码机制，DiffuGR首创了先生成高置信度标识符、再通过扩散去噪优化的新范式。此外，DiffuGR还能显式控制质量与延迟的权衡关系。在标准检索数据集上的大量实验表明，DiffuGR在保持与强基线自回归方法竞争力的同时，可通过调整去噪次数实现灵活的精度-速度权衡。我们的研究结果证明，非自回归扩散模型是生成式文档检索领域具有实用价值的有效替代方案。"
    },
    {
        "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives",
        "summary": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.",
        "entry_id": "http://arxiv.org/abs/2511.08029v1",
        "pub_date": "2025-11-11",
        "translated_summary": "硬负样本对于训练高效检索模型至关重要。硬负样本挖掘通常依赖于使用交叉编码器或基于余弦距离等相似性度量的静态嵌入模型对文档进行排序。在生物医学和科学领域，由于难以区分源文档与硬负样本文档，硬负样本挖掘变得颇具挑战。然而，被引文献天然与源文档具有上下文关联性但并非重复内容，这使其成为理想的硬负样本。本研究提出BiCA：基于引文感知硬负样本的生物医学稠密检索方法，通过利用20,000篇PubMed文献中的引文链接进行硬负样本挖掘，以改进领域专用的小型稠密检索器。我们使用这些引文指导的负样本对GTE_small和GTE_Base模型进行微调，在BEIR数据集的内域和外域任务中通过nDCG@10指标观察到零样本稠密检索的持续提升，并在LoTTE数据集的长尾主题上使用Success@5指标超越基线。我们的研究结果揭示了利用文档链接结构生成高信息量负样本的潜力，通过最小化微调即可实现最先进性能，为高数据效率的领域自适应开辟了新路径。"
    },
    {
        "title": "From IDs to Semantics: A Generative Framework for Cross-Domain Recommendation with Adaptive Semantic Tokenization",
        "summary": "Cross-domain recommendation (CDR) is crucial for improving recommendation accuracy and generalization, yet traditional methods are often hindered by the reliance on shared user/item IDs, which are unavailable in most real-world scenarios. Consequently, many efforts have focused on learning disentangled representations through multi-domain joint training to bridge the domain gaps. Recent Large Language Model (LLM)-based approaches show promise, they still face critical challenges, including: (1) the \\textbf{item ID tokenization dilemma}, which leads to vocabulary explosion and fails to capture high-order collaborative knowledge; and (2) \\textbf{insufficient domain-specific modeling} for the complex evolution of user interests and item semantics. To address these limitations, we propose \\textbf{GenCDR}, a novel \\textbf{Gen}erative \\textbf{C}ross-\\textbf{D}omain \\textbf{R}ecommendation framework. GenCDR first employs a \\textbf{Domain-adaptive Tokenization} module, which generates disentangled semantic IDs for items by dynamically routing between a universal encoder and domain-specific adapters. Symmetrically, a \\textbf{Cross-domain Autoregressive Recommendation} module models user preferences by fusing universal and domain-specific interests. Finally, a \\textbf{Domain-aware Prefix-tree} enables efficient and accurate generation. Extensive experiments on multiple real-world datasets demonstrate that GenCDR significantly outperforms state-of-the-art baselines. Our code is available in the supplementary materials.",
        "entry_id": "http://arxiv.org/abs/2511.08006v1",
        "pub_date": "2025-11-11",
        "translated_summary": "跨领域推荐对提升推荐准确性与泛化能力至关重要，但传统方法常受限于对共享用户/物品ID的依赖，而这类ID在现实场景中往往不可用。为此，研究者们多聚焦于通过多领域联合训练学习解耦表征以弥合领域差异。尽管基于大语言模型的新方法展现出潜力，仍面临两大核心挑战：（1）**物品ID标记化困境**，导致词表爆炸且无法捕获高阶协同知识；（2）对用户兴趣与物品语义复杂演化的**领域特异性建模不足**。针对这些局限，我们提出**GenCDR**——一个创新的**生成式跨领域推荐框架**。该框架首先通过**领域自适应标记化模块**，借助通用编码器与领域适配器间的动态路由生成解耦的物品语义ID；对称地，**跨领域自回归推荐模块**通过融合通用兴趣与领域特异性兴趣建模用户偏好；最后通过**领域感知前缀树**实现高效精准的生成。在多组真实数据集上的大量实验表明，GenCDR显著优于当前最先进的基线模型。代码已附于补充材料中。"
    },
    {
        "title": "TurkEmbed4Retrieval: Turkish Embedding Model for Retrieval Task",
        "summary": "In this work, we introduce TurkEmbed4Retrieval, a retrieval specialized variant of the TurkEmbed model originally designed for Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. By fine-tuning the base model on the MS MARCO TR dataset using advanced training techniques, including Matryoshka representation learning and a tailored multiple negatives ranking loss, we achieve SOTA performance for Turkish retrieval tasks. Extensive experiments demonstrate that our model outperforms Turkish colBERT by 19,26% on key retrieval metrics for the Scifact TR dataset, thereby establishing a new benchmark for Turkish information retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.07595v1",
        "pub_date": "2025-11-10",
        "translated_summary": "本研究推出了TurkEmbed4Retrieval——这是专为检索任务优化的TurkEmbed模型变体，原模型设计用于自然语言推理（NLI）与语义文本相似度（STS）任务。通过在MS MARCO TR数据集上采用先进训练技术对基础模型进行微调，包括套娃表示学习与定制化的多重负样本排序损失函数，我们实现了土耳其语检索任务的性能突破。大量实验表明，在Scifact TR数据集的关键检索指标上，本模型以19.26%的优势超越土耳其语colBERT模型，由此为土耳其语信息检索树立了全新基准。"
    },
    {
        "title": "Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models",
        "summary": "Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.",
        "entry_id": "http://arxiv.org/abs/2511.07581v1",
        "pub_date": "2025-11-10",
        "translated_summary": "高效的信息检索要求能够基于局部证据进行推理，并在信息出现时不断优化策略。然而现有方法存在明显局限：神经检索模型缺乏推理能力，大语言模型虽能提供语义深度但计算成本过高，而查询重写或分解方法仅能实现静态转换。这些方法均无法满足复杂用户查询所需的探索、反馈与修正的迭代动态过程。我们提出Orion训练框架，使轻量化模型（3.5-12亿参数）通过学习搜索策略实现迭代检索。该框架包含三大核心组件：（1）通过合成轨迹生成与监督微调激发模型多样化探索模式；（2）采用强化学习奖励有效的查询优化与回溯行为；（3）基于束搜索的推理算法，利用强化学习阶段习得的自反思能力。尽管仅使用3%的训练数据，我们的12亿参数模型在SciFact上达成77.6%成功率（优于原有72.6%），BRIGHT达到25.2%（原22.1%），NFCorpus提升至63.2%（原57.8%），并在FEVER、HotpotQA和MSMarco保持竞争力。在六项基准测试中有五项超越参数量200-400倍的检索模型。这表明当模型被训练具备搜索、反思与修正能力时，检索性能的提升可源于学习策略本身，而不仅依赖模型规模。"
    },
    {
        "title": "A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain",
        "summary": "Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.",
        "entry_id": "http://arxiv.org/abs/2511.07577v1",
        "pub_date": "2025-11-10",
        "translated_summary": "现有的检索增强生成系统通常采用集中式架构，这导致数据收集、整合与管理成本高昂，并引发隐私担忧。业界亟需一种去中心化的RAG系统，使基础模型能够直接利用数据所有者控制的信息源，同时确保数据所有者保持对数据的完全掌控。然而，去中心化架构面临关键挑战：大量独立数据源的可靠性差异显著，可能降低检索精度与响应质量。为此，我们提出的去中心化RAG系统创新性地引入了可靠性评分机制，该机制根据各数据源在生成响应过程中的贡献质量进行动态评估，并在检索时优先调用高质量数据源。为确保透明度与可信度，评分过程通过基于区块链的智能合约进行安全管理，建立可验证且防篡改的可靠性记录，无需依赖中心化机构。我们使用两个Llama模型（3B和8B参数）在模拟环境中对系统进行评估，该环境包含六个具有不同可靠性等级的数据源。在模拟真实世界不可靠数据环境时，本系统相较集中式系统实现了10.7%的性能提升。值得注意的是，在理想可靠数据环境下，其性能已逼近集中式系统的理论上限。该去中心化基础设施通过批量更新操作实现了约56%的边际成本节约，同时保障了评分管理的安全可信。我们的代码与系统已在github.com/yining610/Reliable-dRAG开源。"
    },
    {
        "title": "GRIN Transfer: A production-ready tool for libraries to retrieve digital copies from Google Books",
        "summary": "Publicly launched in 2004, the Google Books project has scanned tens of millions of items in partnership with libraries around the world. As part of this project, Google created the Google Return Interface (GRIN). Through this platform, libraries can access their scanned collections, the associated metadata, and the ongoing OCR and metadata improvements that become available as Google reprocesses these collections using new technologies. When downloading the Harvard Library Google Books collection from GRIN to develop the Institutional Books dataset, we encountered several challenges related to rate-limiting and atomized metadata within the GRIN platform. To overcome these challenges and help other libraries make more robust use of their Google Books collections, this technical report introduces the initial release of GRIN Transfer. This open-source and production-ready Python pipeline allows partner libraries to efficiently retrieve their Google Books collections from GRIN. This report also introduces an updated version of our Institutional Books 1.0 pipeline, initially used to analyze, augment, and assemble the Institutional Books 1.0 dataset. We have revised this pipeline for compatibility with the output format of GRIN Transfer. A library could pair these two tools to create an end-to-end processing pipeline for their Google Books collection to retrieve, structure, and enhance data available from GRIN. This report gives an overview of how GRIN Transfer was designed to optimize for reliability and usability in different environments, as well as guidance on configuration for various use cases.",
        "entry_id": "http://arxiv.org/abs/2511.11447v1",
        "pub_date": "2025-11-14",
        "translated_summary": "谷歌图书项目于2004年正式启动，已与全球多家图书馆合作扫描数千万册文献。作为该项目的重要组成部分，谷歌开发了谷歌回传接口（GRIN）。通过该平台，合作图书馆可获取其馆藏扫描文献、相关元数据，以及谷歌运用新技术重新处理文献时持续优化的OCR文本与元数据。当哈佛图书馆通过GRIN平台下载谷歌图书资源以构建机构图书数据集时，我们遇到了接口速率限制与元数据原子化等技术挑战。为突破这些限制并助力其他图书馆更高效地利用谷歌图书资源，本技术报告正式发布GRIN Transfer工具。这套开源即用的Python流水线系统，可帮助合作图书馆从GRIN平台快速获取图书资源。报告同时推出了机构图书1.0流水线的升级版本——该原始流水线最初用于分析、增强和整合机构图书1.0数据集。我们已对其进行了重构，使其兼容GRIN Transfer的输出格式。图书馆可组合使用这两套工具，构建端到端的谷歌图书资源处理流程，实现从GRIN平台的数据获取、结构化处理到质量增强的全链条操作。本报告详细阐述了GRIN Transfer如何针对不同环境优化可靠性及易用性的设计思路，并为多种应用场景提供了配置指南。"
    },
    {
        "title": "Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database",
        "summary": "Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.\n  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.\n  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.",
        "entry_id": "http://arxiv.org/abs/2511.11399v1",
        "pub_date": "2025-11-14",
        "translated_summary": "近年来，结合图数据库的图机器学习技术因其处理复杂互联数据的能力，以及运用图数据科学实施机器学习的特点而日益重要。然而，当前图数据库-图机器学习应用在数据分析方式上存在显著缺陷，尤其在知识图谱的知识补全环节表现得尤为突出。现有架构往往忽略知识补全环节，直接处理表面不完整或碎片化的数据集，尽管这些数据实则蕴含宝贵的隐藏知识。这种局限性可能导致在使用这些数据作为图机器学习模型输入时产生错误解读。\n\n本文提出了一种创新架构，将知识补全阶段整合至图数据库-图机器学习应用中，通过实证揭示了显性化隐藏知识如何深刻影响数据集表现与评估指标。为此，我们引入了可扩展的传递关系——这种通过衰减函数建模的关系链能在网络中传播信息，实现跨节点的确定性知识流动。\n\n实验结果表明，我们的创新构想能从根本上重塑数据集的拓扑结构与整体动态特征，这印证了采用新型图数据库-图机器学习架构的必要性：既能构建更优质的模型，又能充分释放图数据分析的全部潜力。"
    },
    {
        "title": "SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation",
        "summary": "LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.\n  To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop \"assess-validate-reflect\" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks.",
        "entry_id": "http://arxiv.org/abs/2511.11370v1",
        "pub_date": "2025-11-14",
        "translated_summary": "基于大语言模型的智能体正成为一种新兴范式，通过模拟用户行为来增强推荐系统性能。然而现有研究多聚焦于对单一物品评分的建模，这种点对点范式导致模型存在用户偏好理解失准和物品语义表征僵化等普遍问题。为突破这些局限，我们提出创新性的集合式反思学习框架。该框架通过构建\"评估-验证-反思\"的闭环流程，充分发挥大语言模型的上下文学习优势。与传统点对点评估不同，我们的框架对整组物品进行整体判断，通过综合分析物品间错综复杂的内部关联及其与用户偏好画像的集体契合度，实现集合层面的情境理解。这种方法使模型能捕捉用户行为中至关重要的复杂关系模式，从而显著提升序列推荐效能。大量实验验证了本方法的优越性，证实集合式视角对实现序列推荐任务最先进性能具有关键作用。"
    },
    {
        "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising",
        "summary": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.",
        "entry_id": "http://arxiv.org/abs/2511.11305v1",
        "pub_date": "2025-11-14",
        "translated_summary": "我们正式推出MOON——一套面向电商应用的多模态表征学习可持续迭代实践体系。该体系已全面部署于淘宝搜索广告系统的检索、相关性、排序等全链路环节，在点击率预测任务上取得显著效果，实现总点击率20.00%的大幅提升。这一历时三年的项目已完成五轮全链路迭代，成为点击率预测任务改进幅度最大的实践。在MOON体系的探索迭代过程中，我们积累了宝贵洞见与实践经验，现将其凝练为包含“预训练-后训练-应用”的三阶段训练范式，有效打通多模态表征与下游任务的衔接通道。值得注意的是，为弥合多模态表征学习目标与下游训练目标之间的错位，我们创新性地定义了“兑换率”指标，用以量化中间指标提升对下游收益的转化效能。通过该分析框架，我们成功锁定基于图像的搜索召回率作为指导多模态模型优化的关键中间指标。历经三年五轮迭代，MOON体系在数据处理、训练策略、模型架构与下游应用四大维度持续演进，相关经验教训与深度洞察将在本文分享。作为对电商领域规模化效应的探索延伸，我们进一步系统研究了多模态表征学习的规模法则，深入解析训练词元数量、负样本规模、用户行为序列长度等多重因素的协同影响规律。"
    },
    {
        "title": "SQuaD: The Software Quality Dataset",
        "summary": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
        "entry_id": "http://arxiv.org/abs/2511.11265v1",
        "pub_date": "2025-11-14",
        "translated_summary": "软件质量研究日益依赖于能够同时衡量软件系统产品维度与过程维度的大规模数据集。然而现有资源往往聚焦于有限维度（如代码异味、技术债或重构活动），从而制约了跨时间维度和质量维度的综合分析。为弥补这一空白，我们推出软件质量数据集SQuaD——这是一个从450个成熟开源项目（涵盖Apache、Mozilla、FFmpeg和Linux内核等多元生态系统）中提取的多维度、时间感知型软件质量指标集合。通过集成九种前沿静态分析工具（SonarQube、CodeScene、PMD、Understand、CK、JaSoMe、RefactoringMiner、RefactoringMiner++和PyRef），本数据集在方法、类、文件和项目层级统一了700余项独特指标。SQuaD覆盖总计63,586个经过分析的项目版本，同时提供版本控制与问题追踪历史、软件漏洞数据（CVE/CWE），以及被证实能增强即时缺陷预测的过程指标。该数据集支持在可维护性、技术债、软件演进和质量评估方面开展前所未有的实证研究。我们还规划了新兴研究方向，包括自动化数据集更新与跨项目质量建模，以支撑软件分析技术的持续演进。本数据集已在ZENODO平台公开发布（DOI: 10.5281/zenodo.17566690）。"
    },
    {
        "title": "Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation",
        "summary": "Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform.",
        "entry_id": "http://arxiv.org/abs/2511.11255v1",
        "pub_date": "2025-11-14",
        "translated_summary": "大语言模型在利用结构化世界知识与多步推理能力方面展现出显著优势，但由于语义和行为层面的错位，将其转化为现实推荐系统仍存在根本性挑战。为弥合这一差距，我们提出Align³GR创新框架，通过三层次对齐实现统一：融合用户-项目语义信号与协同信号的双重标记化机制；基于双向语义对齐的增强行为建模；结合自我博弈优化与真实反馈的渐进式直接偏好优化策略。实验表明，在公开数据集上Align³GR的Recall@10和NDCG@10指标分别超越现有最优基线17.8%和20.2%，在工业级推荐平台的在线A/B测试与全量部署中均取得显著效果提升。"
    },
    {
        "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition",
        "summary": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.",
        "entry_id": "http://arxiv.org/abs/2511.11172v1",
        "pub_date": "2025-11-14",
        "translated_summary": "随着团体活动的日益普及，如何基于群体成员的集体偏好为整个用户群提供推荐的需求不断增长。尽管已有多种团体推荐系统被提出，但这些方法常因实际应用中普遍存在的数据稀疏性和高维度问题而效果受限。本文提出了一种名为Group Soft-Impute SVD的团体推荐系统，通过软填充奇异值分解技术来增强团体推荐效果。该方法利用低秩矩阵补全技术应对稀疏高维数据的挑战。在Goodbooks、Movielens和合成数据集上的实验表明，与基于群体矩阵分解的方法相比，本方法在小规模用户群体的召回率指标上表现更优，同时在所有群体规模下均能取得相当的结果。此外，本方法能恢复比基线模型更低的矩阵秩，证明了其在处理高维数据方面的有效性。"
    },
    {
        "title": "GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs",
        "summary": "Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as \"redacted documents\" or \"pie charts.\" We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net.",
        "entry_id": "http://arxiv.org/abs/2511.11010v1",
        "pub_date": "2025-11-14",
        "translated_summary": "过去三十年的努力已建成包含数十亿网页快照和PB级数据的网络档案馆。仅\"任期结束网络档案馆\"就收录了联邦政府制作的数百万份PDF文件及其他类型文件。尽管网络存档在保存方面成果显著，但在访问和可发现性方面仍存在重大挑战。例如，当前对\"任期结束PDF\"的浏览功能仅限于下载和浏览单个PDF文件，以及执行基本关键词检索。本文推出GovScape公共检索系统，该系统支持对2020年\"任期结束\"网络抓取中10,015,993份联邦政府PDF文件（总计70,958,487页）进行多模态检索——据我们所知，这涵盖了2020年抓取中所有可渲染且不超过50页的PDF文件。GovScape为这千万量级PDF提供四种主要检索方式：除支持(1)基于域名和抓取日期等元数据面的筛选条件及(2)精确文本检索外，还提供(3)语义文本检索与(4)跨页视觉检索，使用户能构建\"经修订文件\"或\"饼状图\"等结构化查询。我们详细阐述了GovScape的构成组件，包括检索功能、嵌入流程、系统架构和开源代码库。值得注意的是，该系统预处理千万份PDF的总计算成本约为1,500美元，相当于每美元计算成本可处理47,000页PDF，展现出即时扩展的潜力。基于此，我们已着手推进亿级PDF规模的多模态检索研究。GovScape可通过https://www.govscape.net 访问。"
    },
    {
        "title": "LEMUR: Large scale End-to-end MUltimodal Recommendation",
        "summary": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.",
        "entry_id": "http://arxiv.org/abs/2511.10962v1",
        "pub_date": "2025-11-14",
        "translated_summary": "传统基于ID的推荐系统常面临冷启动和泛化性挑战。融合文本与视觉特征的多模态推荐系统为缓解这些问题提供了新思路。然而现有工业级方案通常采用两阶段训练范式：先预训练多模态模型，再将其冻结的特征表示应用于推荐模型训练。这种解耦框架存在多模态学习与推荐目标失配、无法动态适配新数据等缺陷。为解决这些问题，我们提出首个基于原始数据端到端训练的大规模多模态推荐系统LEMUR。通过联合优化多模态模块与推荐模块，该系统在实现实时参数更新的同时，能更紧密地对齐下游任务目标。从用户历史行为构建多模态序列表示往往伴随高昂计算成本，为此我们设计了一种新型记忆库机制，可在训练过程中渐进式累积历史多模态表示。在抖音搜索场景部署一个月后，LEMUR使查询变更率衰减降低0.843%，QAUC指标提升0.81%，同时在抖音广告核心离线指标上均取得显著收益。实验结果验证了端到端多模态推荐在真实工业场景中的优越性。"
    },
    {
        "title": "Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports",
        "summary": "Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.",
        "entry_id": "http://arxiv.org/abs/2511.13523v1",
        "pub_date": "2025-11-17",
        "translated_summary": "医疗记录数字化常依赖智能手机拍摄的印刷报告，但生成的图像常因模糊、阴影及其他噪点而质量受损。传统OCR系统虽针对洁净扫描文件优化，在此类现实场景中表现不佳。本研究评估了紧凑型多模态语言模型作为隐私保护方案，在转录含噪临床文档中的应用。通过采用印度医疗环境中常见的、带有地域特色医疗英语撰写的产科超声报告，我们从转录准确率、噪声敏感度、数字精确度及计算效率四个维度比较了八类系统。实验表明，紧凑型多模态模型持续优于传统与神经OCR流程。尽管其计算成本较高，但卓越的鲁棒性与语言适应性使其成为医疗本地化数字化可行的技术选择。"
    },
    {
        "title": "PolicyBot - Reliable Question Answering over Policy Documents",
        "summary": "All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.",
        "entry_id": "http://arxiv.org/abs/2511.13489v1",
        "pub_date": "2025-11-17",
        "translated_summary": "一国公民皆受政府制定的法律政策影响。这些法律政策承担着为公民赋予特定权利、规定相应义务等重要职能。然而此类文件往往篇幅冗长、内容复杂且难以检索，致使公民难以快速定位并理解相关信息。本文推出PolicyBot系统——一个注重透明度与可复现性的检索增强生成框架，专门用于基于政策文档的智能问答。该系统融合领域语义分块、多语言稠密嵌入、多阶段检索重排序及溯源感知生成技术，确保应答内容忠实于原始文献。我们通过实施引文溯源来减少幻觉生成并增强用户信任，同时评估了多种检索与生成方案以确定最优架构。该端到端系统完全基于开源工具构建，可轻松适配其他需基于文档的问答场景。本研究重点阐述了在政务相关场景中部署可信赖检索增强生成系统时的设计考量、实践挑战与经验总结。"
    },
    {
        "title": "Exploring Multi-Table Retrieval Through Iterative Search",
        "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.13418v1",
        "pub_date": "2025-11-17",
        "translated_summary": "面向数据湖的开放域问答需从多表中检索并整合信息，这一具有挑战性的子任务既要求语义相关性又需保持结构连贯性（如可连接性）。虽然混合整数规划等精确优化方法能确保结构连贯，但其计算复杂度往往令人望而却步。相反，仅针对查询覆盖度进行优化的简单贪心启发式方法又难以找到这些具备连贯性的可连接表集合。本文将多表检索构建为迭代搜索过程，论证该方法在可扩展性、可解释性与灵活性方面的优势。我们提出通用框架及具体实现方案——一种快速高效的贪心连接感知检索算法，能够整体平衡相关性、覆盖度与可连接性。在5个NL2SQL基准测试中的实验表明，相较于基于混合整数规划的方法，我们的迭代检索方法在保持竞争力的同时，根据基准测试与搜索空间设置的不同，速度提升达4-400倍。这项研究揭示了迭代启发式方法在实现实用化、可扩展且具备组合感知能力的检索方面的潜力。"
    },
    {
        "title": "Attention Grounded Enhancement for Visual Document Retrieval",
        "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
        "entry_id": "http://arxiv.org/abs/2511.13415v1",
        "pub_date": "2025-11-17",
        "translated_summary": "视觉文档检索需理解异构多模态内容以满足信息需求。近期研究采用基于截图的文档编码与细粒度延迟交互机制，显著提升了检索性能。然而，检索模型仍使用粗粒度的全局相关性标签进行训练，未能揭示支持匹配的具体区域。这导致模型倾向于依赖表层线索，难以捕捉隐式语义关联，制约了处理非抽取式查询的能力。为缓解该问题，我们提出一种基于注意力定位的检索增强框架AGREE。该框架利用多模态大语言模型的跨模态注意力作为代理局部监督信号，引导模型识别相关文档区域。在训练过程中，AGREE将局部信号与全局信号结合以联合优化检索器，使其不仅能判断文档是否匹配，更能学习驱动相关性的具体内容。在具有挑战性的ViDoRe V2基准测试中，AGREE显著优于仅使用全局监督的基线模型。定量与定性分析进一步表明，AGREE促进了查询词与文档区域的深度对齐，实现了超越表层匹配的更精准、可解释的检索。代码已开源：https://anonymous.4open.science/r/AGREE-2025。"
    },
    {
        "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference",
        "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.",
        "entry_id": "http://arxiv.org/abs/2511.13389v1",
        "pub_date": "2025-11-17",
        "translated_summary": "提升工业铸造过程的能源效率是一项关键挑战，因为这类工序属于高能耗作业，且工艺变量间存在复杂的相互依存关系。基于相关性的分析方法往往难以区分真正的因果驱动因素与伪相关性，限制了其在决策中的应用。本文运用时间序列因果推断框架，识别感应炉熔炼过程中直接影响能源效率的操作因素。通过整合丹麦某铸造厂的生产数据，研究采用时间序列聚类将熔炼周期划分为不同操作模式，并运用前沿因果发现算法PCMCI+揭示各模式内的因果关系。跨集群分析表明：能耗、炉温与物料重量间的强因果关系构成了能效的核心驱动因素，而电压对冷却水温的影响则始终存在延迟响应。集群间差异进一步区分了操作机制——高效集群以稳定的因果结构为特征，低效集群则表现出强化反馈回路与非典型依赖关系。本研究的贡献具有双重意义：方法论层面提出了融合聚类与因果推断的分析流程，为能源密集型工艺研究提供了创新工具；实践层面则为铸造企业优化生产性能、降低能耗与排放提供了可操作的见解。"
    },
    {
        "title": "FLOWER: Flow-Oriented Entity-Relationship Tool",
        "summary": "Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.",
        "entry_id": "http://arxiv.org/abs/2511.13357v1",
        "pub_date": "2025-11-17",
        "translated_summary": "跨数据源的关系探索是实体识别优化的关键环节。由于数据库能够存储包含合成数据与自然数据的大体量信息，正确处理全部对象成为重要任务。然而实体关系模型的构建方式往往受到人为因素影响。本文提出面向流程的实体关系工具FLOWER，这是首个端到端解决方案，能够实时消除主流SQL方言在处理、创建及可视化显性与隐性依赖时的重复性资源消耗问题。启动后，FLOWER自动检测内置约束，通过动态采样与鲁棒数据分析技术构建正确且必要的约束体系。该方法可优化实体关系模型与数据叙事功能，帮助用户深入理解数据基础，通过SQL或自然语言从数据库源获取潜在洞察。在STATS前沿基准测试中，FLOWER在分布表征方面较水库采样提升2.4倍，约束学习效率提高2.6倍，并实现2.15倍加速。在数据叙事方面，相比大语言模型准确度提升1.19倍，上下文依赖减少1.86倍。该工具支持23种语言，兼容CPU与GPU架构。实验结果表明FLOWER能更高效处理现实数据，在不同应用场景中确保质量、可扩展性与适用性。"
    },
    {
        "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
        "summary": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
        "entry_id": "http://arxiv.org/abs/2511.13271v1",
        "pub_date": "2025-11-17",
        "translated_summary": "以ChatGPT为代表的生成式人工智能工具兴起，为计算机教育带来新机遇与挑战。现有研究多聚焦于其完成任务的能力及对学习成绩的影响，却常忽视其对知识获取的作用。本研究通过对照实验，比较了不同编程基础的学习者在使用生成式AI与传统网络资源时知识获取的差异。我们招募24名具有初级与中级编程经验的本科生，观察其在完成编程任务时与ChatGPT的互动过程，从任务表现、概念理解及交互行为三个维度进行分析。研究发现：借助生成式AI生成完整解决方案能显著提升任务完成度（尤其对初学者），但未必转化为知识增长；更重要的是，使用策略因经验水平而异——初学者往往过度依赖AI完成作业却未获得知识提升，中级学习者则更善于选择性利用。研究表明，无论过度依赖还是极少使用都会削弱知识获取效果。基于此，我们呼吁师生将生成式AI定位为学习工具而非解题手段，并强调在编程教育中亟需建立引导机制，以促进深度理解。"
    },
    {
        "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation",
        "summary": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.",
        "entry_id": "http://arxiv.org/abs/2511.13201v1",
        "pub_date": "2025-11-17",
        "translated_summary": "检索增强生成（RAG）通过引入外部知识来抑制大语言模型的幻觉现象，有效提升其响应质量与领域适应性。近期研究将图结构引入RAG以增强实体间语义关系捕获，但主要聚焦于低阶成对实体关系，难以建模多实体间的高阶关联。超图增强方法通过超边建模多实体交互突破此局限，但通常局限于语块内部的实体级表征，未能兼顾跨语块的全局主题组织与对齐机制。受人类思维自上而下认知过程的启发，我们提出主题对齐的双超图RAG框架（Cog-RAG），通过主题超图捕捉语块间主题结构，利用实体超图建模高阶语义关系。进一步设计认知启发的两阶段检索策略：先从主题超图激活查询相关主题内容，再引导实体超图进行细粒度召回与扩散，实现从全局主题到局部细节的语义对齐与连贯生成。大量实验表明，Cog-RAG显著优于现有主流基线方法。"
    },
    {
        "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
        "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
        "entry_id": "http://arxiv.org/abs/2511.13189v1",
        "pub_date": "2025-11-17",
        "translated_summary": "基础模型已在众多领域引发人工智能革命，然而其在极端多标签分类（XMC）领域的变革潜力仍待开发。XMC任务需从极大规模标签空间中为查询匹配相关标签，其核心挑战在于效率与性能的平衡。为此，近期研究多采用仅含编码器的小型Transformer架构学习嵌入向量，将XMC高效转化为最大内积搜索问题。本文聚焦XMC两大关键方向：如何有效利用仅含解码器的大型模型，以及如何在保持计算效率的同时整合视觉信息。我们证明这两个方向各自具有重要价值，且能协同提升性能。实验表明，数十亿参数的解码器可在可控计算开销下实现显著性能提升。进一步提出的视觉增强极端多标签学习框架（ViXML）通过单图像嵌入池化技术，在限制计算增长的同时成功融合基础视觉模型，解锁多模态能力。值得注意的是，采用小型编码器的ViXML在多数场景下优于纯文本解码器，印证“一图胜千言”的效能。此外，我们还将现有纯文本数据集扩展为包含视觉元数据的版本，为后续研究提供基准平台。在四个公开纯文本数据集及其图像增强版本上的综合实验验证了方案有效性，在最大数据集上P@1指标较之前最优成果提升达8.21%。ViXML代码已开源：https://github.com/DiegoOrtego/vixml。"
    },
    {
        "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
        "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
        "entry_id": "http://arxiv.org/abs/2511.13166v1",
        "pub_date": "2025-11-17",
        "translated_summary": "为更有效地利用互联网用户行为数据，本文提出一种新颖的协同过滤方法——局部协同过滤(LCF)。该方法通过挖掘用户间的局部相似性，运用大数定律整合用户数据，从而提升用户行为数据的利用率。在Steam游戏数据集上的实验表明，LCF方法的推荐效果符合现实需求。"
    },
    {
        "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning",
        "summary": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.",
        "entry_id": "http://arxiv.org/abs/2511.13125v1",
        "pub_date": "2025-11-17",
        "translated_summary": "近年来基于学习的方法虽然降低了传统轨迹相似性计算的时间复杂度，但现有最优方法仍未能充分利用轨迹信息的完整频谱进行相似性建模。为解决这一问题，我们提出\\textbf{RePo}方法，通过联合编码\\textbf{区域级}与\\textbf{点级}特征来同时捕捉空间上下文和细粒度移动模式。在区域级表征方面，首先将GPS轨迹映射为网格序列，通过结构特征捕捉空间上下文，并借助视觉特征增强语义上下文；在点级表征方面，三个轻量级专家网络分别从密集GPS序列中提取局部特征、关联特征和连续移动模式。随后通过路由网络自适应融合点级特征，再与区域级特征进行交叉注意力融合生成最终轨迹嵌入。我们采用包含困难负样本的对比损失函数进行模型训练，以提供相似度排序监督。实验结果表明，在所有评估指标上，RePo相较现有最优基线模型平均准确率提升22.2\\%。"
    },
    {
        "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation",
        "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.",
        "entry_id": "http://arxiv.org/abs/2511.13063v1",
        "pub_date": "2025-11-17",
        "translated_summary": "电子显微镜图像中神经结构的精确分割对神经科学研究至关重要。然而，该任务面临形态结构复杂、信噪比低及标注稀缺等挑战，限制了现有方法的准确性与泛化能力。为解决这些问题，我们尝试利用视觉基础模型在自然图像上学习到的先验知识来改进分割性能。具体而言，我们提出了一种创新框架，能够将基于自然图像预训练的SAM2模型的知识有效迁移至电子显微镜领域。该框架首先通过SAM2提取强泛化性的通用特征；为弥合领域差异，我们设计了特征引导注意力模块，利用SAM2的语义线索引导轻量级精细编码器聚焦于困难区域；最后通过双亲和度解码器同步生成粗粒度与精细化亲和力图。实验结果表明，在冻结SAM2权重的情况下，我们的方法已达到与现有最优方法相当的精度；当在电子显微镜数据上进行微调后，其性能显著超越当前最优方法。本研究证实：结合针对性领域自适应引导，迁移自然图像预训练表征能有效解决神经元分割中的特定挑战。"
    },
    {
        "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
        "summary": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
        "entry_id": "http://arxiv.org/abs/2511.13057v1",
        "pub_date": "2025-11-17",
        "translated_summary": "稠密检索模型已成为信息检索领域的最先进标准。然而其高维度、高精度（float32）的向量嵌入在实际部署中带来了显著的存储与内存挑战。为应对此问题，我们在BEIR SciFact基准上开展严格实证研究，评估两种主要压缩策略的平衡关系：（1）通过深度自编码器实现维度压缩，将原始384维向量降至12维潜在空间；（2）通过量化实现精度压缩（float16、int8与二值化）。我们通过全套检索指标（NDCG、MAP、MRR、召回率、精确度）在不同k值截断点上相对float32基线的“性能损失（或增益）”，对每种方法进行系统比较。实验结果表明：int8标量量化能提供最佳平衡点，在实现4倍压缩的同时，nDCG@10指标仅出现可忽略的[约1-2%]下降；而自编码器虽呈现平缓的性能衰减，但在同等4倍压缩比（AE-96）下会出现更显著的性能损失；二值化量化由于会导致性能急剧下降，被证明不适用于此任务。本研究为部署高效能检索系统提供了实用指南。"
    },
    {
        "title": "Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning",
        "summary": "Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.",
        "entry_id": "http://arxiv.org/abs/2511.13041v1",
        "pub_date": "2025-11-17",
        "translated_summary": "协同过滤技术在现代推荐系统中发挥着关键作用，它通过分析用户与项目的交互历史来实现个性化推荐。然而基于协同过滤的方法常因训练数据不平衡而产生偏差，这种倾向会导致系统优先推荐热门项目，而对非活跃用户的推荐效果欠佳。现有研究通过重平衡训练样本、重排推荐结果或建立对偏差具有鲁棒性的模型来解决这一问题。这些方法虽有效，但可能影响推荐准确性，或对权重策略过于敏感，导致模型训练困难。本文深入分析了推荐偏差的成因与影响，从表示分布视角提出去偏框架AURL，通过增强表征学习中的群组对齐与全局一致性来实现推荐去偏。具体而言，我们发现了用户与项目表示分布中存在的两个核心问题：群组差异与全局坍缩，这两者直接导致推荐结果产生偏差。为此，我们在表示空间中设计了两个简洁有效的正则化器：群组对齐器致力于拉近长尾实体与热门实体的表示分布，全局一致性器则通过均匀分布表示来最大限度保留实体信息。我们的方法通过同步优化群组对齐与全局一致性正则项来缓解推荐偏差。在三个真实数据集和多种推荐骨干网络上进行的广泛实验，验证了所提框架的优越性。"
    },
    {
        "title": "Personalized Federated Recommendation With Knowledge Guidance",
        "summary": "Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.",
        "entry_id": "http://arxiv.org/abs/2511.12959v1",
        "pub_date": "2025-11-17",
        "translated_summary": "联邦推荐系统已成为构建隐私保护推荐系统的关键范式。然而现有联邦推荐模型面临核心困境：内存高效的单一知识模型受限于次优的知识替换机制，导致有价值的个性化信息被丢弃；而高性能的双知识模型因内存占用过高难以在实际设备上部署。我们提出知识引导的联邦推荐框架FedRKG，这一模型无关的解决方案通过\"知识引导\"核心原则，避免完全替换而将全局知识融合到保留的本地嵌入中，在单一知识内存占用量下实现双知识模型的个性化优势。进一步提出自适应引导机制，通过细粒度动态调节每个用户-项目交互的引导强度，克服静态融合方法的局限性。在基准数据集上的大量实验表明，FedRKG显著优于现有最优方法，验证了本方法的有效性。代码已开源：https://github.com/Jaehyung-Lim/fedrkg。"
    },
    {
        "title": "Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior",
        "summary": "In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.\n  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.",
        "entry_id": "http://arxiv.org/abs/2511.12949v1",
        "pub_date": "2025-11-17",
        "translated_summary": "近年来，大语言模型在语言理解与生成任务中表现出色，推动了智能对话和推荐系统的发展。然而这些系统仍存在显著局限：往往以静态方式建模用户偏好，难以捕捉交互行为中动态连续的潜在规律。用户历史提问序列隐含着兴趣演变与认知模式的丰富信号，但由于语言建模与行为序列建模之间存在固有割裂，如何利用这类时序数据进行预测仍具挑战。\n\n为突破这一局限，我们提出协同过滤增强的提问预测框架。该框架通过融合个性化记忆模块与基于图的偏好传播机制，动态建模用户-问题交互的演变过程。这种双重机制使系统既能自适应学习用户特定历史，又能借助相似用户的协同信号优化预测。实验结果表明，我们的方法可有效生成模拟真实用户提问模式的智能体，展现了构建前瞻自适应对话系统的潜力。"
    },
    {
        "title": "A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation",
        "summary": "Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.",
        "entry_id": "http://arxiv.org/abs/2511.12947v1",
        "pub_date": "2025-11-17",
        "translated_summary": "本地生活推荐服务近年来快速发展，为用户获取日常所需提供了便捷途径。然而该领域面临两大核心挑战：（1）空间约束性——受本地生活场景特性影响，商品通常仅向有限地理范围内的用户展示，间接降低了商品曝光概率；（2）长尾稀疏性——少数热门商品占据大部分用户交互记录，而大量优质长尾商品因交互机会失衡被严重忽视。现有方法多采用用户中心视角，例如建模空间用户偏好或利用协同过滤信号增强长尾表征。但我们认为，基于商品中心的视角更能契合该领域特性，应聚焦于增强符合本地生活服务空间约束特征的长尾商品表征。为此，我们提出ReST框架——一种即插即用的空间约束表征增强框架。具体而言，我们首先设计元ID预热网络，通过注入基础属性层级语义信息来初始化ID表征；随后提出基于对比学习的空间约束ID表征增强网络（SIDENet），该网络融合两项高效策略：空间约束硬采样策略与动态表征对齐策略。该设计能在训练过程中基于属性信息自适应识别弱表征ID，通过捕捉本地生活服务空间约束特性下的潜在商品关联来增强其表征，同时保持与热门商品的兼容性。"
    },
    {
        "title": "AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking",
        "summary": "In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.",
        "entry_id": "http://arxiv.org/abs/2511.12934v1",
        "pub_date": "2025-11-17",
        "translated_summary": "在工业推荐系统中，基于深度神经网络（DNN）的粗排模型通常采用顺序执行框架：只有在接收到上游召回阶段的候选集后，才会触发特征获取和模型前向计算。这种设计存在固有瓶颈，包括对相同用户/项目的重复计算，以及严格顺序操作带来的延迟增加，这些因素共同制约了模型容量与系统效率。为突破这些限制，我们提出异步推理框架（AIF），这是一种高效经济的计算架构，其核心在于将交互无关组件（即仅涉及单一用户或项目内部运算的模块）从实时预测中解耦。AIF通过以下方式重构模型推理流程：用户侧计算与召回阶段并行执行，项目侧计算则采用近线处理模式。这意味着交互无关组件的计算仅需执行一次，并在粗排阶段实时预测开始前完成。该架构显著提升了计算效率并降低延迟，释放的资源使得交互无关组件的特征集与模型架构得以显著优化。此外，我们深入探索了AIF框架内的模型设计，对线上实时预测中的交互相关组件采用近似计算方法。通过框架与模型的协同设计，我们的解决方案在不显著增加计算与延迟成本的前提下实现了显著性能提升，目前已在淘宝展示广告系统成功部署。"
    },
    {
        "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation",
        "summary": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.",
        "entry_id": "http://arxiv.org/abs/2511.12922v1",
        "pub_date": "2025-11-17",
        "translated_summary": "基于大语言模型的推荐系统通过项目标记化技术弥合项目空间与语言空间之间的差异，实现了高质量的推荐性能。然而，现有的项目标记化方法通常需要为每个项目领域单独训练模型，限制了泛化能力。同时，不同项目领域间分布与语义的差异性使得构建能保留领域特异性信息的统一标记化面临挑战。为此，我们提出UniTok——一个统一的项目标记化框架，通过将混合专家架构与系列码本相结合，将项目转化为离散标记，在实现可扩展标记化的同时保留跨领域语义信息。具体而言，不同领域的项目首先通过共享编码器投射到统一潜在空间，随后被路由至领域特定专家以捕获独特语义，而始终保持激活状态的共享专家则负责编码可跨领域迁移的通用知识。此外，为缓解领域间语义不均衡问题，我们提出互信息校准机制，引导模型为各领域保留相近层级的语义信息。基于多领域真实数据集的综合实验表明，UniTok框架具有以下优势：（a）高效性：在强基准测试中实现最高51.89%的性能提升；（b）理论严谨性：架构设计与优化的分析有效性得到验证；（c）强泛化性：无需针对每个领域重新训练即可在多样化领域保持稳健性能，这是现有基线方法所不具备的能力。"
    },
    {
        "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy",
        "summary": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.",
        "entry_id": "http://arxiv.org/abs/2511.12920v1",
        "pub_date": "2025-11-17",
        "translated_summary": "谷歌搜索正通过\"AI概览\"和\"精选摘要\"等功能呈现越来越多AI生成内容，用户虽无法控制其呈现形式却日益依赖。本研究通过对1,508个真实育儿及孕期相关查询进行系统算法审计，评估了这些信息展示的质量与一致性。我们构建的稳健评估框架涵盖多维度质量指标：答案一致性、相关性、医疗安全措施、信息来源类型及情感倾向。研究结果显示，同一搜索结果页面上AI概览与精选摘要的信息不一致率高达33%。尽管相关性评分普遍较高，但两者均严重缺乏医疗安全提示（AI概览仅11%含警示，精选摘要仅7%）。在信息来源方面，健康类网站虽占主导，但精选摘要还频繁链接商业来源。这些发现对公共卫生信息获取具有重要意义，表明AI赋能的健康信息亟需更严格的质量管控。本研究方法论为审计高风险领域AI系统提供了可迁移框架，这些领域的信息质量直接关系用户福祉。"
    },
    {
        "title": "NeuCLIRBench: A Modern Evaluation Collection for Monolingual, Cross-Language, and Multilingual Information Retrieval",
        "summary": "To measure advances in retrieval, test collections with relevance judgments that can faithfully distinguish systems are required. This paper presents NeuCLIRBench, an evaluation collection for cross-language and multilingual retrieval. The collection consists of documents written natively in Chinese, Persian, and Russian, as well as those same documents machine translated into English. The collection supports several retrieval scenarios including: monolingual retrieval in English, Chinese, Persian, or Russian; cross-language retrieval with English as the query language and one of the other three languages as the document language; and multilingual retrieval, again with English as the query language and relevant documents in all three languages. NeuCLIRBench combines the TREC NeuCLIR track topics of 2022, 2023, and 2024. The 250,128 judgments across approximately 150 queries for the monolingual and cross-language tasks and 100 queries for multilingual retrieval provide strong statistical discriminatory power to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included with the collection so that developers of reranking algorithms are no longer reliant on BM25 as their first-stage retriever. NeuCLIRBench is publicly available.",
        "entry_id": "http://arxiv.org/abs/2511.14758v1",
        "pub_date": "2025-11-18",
        "translated_summary": "为有效衡量检索技术的进展，需要具备能够准确区分系统性能的相关性评估测试集。本文推出NeuCLIRBench——一个面向跨语言与多语言检索的评估数据集。该数据集包含中文、波斯语和俄语的原始文档，以及这些文档经机器翻译后的英文版本，支持多种检索场景：英语、中文、波斯语或俄语的单语检索；以英语为查询语言、其他三种语言之一为文档语言的跨语言检索；以及以英语为查询语言、三种语言文档均需检索的多语言检索。该数据集整合了TREC NeuCLIR赛道2022至2024年的全部主题，包含单语及跨语言任务约150个查询的250,128条评估结果，以及多语言检索100个查询的评估数据，具备强大的统计区分能力以甄别不同检索方法。数据集还提供了强神经检索系统的融合基线，使重排序算法开发者无需再依赖BM25作为首阶段检索器。NeuCLIRBench已面向公众开放。"
    },
    {
        "title": "LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation",
        "summary": "With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.",
        "entry_id": "http://arxiv.org/abs/2511.14531v1",
        "pub_date": "2025-11-18",
        "translated_summary": "随着检索增强生成(RAG)技术在生成式AI解决方案中的地位日益凸显，系统化评估其效能的需求也愈发迫切。我们推出LiveRAG基准测试——一个包含895组合成问答对的公开数据集，专为支持基于RAG的问答系统进行系统性评估而设计。该合成基准源自SIGIR'2025 LiveRAG挑战赛的竞赛题库，所有参赛者曾在严格时间限制下接受该题库的测试。当前版本新增了挑战赛期间未公开的关键信息，包括用于评估参赛答案的基准答案及其支撑依据。此外，通过将项目反应理论模型应用于参赛者答题数据，我们为每个问题标注了预估难度系数与区分度指数。分析表明，该基准测试兼具问题多样性、难度广谱性以及系统能力区分度三大特性。期待LiveRAG基准能助力学界推进RAG研究、实施系统化评估，并开发出更稳健的问答系统。"
    },
    {
        "title": "Effective Diversification of Multi-Carousel Book Recommendation",
        "summary": "Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.",
        "entry_id": "http://arxiv.org/abs/2511.14461v1",
        "pub_date": "2025-11-18",
        "translated_summary": "在当代大多数电影流媒体平台中，采用可循环滚动的多轮播列表已成为内容呈现的基本模式。这种设计通过类型、作者等分类维度，有效凸显用户兴趣的不同面向。尽管轮播界面能优化信息结构与导航效率，但单纯依靠这种形式并不能提升推荐内容的多样性——而多样性恰恰是维持用户参与度的关键要素。本研究基于协同过滤算法，提出多种有效提升图书推荐多样性的解决方案，旨在优化公共图书馆线上目录的荐书体系。我们同时建立了评估指标体系，通过实验证明该推荐系统能够在准确性与超准确性指标之间实现良好平衡。"
    },
    {
        "title": "Jasper-Token-Compression-600M Technical Report",
        "summary": "This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.",
        "entry_id": "http://arxiv.org/abs/2511.14405v1",
        "pub_date": "2025-11-18",
        "translated_summary": "本技术报告介绍了2025年11月发布的开源Jasper-Token-Compression-600M模型的训练方法与评估结果。基于先前英文Stella和Jasper模型的蒸馏方案，我们成功将该方法扩展至中英双语领域，并通过引入对比学习进一步提升了模型性能。本模型的核心创新在于引入基于一维卷积的令牌压缩模块，通过动态调整训练过程中的压缩率，使模型能够学习更鲁棒、更高效的压缩文本表示。通过将知识蒸馏与令牌压缩技术相结合，我们在嵌入质量和推理效率两方面均实现了显著提升。该模型在达到与80亿参数模型相当性能的同时，运行效率显著优于传统的6亿参数模型。更多模型发布信息请访问：https://huggingface.co/infgrad/Jasper-Token-Compression-600M。"
    },
    {
        "title": "Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction",
        "summary": "Generative models are increasingly being explored in click-through rate (CTR) prediction field to overcome the limitations of the conventional discriminative paradigm, which rely on a simple binary classification objective. However, existing generative models typically confine the generative paradigm to the training phase, primarily for representation learning. During online inference, they revert to a standard discriminative paradigm, failing to leverage their powerful generative capabilities to further improve prediction accuracy. This fundamental asymmetry between the training and inference phases prevents the generative paradigm from realizing its full potential. To address this limitation, we propose the Symmetric Masked Generative Paradigm for CTR prediction (SGCTR), a novel framework that establishes symmetry between the training and inference phases. Specifically, after acquiring generative capabilities by learning feature dependencies during training, SGCTR applies the generative capabilities during online inference to iteratively redefine the features of input samples, which mitigates the impact of noisy features and enhances prediction accuracy. Extensive experiments validate the superiority of SGCTR, demonstrating that applying the generative paradigm symmetrically across both training and inference significantly unlocks its power in CTR prediction.",
        "entry_id": "http://arxiv.org/abs/2511.14403v1",
        "pub_date": "2025-11-18",
        "translated_summary": "生成式模型在点击率预估领域日益受到关注，旨在突破传统判别式范式仅依赖简单二元分类目标的局限。然而现有生成模型通常将生成范式局限于训练阶段，主要用于表征学习。在线推理时它们仍回归标准判别式范式，未能利用强大的生成能力进一步提升预测精度。这种训练与推理阶段的根本性不对称阻碍了生成范式发挥全部潜力。为解决这一局限，我们提出对称掩码生成式点击率预估框架，构建训练与推理阶段的对称性。具体而言，在训练阶段通过学习特征依赖获得生成能力后，该框架在在线推理阶段运用这种生成能力迭代重构输入样本特征，从而有效缓解噪声特征影响并提升预测准确性。大量实验验证了该框架的优越性，证明在训练和推理阶段对称应用生成范式能显著释放其在点击率预估中的潜力。"
    },
    {
        "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models",
        "summary": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.",
        "entry_id": "http://arxiv.org/abs/2511.14256v1",
        "pub_date": "2025-11-18",
        "translated_summary": "知识图谱推理（KGR）是通过对知识图谱进行逻辑推演以推断新知识的任务。近年来，大语言模型（LLM）在复杂推理任务中展现出卓越性能。尽管取得显著进展，当前基于LLM的KGR方法仍面临两个关键局限：其一，现有方法往往不加区分地提取推理路径，未能评估路径的重要性差异，可能引入无关噪声误导LLM；其二，多数方法依赖LLM动态探索潜在推理路径，但需要高频次检索和大量LLM调用。为应对这些挑战，我们提出PathMind框架，通过选择性引导LLM关注重要推理路径，提升推理的可靠性与可解释性。该框架采用“检索-优先级排序-推理”范式：首先通过检索模块从知识图谱中获取查询子图；随后引入路径优先级机制，通过语义感知的路径优先级函数识别重要推理路径，该函数同步考量路径累积代价与抵达目标的预估未来代价；最后通过双阶段训练策略（包括任务特定指令微调与路径偏好对齐）生成精准且逻辑一致的答案。在基准数据集上的大量实验表明，PathMind通过识别关键推理路径，在输入标记更少的复杂推理任务中持续优于现有基线模型。"
    },
    {
        "title": "LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation",
        "summary": "Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLM's geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.",
        "entry_id": "http://arxiv.org/abs/2511.14221v1",
        "pub_date": "2025-11-18",
        "translated_summary": "大型语言模型的最新进展通过增强语义泛化能力，改进了基于文本的推荐系统，突破了传统基于ID方法的局限。现有文本方法通常通过提示设计编码物品文本信息，并借助物品标记化生成离散语义ID。然而在本地生活服务等垂直领域任务中，仅将位置信息注入提示模板难以捕捉细粒度空间特征及物品间真实距离感知。为此，我们提出LGSID——面向本地生活推荐的LLM对齐地理标记化框架，该框架包含两大核心组件：（1）基于强化学习的地理LLM对齐；（2）分层级地理物品标记化。在强化学习对齐模块中，我们首先训练列表式奖励模型以捕捉物品间真实空间关系，继而提出创新性G-DPO算法，利用预训练奖励模型向LLM注入泛化空间知识与协同信号，同时保持其语义理解能力。进一步提出分层级地理物品标记化策略：主标记源自离散化空间与内容属性，残差标记则通过对齐后LLM的地理表征向量进行精细化生成。在快手真实工业数据集上的大量实验表明，LGSID在各项指标上持续优于当前最先进的判别式与生成式推荐模型。消融实验、可视化分析与案例研究进一步验证了该框架的有效性。"
    },
    {
        "title": "WebRec: Enhancing LLM-based Recommendations with Attention-guided RAG from Web",
        "summary": "Recommender systems play a vital role in alleviating information overload and enriching users' online experience. In the era of large language models (LLMs), LLM-based recommender systems have emerged as a prevalent paradigm for advancing personalized recommendations. Recently, retrieval-augmented generation (RAG) has drawn growing interest to facilitate the recommendation capability of LLMs, incorporating useful information retrieved from external knowledge bases. However, as a rich source of up-to-date information, the web remains under-explored by existing RAG-based recommendations. In particular, unique challenges are posed from two perspectives: one is to generate effective queries for web retrieval, considering the inherent knowledge gap between web search and recommendations; another challenge lies in harnessing online websites that contain substantial noisy content. To tackle these limitations, we propose WebRec, a novel web-based RAG framework, which takes advantage of the reasoning capability of LLMs to interpret recommendation tasks into queries of user preferences that cater to web retrieval. Moreover, given noisy web-retrieved information, where relevant pieces of evidence are scattered far apart, an insightful MP-Head is designed to enhance LLM attentions between distant tokens of relevant information via message passing. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed web-based RAG methods in recommendation scenarios.",
        "entry_id": "http://arxiv.org/abs/2511.14182v1",
        "pub_date": "2025-11-18",
        "translated_summary": "推荐系统在缓解信息过载、丰富用户在线体验方面发挥着关键作用。在大语言模型时代，基于大语言模型的推荐系统已成为推进个性化推荐的主流范式。近年来，检索增强生成技术通过整合外部知识库中的有效信息，日益受到学界关注，以增强大语言模型的推荐能力。然而，作为实时信息的丰富来源，网络资源在当前基于检索增强生成的推荐系统中尚未得到充分探索。具体而言存在两大挑战：一是考虑到网络搜索与推荐任务之间固有的知识鸿沟，如何生成适用于网络检索的有效查询；二是如何有效利用包含大量噪声内容的在线网站资源。为突破这些局限，我们提出WebRec——一个创新的基于网络的检索增强生成框架，该框架利用大语言模型的推理能力，将推荐任务解析为符合网络检索需求的用户偏好查询。此外，针对网络检索信息中相关证据分散存在的噪声问题，我们设计了具有洞察力的MP-Head模块，通过消息传递机制增强大语言模型对分散相关信息的注意力聚焦。大量实验证明，我们提出的基于网络的检索增强生成方法在推荐场景中具有显著有效性。"
    },
    {
        "title": "Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions",
        "summary": "In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the \"fill-in-the-blank\" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.",
        "entry_id": "http://arxiv.org/abs/2511.14144v1",
        "pub_date": "2025-11-18",
        "translated_summary": "本研究将基于Transformer的关系抽取与知识图谱匹配相结合，应用于填空题形式的多项选择题解答，同时保持输出过程的可追溯性。知识图谱是由实体和关系构成的结构化事实知识表示。由于构建成本高昂，传统上被视为具有已验证链接的静态数据库。但近年来基于Transformer的关系抽取技术发展使我们能够通过输入自然文本来动态生成知识图谱，从而开创了用生成图谱表征输入语句语义的新可能。基于此，我们提出一种针对填空题的解答方法，重点关注当输入事实错误的文本时，关系抽取方法会生成包含虚假信息知识图谱的特性。我们通过以下方式评估问题陈述的真实性：(i)使用关系抽取方法将语句转换为关系图谱，(ii)在封闭世界假设下对照事实正确的知识图谱进行验证。实验结果表明，本方法在保持过程可追溯性的同时，能对约70%的问题给出正确答案。我们还发现问题类别对准确率存在显著影响。"
    },
    {
        "title": "PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval",
        "summary": "With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.",
        "entry_id": "http://arxiv.org/abs/2511.14130v1",
        "pub_date": "2025-11-18",
        "translated_summary": "随着大语言模型的快速发展，金融信息检索已成为关键的工业应用场景。从冗长的财务报告中提取任务相关信息对运营决策与分析决策都至关重要。FinAgentBench数据集通过文档排序和文本块排序两项任务将这一问题规范化。我们提出PRISM这一免训练框架，融合了精细化系统提示、上下文学习与轻量级多代理系统。通过深入剖析各组件间的协同机制：提示工程提供精准任务指令，上下文学习注入语义相关的少样本示例，多代理系统则模拟协同评分行为。我们的最优配置在受限验证集上取得了0.71818的NDCG@5评分。进一步实验表明，PRISM在生产级金融检索场景中兼具可行性与鲁棒性，其模块化、纯推理的设计特性更契合实际应用需求。源代码已发布于https://bit.ly/prism-ailens。"
    },
    {
        "title": "NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval",
        "summary": "Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.",
        "entry_id": "http://arxiv.org/abs/2511.14096v1",
        "pub_date": "2025-11-18",
        "translated_summary": "检索增强生成（RAG）技术显著提升了大型语言模型在知识密集型任务中的表现。然而，传统RAG方法在应对多跳问答任务时，由于难以捕捉文档间的复杂依赖关系而存在局限。近期研究尝试通过基于图结构的RAG方法建立文档关联，但这类方法在节点匹配和子图构建过程中易导致语义连贯性缺失并引入无关噪声。为此，我们受神经生物学中位置细胞路径导航机制的启发，提出NeuroPath——一种基于大语言模型的语义路径追踪RAG框架。该框架包含动态路径追踪与检索后补全两个核心阶段：动态路径追踪通过在构建的知识图谱上进行目标导向的语义路径追踪与剪枝，有效提升噪声抑制与语义连贯性；检索后补全则通过结合中间推理过程与原始查询进行二次检索，优化查询目标并补全推理路径中的缺失信息。在三个多跳问答数据集上的实验表明，NeuroPath相较当前最先进的图结构RAG方法，在召回率@2和@5指标上分别实现16.3%和13.5%的平均提升；与迭代式RAG方法相比，在获得更高准确率的同时降低22.8%的token消耗。此外，我们在四款轻量化大模型（Llama3.1、GLM4、Mistral0.3和Gemma3）上验证了NeuroPath的鲁棒性，并进一步证明了其在处理不同复杂度任务时的可扩展性。项目代码已开源：https://github.com/KennyCaty/NeuroPath。"
    },
    {
        "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees",
        "summary": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.",
        "entry_id": "http://arxiv.org/abs/2511.13942v1",
        "pub_date": "2025-11-17",
        "translated_summary": "基于规则的系统必须在严格的时间限制内解决复杂的匹配问题，才能在实时应用中保持高效，例如AI代理的规划与反应控制，以及低延迟关系数据库查询。模式匹配系统可能遇到这样的问题：当规则包含大量欠约束变量，或产生组合爆炸的中间部分匹配时（尽管其他方面约束良好），寻找匹配需要指数级的时间和空间。在线AI系统通过示例驱动归纳或代码合成自动生成规则时，很容易产生最坏情况下的匹配模式，导致程序因超出可用内存而减速或停止。在我们基于示例学习的认知系统研究中发现，采用激进的反泛化泛化方法极易引发此类情况。为使这些系统无需人工设计约束即可投入实用，同时避免不可预测的故障模式，我们提出名为CORGI（面向集合的关系图迭代）的新型匹配算法。与基于RETE的方法不同，CORGI在寻找单个满意匹配时具备二次时间/空间保证，并能通过迭代流式输出后续匹配，而无需将完整冲突集载入内存。CORGI的独特之处在于摒弃了传统用于收集部分匹配的$β$存储器，转而采用两步法：前向传递中构建/维护接地关系图，后向迭代器按需遍历该图生成匹配。这种方法消除了因填充完整冲突集导致的高延迟和内存溢出问题。性能评估显示，在简单组合匹配任务中，CORGI显著优于SOAR和OPS5的RETE实现。"
    },
    {
        "title": "TaoSearchEmb: A Multi-Objective Reinforcement Learning Framework for Dense Retrieval in Taobao Search",
        "summary": "Dense retrieval, as the core component of e-commerce search engines, maps user queries and items into a unified semantic space through pre-trained embedding models to enable large-scale real-time semantic retrieval. Despite the rapid advancement of LLMs gradually replacing traditional BERT architectures for embedding, their training paradigms still adhere to BERT-like supervised fine-tuning and hard negative mining strategies. This approach relies on complex offline hard negative sample construction pipelines, which constrain model iteration efficiency and hinder the evolutionary potential of semantic representation capabilities. Besides, existing multi-task learning frameworks face the seesaw effect when simultaneously optimizing semantic relevance and non-relevance objectives. In this paper, we propose Retrieval-GRPO, a multi-objective reinforcement learning-based dense retrieval framework designed to address these challenges. The method eliminates offline hard negative sample construction by dynamically retrieving Top-K candidate products for each query during training, while introducing a relevance LLM as a reward model to generate real-time feedback. Specifically, the retrieval model dynamically optimizes embedding representations through reinforcement learning, with reward signals combining LLM-generated relevance scores, product quality scores, and multi-way exclusivity metrics to achieve multi-objective user preference alignment and real-time error correction. This mechanism not only removes dependency on hard negatives but also mitigates the seesaw effect through collaborative multi-objective optimization, significantly enhancing the model's semantic generalization capability for complex long-tail queries. Extensive offline and online experiments validate the effectiveness of Retrieval-GRPO, which has been deployed on China's largest e-commerce platform.",
        "entry_id": "http://arxiv.org/abs/2511.13885v1",
        "pub_date": "2025-11-17",
        "translated_summary": "稠密检索作为电商搜索引擎的核心组件，通过预训练嵌入模型将用户查询与商品映射至统一语义空间，以实现大规模实时语义检索。尽管大语言模型的快速发展正逐步替代传统BERT架构进行嵌入，但其训练范式仍遵循类BERT的监督微调与难负例挖掘策略。该方法依赖复杂的离线难负例样本构建流程，制约模型迭代效率并阻碍语义表征能力的进化潜力。此外，现有多任务学习框架在同时优化语义相关性与非相关性目标时存在“跷跷板效应”。本文提出Retrieval-GRPO——基于多目标强化学习的稠密检索框架以应对上述挑战。该方法通过训练期间动态检索每个查询的Top-K候选商品，消除离线难负例构建环节，同时引入相关性大语言模型作为奖励模型生成实时反馈。具体而言，检索模型通过强化学习动态优化嵌入表征，其奖励信号融合了LLM生成的相关性分数、商品质量分及多路排他性指标，实现多目标用户偏好对齐与实时纠偏。该机制不仅消除对难负例的依赖，更通过多目标协同优化缓解跷跷板效应，显著增强模型对复杂长尾查询的语义泛化能力。大量离线与在线实验验证了Retrieval-GRPO的有效性，该方案已在中国头部电商平台完成部署。"
    },
    {
        "title": "CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search",
        "summary": "Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.",
        "entry_id": "http://arxiv.org/abs/2511.15443v1",
        "pub_date": "2025-11-19",
        "translated_summary": "稠密检索已成为现代搜索系统的基础范式，尤其在短视频平台中应用广泛。然而大多数工业系统采用自增强训练流程，依赖历史曝光用户交互作为监督信号。这种模式不可避免地导致信息茧房效应——潜在相关但未被用户接触的内容被排除在训练信号之外，使模型偏向于狭窄保守的检索。本文提出CroPS（跨视角正样本），一种通过多视角引入多样化语义正例的新型检索数据引擎。CroPS从用户查询重构行为（查询层面）、推荐流交互数据（系统层面）以及大语言模型合成的世界知识（知识层面）三个维度增强正信号训练。为有效利用这些异构信号，我们提出分层标签分配策略及其对应的H-InfoNCE损失函数，共同实现细粒度、相关性感知的优化。在大型商业短视频搜索平台快手搜索上的大量实验表明，CroPS在离线评估和在线A/B测试中均显著超越强基线模型，既提升了检索性能又降低了用户查询重构率。目前CroPS已在快手搜索全量部署，每日服务数亿用户。"
    },
    {
        "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation",
        "summary": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",
        "entry_id": "http://arxiv.org/abs/2511.15435v1",
        "pub_date": "2025-11-19",
        "translated_summary": "先进的多模态检索增强生成技术已被广泛应用于增强大型多模态模型的能力，但同时也带来了新的安全隐患。现有对抗性研究揭示了MRAG系统易受知识投毒攻击的脆弱性，此类攻击会诱使检索器召回被植入的污染内容。然而，本研究探索了一种全新攻击范式：仅通过在用户输入的图像中添加人眼难以察觉的扰动来实现对MRAG的视觉攻击，无需操控其他组件。由于微调后的检索器与大规模生成器具有较强鲁棒性，且视觉扰动在RAG链式传播中会持续衰减，该攻击极具挑战性。我们提出了一种创新的分层视觉攻击方法，通过错位干扰MRAG生成器的两个输入源（多模态查询与增强知识）来扰乱其生成过程。进一步设计了分层双阶段策略来获取错位的增强知识：首先破坏跨模态对齐，进而干扰多模态语义对齐，通过优化扰动使检索器从原始数据库中召回无关知识。我们在OK-VQA和InfoSeek两个主流MRAG数据集上进行了广泛实验，采用CLIP系列检索器及BLIP-2、LLaVA两种大型多模态模型作为生成器。实验结果表明，我们的视觉攻击能显著降低MRAG系统的检索与生成性能，验证了该攻击方法的有效性。"
    },
    {
        "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
        "summary": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
        "entry_id": "http://arxiv.org/abs/2511.15408v1",
        "pub_date": "2025-11-19",
        "translated_summary": "在多样化人工文本训练下，大语言模型（LLMs）释放了创意自然语言生成（CNLG）的潜力，为广告、故事创作等应用场景带来价值。然而CNLG仍面临两大核心挑战：（1）多目标灵活性：用户需求往往具有个性化、细粒度、多元化特征，LLMs难以同时满足；（2）阐释复杂性：创意不仅关乎生成，更需理解与诠释深层含义以提升用户感知。这些挑战严重制约现有方法在短文本生成中产出兼具创意与深度的内容。为此，我们聚焦中文起名这一典型短文本CNLG任务——需满足用户对长度、语义、命名学等显性约束，同时提供具有美学价值的阐释。提出NAMeGEn创新多智能体优化框架，通过目标提取、姓名生成、评估验证三阶段迭代循环，兼顾多元化需求并生成精准阐释。为此任务构建含1.7万首古诗的增强美学诗歌语料库，推出配备定制化评估指标的CBNames基准测试。大量实验表明，NAMeGEn无需训练即可在多种LLM基座上超越六类基线方法，有效生成符合个性化需求的创意姓名并产出有意义阐释。"
    },
    {
        "title": "Unveiling Inference Scaling for Difference-Aware User Modeling in LLM Personalization",
        "summary": "Large Language Models (LLMs) are increasingly integrated into users' daily lives, driving a growing demand for personalized outputs. Prior work has primarily leveraged a user's own history, often overlooking inter-user differences that are critical for effective personalization. While recent methods have attempted to model such differences, their feature extraction processes typically rely on fixed dimensions and quick, intuitive inference (System-1 thinking), limiting both the coverage and granularity of captured user differences. To address these limitations, we propose Difference-aware Reasoning Personalization (DRP), a framework that reconstructs the difference extraction mechanism by leveraging inference scaling to enhance LLM personalization. DRP autonomously identifies relevant difference feature dimensions and generates structured definitions and descriptions, enabling slow, deliberate reasoning (System-2 thinking) over user differences. Experiments on personalized review generation demonstrate that DRP consistently outperforms baseline methods across multiple metrics.",
        "entry_id": "http://arxiv.org/abs/2511.15389v1",
        "pub_date": "2025-11-19",
        "translated_summary": "大型语言模型正日益融入用户的日常生活，推动着对个性化输出的需求增长。现有研究主要利用用户自身的历史数据，往往忽视了对于实现有效个性化至关重要的用户间差异。虽然近期方法尝试对此类差异进行建模，但其特征提取过程通常依赖固定维度和快速直观推断（系统1思维），限制了所捕获用户差异的覆盖范围与精细度。为突破这些局限，我们提出差异感知推理个性化框架，该框架通过利用推理扩展重构差异提取机制，以增强大型语言模型的个性化能力。该框架能自主识别相关的差异特征维度，生成结构化定义与描述，从而实现对用户差异的慢速深度推理（系统2思维）。在个性化评论生成任务上的实验表明，该框架在多项指标上持续优于基线方法。"
    },
    {
        "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
        "summary": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
        "entry_id": "http://arxiv.org/abs/2511.15383v1",
        "pub_date": "2025-11-19",
        "translated_summary": "飞机维修技师目前需花费高达30%的工作时间查阅技术手册，这在必须确保每个操作步骤都可追溯至认证来源的航空维修场景中，已成为公认的效率瓶颈。我们开发了一套合规检索系统，通过适配大语言模型重排与语义搜索技术，使其在现有认证查阅系统旁并行运行而非替代原有系统。该系统基于ATA章节层级构建版本鲁棒性向量，并运用视觉语言解析技术对认证内容进行结构化处理，使技术人员既能预览排序任务列表，又能通过原有查阅器获取核验流程。在4.9万条合成查询测试中实现超90%检索准确率；针对10名持证技师的跨语言对照研究显示，系统达成90.9%的前十检索成功率，并将单任务查阅时间从6-15分钟压缩至18秒，降幅达95%。这些成果实证表明，语义检索技术能在严格监管框架下有效运行，并为多语言航空维修实践带来实质性效率提升。"
    },
    {
        "title": "Opinion Dynamics Models for Sentiment Evolution in Weibo Blogs",
        "summary": "Online social media platforms enable influencers to distribute content and quickly capture audience reactions, significantly shaping their promotional strategies and advertising agreements. Understanding how sentiment dynamics and emotional contagion unfold among followers is vital for influencers and marketers, as these processes shape engagement, brand perception, and purchasing behavior. While sentiment analysis tools effectively track sentiment fluctuations, dynamical models explaining their evolution remain limited, often neglecting network structures and interactions both among blogs and between their topic-focused follower groups. In this study, we tracked influential tech-focused Weibo bloggers over six months, quantifying follower sentiment from text-mined feedback. By treating each blogger's audience as a single \"macro-agent\", we find that sentiment trajectories follow the principle of iterative averaging -- a foundational mechanism in many dynamical models of opinion formation, a theoretical framework at the intersection of social network analysis and dynamical systems theory. The sentiment evolution aligns closely with opinion-dynamics models, particularly modified versions of the classical French-DeGroot model that incorporate delayed perception and distinguish between expressed and private opinions. The inferred influence structures reveal interdependencies among blogs that may arise from homophily, whereby emotionally similar users subscribe to the same blogs and collectively shape the shared sentiment expressed within these communities.",
        "entry_id": "http://arxiv.org/abs/2511.15303v1",
        "pub_date": "2025-11-19",
        "translated_summary": "在线社交媒体平台使意见领袖能够发布内容并快速获取受众反馈，这显著影响着他们的推广策略与广告合作。理解追随者群体中的情绪动态与情感传染机制对意见领袖和营销者至关重要，因为这些过程直接塑造着用户参与度、品牌认知与消费行为。尽管情感分析工具能有效追踪情绪波动，但解释其演变规律的动态模型仍相对匮乏，往往忽略了博客间的网络结构及其垂直领域粉丝群之间的互动关系。本研究通过持续六个月追踪科技领域微博大V，基于文本挖掘的粉丝反馈量化其情绪走向。当我们将每位博主的受众视为单一“宏观主体”时，发现情绪轨迹遵循迭代平均原则——这一观点形成动态模型中的基础机制，也是社交网络分析与动态系统理论交叉领域的核心框架。情绪演变规律与观点动力学模型高度吻合，尤其是融合延迟感知机制、区分公开与私人意见的改进版French-DeGroot模型。通过推导出的影响力结构，我们发现博客间存在由同质效应产生的相互依赖：情感倾向相似的用户会订阅相同博客，并共同塑造这些社群内显现的集体情绪。"
    },
    {
        "title": "Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing",
        "summary": "Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.",
        "entry_id": "http://arxiv.org/abs/2511.15241v1",
        "pub_date": "2025-11-19",
        "translated_summary": "计算机化自适应测试是在线教育平台中广泛使用的学习者能力评估技术。该技术通过利用先验能力估计值动态选题，并依据答题结果迭代更新估计值，实现个性化学习者建模，已引起广泛关注。然而现有研究大多聚焦于提升诊断精度，忽视了自适应过程中固有的选择偏差问题。由于选题策略受能力估计值显著影响（如向低能力者分配简单题目，向高能力者分配难题），而题目选择又依赖于先验估计，这种偏差会渗入诊断模型，并在迭代更新过程中不断放大，最终导致预测结果失准。此外，学习者历史交互数据的不平衡性往往加剧诊断模型的偏差。针对该问题，我们提出包含双重核心模块的去偏框架：跨属性考生检索与选择性混合正则化。首先，我们检索具有均衡正误答题分布的考生作为参照基准，以其作为存在偏差考生的中性参照。随后在保持标签一致性的前提下，对存在偏差的考生与其匹配的均衡参照对象实施混合增强。这种数据增强策略有效丰富了偏差冲突样本的多样性，并平滑了选择边界。最终在两大基准数据集上的实验表明，本方法基于多种先进诊断模型，显著提升了计算机化自适应测试中选题策略的泛化能力与公平性。"
    },
    {
        "title": "ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation",
        "summary": "Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.",
        "entry_id": "http://arxiv.org/abs/2511.15141v1",
        "pub_date": "2025-11-19",
        "translated_summary": "近年来，大型语言模型凭借其强大的推理能力和处理冷启动物品的有效性，被广泛用作推荐系统。为更好地适配推荐场景，检索增强生成技术被引入应用。现有RAG方法多基于用户视角，通过检索与目标用户相似的用户购买模式，并将其提供给大语言模型。本研究提出ItemRAG——一种基于物品的RAG方法，该方法从物品间协同购买历史中检索相关物品（而非用户），帮助大语言模型捕捉对推荐有益的物品协同购买模式。特别地，我们的检索策略融合语义相似的物品以优化冷启动物品处理，同时利用协同购买频率提升检索物品的相关性。大量实验表明，ItemRAG在以下方面表现卓越：（1）将基于大语言模型的零样本推荐系统的命中率提升最高达43%；（2）在常规场景与冷启动物品推荐场景下均优于基于用户的RAG基线方法。"
    },
    {
        "title": "Multi-Aspect Cross-modal Quantization for Generative Recommendation",
        "summary": "Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.",
        "entry_id": "http://arxiv.org/abs/2511.15122v1",
        "pub_date": "2025-11-19",
        "translated_summary": "生成式推荐作为推荐系统的新范式，通过量化表征对项目特征进行离散化处理，将用户历史交互行为建模为离散标记序列，并基于该序列采用下一标记预测方法进行项目推荐。该范式的核心挑战在于构建层次清晰、冲突率低且利于生成模型训练的高质量语义标识符。然而，现有方法在利用多模态信息捕捉深层跨模态交互关系方面仍存在局限，而这些特性对学习优质语义标识符和有效训练生成式推荐模型至关重要。为此，我们提出多视角跨模态量化生成式推荐框架MACRec，从多维度将跨模态信息融入语义标识符学习与生成模型训练全过程。具体而言，在标识符学习阶段引入跨模态量化机制，通过多模态信息的互补融合有效降低冲突率，提升码本可用性；同时构建包含显性与隐性对齐的多视角跨模态对齐策略，进一步增强生成式推荐模型的推理能力。我们在三个知名推荐数据集上的实验结果表明，该方法的性能显著优于现有主流方案。"
    },
    {
        "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
        "summary": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.\n  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.\n  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
        "entry_id": "http://arxiv.org/abs/2511.15061v1",
        "pub_date": "2025-11-19",
        "translated_summary": "基因组问答通常需要复杂推理和跨生物医学数据源的整合。GeneGPT通过将领域专用API与OpenAI的代码生成大模型相结合，实现了与基因组数据库的自然语言交互。然而，其依赖专有模型的特性限制了可扩展性，增加了运营成本，并引发了对数据隐私与泛化能力的担忧。\n\n本研究通过采用开源模型（包括Llama 3.1、Qwen2.5及Qwen2.5 Coder）在单体架构中复现GeneGPT，率先通过实验验证了该方案的局限性。在此基础上，我们开发了OpenBioLLM——采用模块化多智能体框架，通过引入工具路由、查询生成和响应验证的智能体分工机制，扩展了GeneGPT的协同推理与角色化任务执行能力。\n\n在超过90%的基准任务中，OpenBioLLM达到或超越了GeneGPT的性能，在Gene-Turing和GeneHop基准上分别取得0.849和0.830的平均分，且仅使用未经额外微调的小型开源模型。该框架的模块化多智能体设计使基准任务延迟降低40-50%，在保持模型能力的同时显著提升效率。综合评估结果凸显了开源多智能体系统在基因组问答领域的潜力。代码与资源已开源：https://github.com/ielab/OpenBioLLM。"
    },
    {
        "title": "SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs",
        "summary": "Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.\n  In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.\n  Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.",
        "entry_id": "http://arxiv.org/abs/2511.14881v1",
        "pub_date": "2025-11-18",
        "translated_summary": "基于深度学习的大规模推荐模型服务化部署面临诸多挑战。现有系统依赖基于CPU的近似最近邻索引与过滤服务，存在不可忽视的资源开销且丧失了联合优化机会。这种低效性导致系统难以支撑更复杂的模型架构，例如基于学习的相似度计算和多任务检索。\n\n本文提出SilverTorch——一种基于GPU的推荐模型服务化系统。该系统通过将独立索引与过滤服务替换为模型服务层，实现了模型服务的统一化。我们提出GPU上的布隆索引算法用于特征过滤，并开发了基于张量原生融合的Int8近似最近邻计算内核。通过协同设计近似最近邻搜索索引与过滤索引，有效降低了GPU显存占用并消除了冗余计算。受益于该服务范式，我们引入了顶层架构评分层与价值模型来实现多任务结果聚合，从而提升检索精度并为未来复杂模型服务研究奠定基础。在排序阶段，该系统通过预缓存服务模型内的物品嵌入向量，显著加速了嵌入计算过程。\n\n在工业级数据集上的评估表明，相较于现有最优方案，SilverTorch可实现延迟降低5.6倍、吞吐量提升23.7倍。实验同时证明，在通过部署更复杂模型提升精度的前提下，该方案比基于CPU的解决方案成本效益高出13.35倍。目前SilverTorch已在多个核心产品中在线服务数百个模型，为日均数十亿活跃用户提供内容推荐服务。"
    },
    {
        "title": "Jasper-Token-Compression-600M Technical Report",
        "summary": "This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.",
        "entry_id": "http://arxiv.org/abs/2511.14405v2",
        "pub_date": "2025-11-18",
        "translated_summary": "本技术报告介绍了2025年11月发布的开源Jasper-Token-Compression-600M模型的训练方法与评估结果。基于先前英文Stella与Jasper模型的蒸馏方案，我们成功将该方法扩展至双语（英文与中文）领域，并通过引入对比学习进一步提升了模型性能。本模型的核心创新在于提出基于一维卷积的令牌压缩模块，在训练过程中动态调整压缩率，使模型能够学习更鲁棒、更高效的压缩文本表示。通过将知识蒸馏与令牌压缩技术相结合，我们在嵌入质量与推理效率方面均实现显著提升。该模型在达到与80亿参数模型相当性能的同时，运行效率显著优于传统6亿参数模型。更多模型发布信息请访问：https://huggingface.co/infgrad/Jasper-Token-Compression-600M。"
    },
    {
        "title": "PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest Neighbor Search",
        "summary": "Similarity searches are a critical task in data mining. As data sets grow larger, exact nearest neighbor searches quickly become unfeasible, leading to the adoption of approximate nearest neighbor (ANN) searches. ANN has been studied for text data, images, and trajectories. However, there has been little effort to develop ANN systems for polygons in spatial database systems and geographic information systems. We present PolyMinHash, a system for approximate polygon similarity search that adapts MinHashing into a novel 2D polygon-hashing scheme to generate short, similarity-preserving signatures of input polygons. Minhash is generated by counting the number of randomly sampled points needed before the sampled point lands within the polygon's interior area, yielding hash values that preserve area-based Jaccard similarity. We present the tradeoff between search accuracy and runtime of our PolyMinHash system. Our hashing mechanism reduces the number of candidates to be processed in the query refinement phase by up to 98% compared to the number of candidates processed by the brute-force algorithm.",
        "entry_id": "http://arxiv.org/abs/2511.16576v1",
        "pub_date": "2025-11-20",
        "translated_summary": "相似性搜索是数据挖掘中的关键任务。随着数据集规模不断扩大，精确最近邻搜索迅速变得不可行，促使近似最近邻搜索技术得到广泛应用。目前针对文本数据、图像和轨迹的近似最近邻搜索已有深入研究，但在空间数据库系统和地理信息系统中，针对多边形的近似最近邻搜索系统开发却鲜有进展。我们提出了PolyMinHash系统，这是一种近似多边形相似性搜索方案，通过将MinHashing算法创新性地适配为二维多边形哈希方案，生成简洁且保持相似性的输入多边形签名。该哈希机制通过统计随机采样点落入多边形内部区域所需的采样次数来生成哈希值，从而保持基于面积的杰卡德相似性。我们展示了PolyMinHash系统在搜索精度与运行时间之间的权衡关系。实验表明，与暴力算法相比，我们的哈希机制在查询优化阶段需要处理的候选数据量最高可减少98%。"
    },
    {
        "title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation",
        "summary": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.\n  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.\n  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.",
        "entry_id": "http://arxiv.org/abs/2511.16543v1",
        "pub_date": "2025-11-20",
        "translated_summary": "将大语言模型（LLM）融入可解释推荐系统时，端到端架构常面临性能与效率的权衡——排序与解释的联合优化往往导致双重妥协。为此，我们提出Prism这一创新解耦框架，将推荐过程严格分离为独立排序阶段和解释生成阶段。\n\n受知识蒸馏启发，Prism引入强力教师LLM（如FLAN-T5-XXL）作为预言机，生成高保真解释性知识；随后由精调后的轻量级学生模型Prism（如BART-Base）专门将这些知识合成为个性化解释。这种解耦架构确保各组件专注于特定目标，消除了耦合模型的内在冲突。\n\n在基准数据集上的大量实验表明：仅1.4亿参数的Prism模型在忠实度与个性化的人类评估中显著优于110亿参数的教师模型，推理速度提升24倍，内存消耗降低10倍。这些结果验证了“解耦+定向蒸馏”能为高质量可解释推荐提供高效可行的技术路径。"
    },
    {
        "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
        "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5$\\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\\times$ faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
        "entry_id": "http://arxiv.org/abs/2511.16528v1",
        "pub_date": "2025-11-20",
        "translated_summary": "神经信息检索系统在高资源语言中表现优异，但在土耳其语等形态复杂、资源相对匮乏的语言中仍有待探索。目前稠密双编码器主导土耳其信息检索领域，而保留词元级表征进行细粒度匹配的延迟交互模型尚未得到系统评估。我们推出TurkColBERT——首个针对土耳其语检索的稠密编码器与延迟交互模型的综合基准。通过两阶段适配流程：先在土耳其自然语言推理/语义文本相似性任务上微调英语和多语言编码器，再利用基于MS MARCO-TR训练的PyLate将其转换为ColBERT风格检索器。我们在覆盖科学、金融及论证领域的五个土耳其BEIR数据集上评估10个模型，结果显示：参数量仅100万的colbert-hash-nano-tr比6亿参数的turkish-e5-large稠密编码器缩小600倍，却能保持其71%以上的平均平均精度。延迟交互模型体积比稠密编码器小3-5倍，性能却显著更优：ColmmBERT-base-TR在特定领域任务中平均精度提升高达13.8%。为满足生产需求，我们对比索引算法：MUVERA+重排序比PLAID快3.33倍，并带来1.7%的相对平均精度提升。这使得ColmmBERT-base-TR在MUVERA下实现0.54毫秒查询延迟。我们公开所有检查点、配置和评估脚本。当前局限包括依赖中等规模数据集（≤5万文档）及翻译基准，可能无法完全反映真实土耳其语检索环境；更大规模的MUVERA评估仍有待进行。"
    },
    {
        "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation",
        "summary": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.\n  This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.",
        "entry_id": "http://arxiv.org/abs/2511.16478v1",
        "pub_date": "2025-11-20",
        "translated_summary": "音乐推荐系统长期依赖信息检索框架，其进展主要通过检索导向子任务的准确度来衡量。这种简化范式虽有效，却难以回答\"何为优质推荐\"的核心问题，而通过用户研究或公平性分析来拓宽评估维度的尝试收效甚微。大语言模型的出现打破了这一框架：其生成式特性与基于排序的传统方法截然不同，使得标准准确度指标不再适用。同时，大语言模型也带来幻觉、知识截止期、非确定性及训练数据不透明等新挑战，导致传统训练/测试方案难以奏效。但另一方面，它们也创造了自然语言交互乃至让模型充当评估者的新机遇。\n\n本文主张，大语言模型驱动的音乐推荐系统需要重建评估体系。我们首先梳理大语言模型如何重塑用户建模、内容建模及自然语言推荐三个维度，继而考察自然语言处理领域的评估实践，提炼适用于音乐推荐系统的方法论与开放挑战。最后通过聚焦提示工程在音乐推荐系统的应用，我们构建出包含成功维度与风险维度的结构化评估框架。本研究旨在为音乐推荐领域提供与时俱进的、具有教学意义的跨学科评估视角。"
    },
    {
        "title": "ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports",
        "summary": "We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.",
        "entry_id": "http://arxiv.org/abs/2511.16438v1",
        "pub_date": "2025-11-20",
        "translated_summary": "我们推出ESGBench——一个基于企业可持续发展报告的ESG可解释问答系统评估基准数据集与评估框架。该基准包含跨越多维度ESG主题的领域扎根问题，并配备人工精校的参考答案与佐证依据，支持对模型推理过程进行细粒度评估。通过对前沿大语言模型在ESGBench上的表现分析，我们揭示了其在事实一致性、答案溯源性及领域适配性方面的核心挑战。该基准旨在推动面向ESG领域的透明可信人工智能系统研究进程。"
    },
    {
        "title": "An Efficient LLM-based Evolutional Recommendation with Locate-Forget-Update Paradigm",
        "summary": "Nowadays, Large Language Models (LLMs) have shown exceptional performance in sequential recommendations, and the adoption of LLM-based recommender systems (LLMRec) is becoming increasingly widespread in existing e-commerce platforms. Despite the impressive performance, the constant high volume of new user-item interactions makes it difficult to adapt to the evolution of user preference over time, especially for LLM-based recommender systems. The challenge arises from the large number of parameters in LLMs, which makes traditional evolution methods (i.e., Re-training or Fine-tuning) impractical. Specifically, Re-training with all interactions results in prohibitively high computational costs. On the other hand, fine-tuning with only new interactions leads to preference forgetting among inactive users, ultimately compromising overall performance. To tackle this problem, we propose EvoRec, an efficient Locate-Forget-Update framework designed for LLM-based recommender systems to model the evolution of user preferences. EvoRec identifies a small set of parameters associated with preference changes and updates them precisely, thereby saving computational resources while maintaining strong recommendation performance. Notably, the modified parameters account for only 30\\% of LoRA adapter parameters, with no additional parameters introduced. Extensive experiments on two real-world datasets demonstrate that, compared to existing methods, EvoRec not only efficiently evolves LLMRec to adapt to the preferences of active users, but also preserves the interests of inactive users from being disturbed during evolution.",
        "entry_id": "http://arxiv.org/abs/2511.16414v1",
        "pub_date": "2025-11-20",
        "translated_summary": "当前，大语言模型在序列推荐任务中展现出卓越性能，基于大语言模型的推荐系统在电商平台中的应用日益广泛。然而，尽管性能优异，持续涌入的新用户-物品交互数据使其难以适应用户偏好的动态演化，这一挑战对大语言模型推荐系统尤为突出。其根源在于大语言模型参数量庞大，使得传统演化方法（如重训练或微调）难以实施：若采用全量交互数据重训练，将产生难以承受的计算开销；若仅用新交互数据微调，又会导致非活跃用户的偏好遗忘，最终影响整体性能。为此，我们提出EvoRec——一个面向大语言模型推荐系统的高效“定位-遗忘-更新”框架，通过精准识别与偏好演化相关的极小参量子集并进行定向更新，在节约计算资源的同时保持强劲的推荐性能。值得注意的是，该方法仅需调整相当于LoRA适配器30%的参数量，且未引入任何额外参数。在两个真实数据集上的大量实验表明，相较于现有方法，EvoRec不仅能高效推动大语言模型推荐系统适应用户偏好演化，还能在演化过程中有效保护非活跃用户的兴趣不受干扰。"
    },
    {
        "title": "ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge this gap, we propose a novel fine-tuning framework that optimizes the retriever for Answer Alignment. Specifically, we first identify high-quality positive chunks by evaluating their sufficiency to generate the correct answer. We then employ a curriculum-based contrastive learning scheme to fine-tune the retriever. This curriculum leverages LLM-constructed Knowledge Graphs (KGs) to generate augmented queries, which in turn mine progressively challenging hard negatives. This process trains the retriever to distinguish the answer-sufficient positive chunks from these nuanced distractors, enhancing its generalization. Extensive experiments on 10 datasets from the Ultradomain and LongBench benchmarks demonstrate that our fine-tuned retriever achieves state-of-the-art performance, improving 14.5% over the base model without substantial architectural modifications and maintaining strong efficiency for long-context RAG. Our work presents a robust and effective methodology for building truly answer-centric retrievers.",
        "entry_id": "http://arxiv.org/abs/2511.16326v1",
        "pub_date": "2025-11-20",
        "translated_summary": "检索增强生成（RAG）已成为处理知识密集型任务的重要框架，但其在长文本场景中的效果常受限于检索器难以识别稀疏却关键的证据。传统检索器虽针对查询-文档相似度进行优化，却往往无法与生成精确答案的下游目标对齐。为弥补这一差距，我们提出了一个创新微调框架，通过答案对齐机制优化检索器。具体而言，我们首先通过评估文本块生成正确答案的充分性来筛选高质量正样本，随后采用基于课程学习的对比训练方案，利用大语言模型构建的知识图谱生成增强查询，进而挖掘难度渐增的困难负样本。该方法训练检索器从精妙设计的干扰项中识别足以支撑答案的正样本，从而提升其泛化能力。在Ultradomain和LongBench基准的10个数据集上的实验表明，经微调的检索器实现了最先进性能，较基础模型提升14.5%且无需重大结构改动，同时在长文本RAG场景中保持高效性。本研究为构建真正以答案为中心的检索器提供了稳健有效的解决方案。"
    },
    {
        "title": "Incorporating Token Importance in Multi-Vector Retrieval",
        "summary": "ColBERT introduced a late interaction mechanism that independently encodes queries and documents using BERT, and computes similarity via fine-grained interactions over token-level vector representations. This design enables expressive matching while allowing efficient computation of scores, as the multi-vector document representations could be pre-computed offline. ColBERT models distance using a Chamfer-style function: for each query token, it selects the closest document token and sums these distances across all query tokens.\n  In our work, we explore enhancements to the Chamfer distance function by computing a weighted sum over query token contributions, where weights reflect the token importance. Empirically, we show that this simple extension, requiring only token-weight training while keeping the multi-vector representations fixed, further enhances the expressiveness of late interaction multi-vector mechanism. In particular, on the BEIR benchmark, our method achieves an average improvement of 1.28\\% in Recall@10 in the zero-shot setting using IDF-based weights, and 3.66\\% through few-shot fine-tuning.",
        "entry_id": "http://arxiv.org/abs/2511.16106v1",
        "pub_date": "2025-11-20",
        "translated_summary": "ColBERT通过引入延迟交互机制，采用BERT分别编码查询和文档，并基于词元级向量表示进行细粒度交互计算相似度。该设计在实现高效表达匹配的同时，支持离线预计算多向量文档表示以提升评分效率。该模型采用Chamfer风格的距离函数：为每个查询词元选取最接近的文档词元，并将所有查询词元的距离求和。\n\n本研究针对Chamfer距离函数提出改进方案，通过计算查询词元贡献度的加权和来增强模型性能，其中权重反映词元重要性。实证研究表明，在保持多向量表示固定的前提下，仅需进行词元权重训练，这一简单扩展即可进一步提升延迟交互多向量机制的表达能力。具体而言，在BEIR基准测试中，基于IDF权重的零样本设置下Recall@10指标平均提升1.28%，而经过少量样本微调后，该指标提升幅度可达3.66%。"
    },
    {
        "title": "QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation",
        "summary": "We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym.",
        "entry_id": "http://arxiv.org/abs/2511.15996v1",
        "pub_date": "2025-11-20",
        "translated_summary": "我们推出QueryGym——一个轻量级、可扩展的Python工具包，专门支持基于大语言模型（LLM）的查询重构。这一工具的开发具有重要意义，因为近期研究表明基于LLM的查询重构能显著提升检索效果。然而，尽管不同研究者曾零散地分享过各自方法的实现，目前仍缺乏统一的工具包来提供标准化的实现方案，这阻碍了公平比较、快速实验、一致性基准测试和可靠部署。QueryGym通过提供统一的框架来解决这一痛点，支持基于LLM的重构方法的实施、执行与比较。该工具包具备五大特性：（1）提供应用多种LLM方法的Python API；（2）采用检索无关接口设计，支持与Pyserini、PyTerrier等后端系统集成；（3）建立集中式提示管理系统，支持版本控制与元数据追踪；（4）内置对BEIR、MS MARCO等基准测试的支持；（5）完全开源的扩展实现，向所有研究者开放。QueryGym已在https://github.com/radinhamidi/QueryGym 公开。"
    },
    {
        "title": "Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing",
        "summary": "Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.",
        "entry_id": "http://arxiv.org/abs/2511.15241v2",
        "pub_date": "2025-11-19",
        "translated_summary": "计算机化自适应测试是在线教育平台中广泛使用的学习者能力评估技术。该技术通过基于能力预估值动态选题，并依据答题结果迭代更新能力估计，实现个性化学习者建模，已引起广泛关注。然而现有研究大多聚焦于提升诊断精度，忽视了自适应过程中固有的选择偏差问题。由于选题策略受能力估计值强烈影响（如向低能力者分配简单题目、向高能力者分配难题），而题目选择又依赖于先验估计，这种偏差会渗入诊断模型，并在迭代更新过程中被不断放大，最终导致预测结果失准。此外，学习者历史交互数据的不平衡性往往会加剧诊断模型的偏差。为解决该问题，我们提出包含双重核心模块的去偏框架：跨属性考生检索与选择性混合正则化。首先检索具有均衡正误答题分布的考生作为偏斜考生的中性参照，随后在保持标签一致性的前提下对偏斜考生与其匹配的均衡参照实施混合增强。该方法有效丰富了偏差冲突样本的多样性，平滑了选择边界。最终在两大基准数据集上的实验表明，我们的方案能显著提升计算机化自适应测试中选题策略的泛化能力与公平性。"
    },
    {
        "title": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback",
        "summary": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.",
        "entry_id": "http://arxiv.org/abs/2511.17255v1",
        "pub_date": "2025-11-21",
        "translated_summary": "大型视觉语言模型（VLMs）能够通过自然语言查询实现直观的视觉搜索。然而，提升其性能通常需要对模型进行微调或扩展至更大规模。本研究受传统文本搜索机制启发，提出在推理阶段通过相关性反馈提升检索性能的方法。该模型无关的设计既可替代微调，也可与经过微调的VLMs协同使用。具体而言，我们针对基于VLM的检索提出并评估了四种反馈策略：首先改进经典伪相关性反馈（PRF），通过top-ranked结果优化查询嵌入；为克服其局限性，提出生成式相关性反馈（GRF），利用合成描述文本进行查询优化；进一步设计注意力反馈汇总器（AFS），这是一种基于Transformer的定制模型，可整合相关项目的多模态细粒度特征；最后通过真实标注文本模拟显式反馈作为性能上限基准。在Flickr30k和COCO数据集上的实验表明，相较于无反馈检索，GRF、AFS与显式反馈能使较小VLM的MRR@5提升3-5%，较大模型提升1-3%。值得注意的是，AFS与显式反馈类似，能有效抑制查询漂移现象，并在迭代式多轮检索中表现出比GRF更强的鲁棒性。本研究证实相关性反馈能持续增强不同VLM的检索性能，为交互式自适应视觉搜索开辟了新路径。"
    },
    {
        "title": "Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters",
        "summary": "Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \\textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.",
        "entry_id": "http://arxiv.org/abs/2511.17044v1",
        "pub_date": "2025-11-21",
        "translated_summary": "参数化检索增强生成（PRAG）是一种新型RAG范式，它通过LoRA适配器对文档进行参数化，将外部知识直接整合到大语言模型中，相比传统RAG方法显著降低了推理成本。然而，现有PRAG方法采用**一对一**的文档编码方案，为每个独立文档配备专属LoRA适配器。这种方案存在两大局限：首先，由于单个LoRA适配器的训练数据有限，会导致数据稀疏问题；其次，在推理过程中需要为每个候选文本段合并新的LoRA适配器与LLM权重，产生了高昂的计算开销。为突破这些限制，我们提出了一种新颖的PRAG文本编码范式——潜在路由编码机制（Poly-PRAG）。在离线编码阶段，我们将文档集的编码视为多任务学习过程，为每个文本段分配唯一任务标识符。通过路由函数调度，仅需少量潜在LoRA适配器即可覆盖整个文本空间的编码需求。在线推理时，该路由函数会根据输入查询动态激活对应的潜在专家子集。我们在多个知识密集型NLP任务上对Poly-PRAG进行了全面评估，大量实验证明该方法的有效性，在四个不同数据集上均取得了最先进的性能表现。"
    },
    {
        "title": "CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation",
        "summary": "The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.",
        "entry_id": "http://arxiv.org/abs/2511.17041v1",
        "pub_date": "2025-11-21",
        "translated_summary": "大规模在线开放课程(MOOC)的快速发展为个性化学习带来了重大挑战，其中概念推荐尤为关键。现有方法通常依赖异构信息网络或知识图谱来捕捉概念关系，并结合知识追踪模型评估学习者认知状态。然而，这些方法因依赖高质量结构化知识图谱而存在明显局限，而现实教育场景中此类图谱往往稀缺。为应对这一根本性挑战，本文提出CLLMRec创新框架，通过两大协同技术支柱利用大语言模型：语义对齐与先修知识蒸馏。语义对齐组件通过编码学习者与概念的非结构化文本描述，构建统一表征空间；先修知识蒸馏范式采用师生架构，由大型教师LLM（实现为先验知识感知组件）从其内化的世界知识中提取概念先修关系，并将其蒸馏为软标签以训练高效的学生排序器。在此基础上，我们的框架引入精细排序机制，通过深度知识追踪显式建模学习者实时认知状态，确保推荐既符合知识结构又契合认知水平。在两个真实MOOC数据集上的大量实验表明，CLLMRec在多项评估指标上显著优于现有基线方法，验证了该框架在不依赖显式结构先验的情况下，能有效生成真正具有认知意识且个性化的概念推荐。"
    },
    {
        "title": "RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers",
        "summary": "Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.",
        "entry_id": "http://arxiv.org/abs/2511.16943v1",
        "pub_date": "2025-11-21",
        "translated_summary": "生成式推荐系统通常利用语义标识符（SID），将每个物品表示为编码语义信息的标记序列。然而，使用多个SID表示物品ID会显著增加输入序列长度，这成为计算复杂度和内存消耗的主要制约因素。现有研究主要聚焦于优化注意力计算和KV缓存，而本文提出RASTP（表征感知语义标记剪枝）方法，直接对输入序列中信息量较低的标记进行剪枝。具体而言，RASTP通过结合语义显著性（通过表征幅度度量）和注意力中心性（源自累积注意力权重）来评估标记重要性。由于RASTP能动态剪枝低信息量或不相关的语义标记，在三个真实亚马逊数据集上的实验表明，该方法在保持或略微提升推荐性能的同时，将训练时间缩短了26.7%。相关代码已开源：https://github.com/Yuzt-zju/RASTP。"
    },
    {
        "title": "δ-EMG: A Monotonic Graph Index for Approximate Nearest Neighbor Search",
        "summary": "Approximate nearest neighbor (ANN) search in high-dimensional spaces is a foundational component of many modern retrieval and recommendation systems. Currently, almost all algorithms follow an $ε$-Recall-Bounded principle when comparing performance: they require the ANN search results to achieve a recall of more than $1-ε$ and then compare query-per-second (QPS) performance. However, this approach only accounts for the recall of true positive results and does not provide guarantees on the deviation of incorrect results. To address this limitation, we focus on an Error-Bounded ANN method, which ensures that the returned results are a $(1/δ)$-approximation of the true values. Our approach adopts a graph-based framework. To enable Error-Bounded ANN search, we propose a $δ$-EMG (Error-bounded Monotonic Graph), which, for the first time, provides a provable approximation for arbitrary queries. By enforcing a $δ$-monotonic geometric constraint during graph construction, $δ$-EMG ensures that any greedy search converges to a $(1/δ)$-approximate neighbor without backtracking. Building on this foundation, we design an error-bounded top-$k$ ANN search algorithm that adaptively controls approximation accuracy during query time. To make the framework practical at scale, we introduce $δ$-EMQG (Error-bounded Monotonic Quantized Graph), a localized and degree-balanced variant with near-linear construction complexity. We further integrate vector quantization to accelerate distance computation while preserving theoretical guarantees. Extensive experiments on the ANN-Benchmarks dataset demonstrate the effectiveness of our approach. Under a recall requirement of 0.99, our algorithm achieves 19,000 QPS on the SIFT1M dataset, outperforming other methods by more than 40\\%.",
        "entry_id": "http://arxiv.org/abs/2511.16921v1",
        "pub_date": "2025-11-21",
        "translated_summary": "高维空间中的近似最近邻搜索是现代检索与推荐系统的核心组件。当前算法在性能比较时普遍遵循$ε$-召回率约束原则：要求搜索结果达到$1-ε$以上的召回率后比较每秒查询率。但这种方法仅考虑正样本召回率，无法保证错误结果的偏离程度。为突破这一局限，我们提出误差有界的近似最近邻方法，确保返回结果与真实值构成$(1/δ)$-近似。该框架采用图结构实现，我们首次提出具备可证明近似能力的$δ$-误差有界单调图，通过在构图阶段强制施加$δ$-单调几何约束，使得任意查询的贪婪搜索无需回溯即可收敛至$(1/δ)$-近似解。基于该结构，我们设计了误差有界的Top-$k$搜索算法，在查询时自适应控制近似精度。为实现大规模应用，我们进一步提出$δ$-误差有界量化图，这种具备局部性与度平衡特性的变体具有近线性构建复杂度，并通过向量量化加速距离计算且保持理论保证。在ANN-Benchmarks数据集上的实验表明，当召回率要求为0.99时，我们的算法在SIFT1M数据集上实现19,000 QPS，较现有方法提升超40%。"
    },
    {
        "title": "Revisiting Feedback Models for HyDE",
        "summary": "Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.",
        "entry_id": "http://arxiv.org/abs/2511.19349v1",
        "pub_date": "2025-11-24",
        "translated_summary": "当前利用大语言模型进行伪相关反馈的研究，在扩展BM25等稀疏检索器的查询时，通常未采用Rocchio、RM3等成熟反馈模型，而是直接将原始查询与LLM生成的扩展内容进行字符串拼接。这种简单处理是否最优？为解答该问题，我们以HyDE方法为研究对象——该方法通过LLM生成假设性答案文档来丰富查询表征，系统性地重新评估了传统反馈模型的应用价值。实验表明：当采用Rocchio等反馈算法对扩展词项进行提取和加权时，HyDE的检索效果可获得显著提升，这为增强基于LLM的伪相关反馈方法准确率提供了一条简洁有效的改进路径。"
    },
    {
        "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
        "summary": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
        "entry_id": "http://arxiv.org/abs/2511.19325v1",
        "pub_date": "2025-11-24",
        "translated_summary": "查询扩展是通过添加语义相关信息对用户查询进行重构的技术，是单语与跨语言信息检索中确保相关文档不被遗漏的关键环节。随着多语种大语言模型（mLLMs）的发展，查询扩展已从同义词和关联词语义增强转向伪文档生成。伪文档不仅能引入更多相关术语，更能弥合简短查询与长文档之间的鸿沟，这对稠密检索尤为有利。本研究通过多种生成式扩展策略评估了当前主流mLLMs及其微调变体，以探究驱动跨语言检索性能的关键因素。结果显示：查询长度很大程度上决定了提示技术的有效性，而更复杂的提示往往无法带来额外收益；语言差异问题依然显著——基线性能最弱的语言通过跨语言查询扩展可获得最大提升，但不同文字体系语言间的检索效果仍不理想；微调仅当训练与测试数据格式相近时才能提升性能。这些发现凸显了建立更均衡的多语言与跨语言训练及评估资源的迫切性。"
    },
    {
        "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
        "summary": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
        "entry_id": "http://arxiv.org/abs/2511.19324v1",
        "pub_date": "2025-11-24",
        "translated_summary": "跨语言信息检索虽能帮助人们获取多语言知识，但由于资源差异、文字体系不同以及嵌入模型中跨语言语义对齐能力较弱，该技术仍面临挑战。现有流程通常依赖翻译和单语检索启发式方法，这会增加计算开销并引入噪声，导致性能下降。本研究系统评估了四种干预类型——文档翻译、基于预训练编码器的多语言稠密检索、在词汇/短语/查询-文档层面的对比学习，以及交叉编码器重排序——在三个基准数据集上的表现。我们发现：专门针对跨语言检索训练的稠密检索模型持续优于词汇匹配方法，且几乎无法从文档翻译中获益；对比学习能有效缓解语言偏见，对初始对齐能力较弱的编码器带来显著提升；重排序虽具有潜力，但其效果取决于交叉编码器训练数据的质量。尽管高资源语言在整体性能上仍占优势，但针对低资源语言及跨文字体系语言对的检索效果，相比词汇匹配和文档翻译基线方法提升最为显著。这些发现表明，跨语言搜索系统应优先考虑基于语义的多语言嵌入和有针对性的学习对齐方法，而非依赖翻译流程，这对跨文字体系及资源匮乏语言尤为重要。"
    },
    {
        "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation",
        "summary": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.",
        "entry_id": "http://arxiv.org/abs/2511.19176v1",
        "pub_date": "2025-11-24",
        "translated_summary": "食谱推荐已成为在线美食平台的核心任务，其关键挑战在于如何有效利用用户-食谱交互之外的多模态特征。分析表明，即使简单运用多模态信号也能取得可观效果，这预示着系统化增强此类信号具有巨大潜力。我们提出TESMR三阶段推荐框架，通过以下方式将原始多模态特征逐步优化为高效嵌入表示：（1）基于多模态理解基础模型的内容增强；（2）通过用户-食谱交互消息传递的关系增强；（3）结合可学习嵌入对比学习的学习增强。在两个真实数据集上的实验表明，TESMR以7-15%的Recall@10提升率超越现有方法。"
    },
    {
        "title": "Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation",
        "summary": "The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users.",
        "entry_id": "http://arxiv.org/abs/2511.18997v1",
        "pub_date": "2025-11-24",
        "translated_summary": "社交媒体平台上短视频的快速传播为推荐系统带来了独特的挑战与机遇。用户偏好呈现多元化特征，不同策略引发的响应常相互冲突，观看时长与视频点击量等指标间甚至可能呈现负相关性。现有提升模型在处理短视频推荐中的异构多策略场景时存在局限，往往难以同时捕捉不同策略的协同效应与独立因果影响。此外，传统固定权重的响应平衡方法缺乏个性化考量，易导致决策偏差。针对这些问题，我们提出一种新颖的异构多策略提升模型（HMUM）框架，用于短视频推荐中的权衡优化。该框架包含离线混合提升建模（HUM）模块——用于捕捉多策略的协同与独立效应，以及在线动态决策（DDM）模块——通过实时评估不同用户响应的价值权重实现个性化决策。在两组公开数据集、一组工业数据集及快手平台的在线A/B测试中，我们的模型展现出卓越的离线性能及关键指标的显著提升。目前该模型已在平台全面部署，服务数亿用户。"
    },
    {
        "title": "STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models",
        "summary": "Ranking models have become an important part of modern personalized recommendation systems. However, significant challenges persist in handling high-cardinality, heterogeneous, and sparse feature spaces, particularly regarding model scalability and efficiency. We identify two key bottlenecks: (i) Representation Bottleneck: Driven by the high cardinality and dynamic nature of features, model capacity is forced into sparse-activated embedding layers, leading to low-rank representations. This, in turn, triggers phenomena like \"One-Epoch\" and \"Interaction-Collapse,\" ultimately hindering model scalability.(ii) Computational Bottleneck: Integrating all heterogeneous features into a unified model triggers an explosion in the number of feature tokens, rendering traditional attention mechanisms computationally demanding and susceptible to attention dispersion. To dismantle these barriers, we introduce STORE, a unified and scalable token-based ranking framework built upon three core innovations: (1) Semantic Tokenization fundamentally tackles feature heterogeneity and sparsity by decomposing high-cardinality sparse features into a compact set of stable semantic tokens; and (2) Orthogonal Rotation Transformation is employed to rotate the subspace spanned by low-cardinality static features, which facilitates more efficient and effective feature interactions; and (3) Efficient attention that filters low-contributing tokens to improve computional efficiency while preserving model accuracy. Across extensive offline experiments and online A/B tests, our framework consistently improves prediction accuracy(online CTR by 2.71%, AUC by 1.195%) and training effeciency (1.84 throughput).",
        "entry_id": "http://arxiv.org/abs/2511.18805v1",
        "pub_date": "2025-11-24",
        "translated_summary": "排序模型已成为现代个性化推荐系统的重要组成部分。然而在处理高基数、异构且稀疏的特征空间时，尤其在模型可扩展性与效率方面仍存在显著挑战。我们识别出两大核心瓶颈：（i）表征瓶颈：受高基数特征动态特性驱动，模型容量被迫集中于稀疏激活的嵌入层，导致低秩表征，进而引发\"单周期\"与\"交互坍缩\"现象，最终制约模型扩展性；（ii）计算瓶颈：将所有异构特征整合至统一模型会引发特征令牌数量激增，使得传统注意力机制计算成本高昂且易受注意力分散影响。为突破这些障碍，我们提出STORE——基于三大核心创新的统一可扩展令牌排序框架：（1）语义令牌化通过将高基数稀疏特征解构为紧凑的稳定语义令牌集合，从根本上解决特征异构性与稀疏性问题；（2）采用正交旋转变换对低基数静态特征张成的子空间进行旋转，促进更高效的特征交互；（3）通过过滤低贡献令牌的高效注意力机制，在保持模型精度的同时提升计算效率。经过大量离线实验与在线A/B测试，本框架持续提升预测准确率（在线CTR提升2.71%，AUC提升1.195%）与训练效率（吞吐量提升1.84倍）。"
    },
    {
        "title": "Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search",
        "summary": "Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.",
        "entry_id": "http://arxiv.org/abs/2511.18749v1",
        "pub_date": "2025-11-24",
        "translated_summary": "大型语言模型为自动化端到端事实核查带来了希望，但先前研究得出的结果好坏参半。随着主流聊天机器人普遍配备推理能力和网络搜索工具——且已有数百万用户依赖其进行信息验证——开展严谨评估已刻不容缓。我们基于PolitiFact核查的6000余条声明，对OpenAI、谷歌、Meta和DeepSeek的15款最新大模型进行评估，对比了标准模型与具备推理能力及网络搜索功能的变体。结果显示：标准模型表现欠佳，推理能力带来的提升微乎其微，尽管网络中存在事实核查记录，网络搜索仅带来有限改进。相比之下，采用PolitiFact摘要的定向检索增强生成系统，在不同模型变体上平均将宏观F1值提升了233%。这些发现表明，为模型提供经过筛选的高质量上下文，是实现自动化事实核查的有效路径。"
    },
    {
        "title": "Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation",
        "summary": "Recent advances in Large Language Models (LLMs) have opened new avenues for sequential recommendation by enabling natural language reasoning over user behavior sequences. A common approach formulates recommendation as a language modeling task, where interaction histories are transformed into prompts and user preferences are learned via supervised fine-tuning. However, these methods operate solely in the textual modality and often miss users' fine-grained interests, especially when shaped by rich visual signals such as product images or movie posters. Multimodal Large Language Models (MLLMs) offer a promising alternative by aligning text and vision in a shared semantic space. A prevalent training paradigm applies Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) to model user preferences. Yet, two core challenges remain: 1) Imbalanced sample hardness, where random negative sampling causes overfitting on easy examples and under-training on hard ones; 2) Cross-modal semantic bias, where the fixed reference model in DPO prevents the policy model from correcting modality misalignments--especially over long sequences. To address these issues, we propose a Multimodal LLM framework that integrates Hardness-aware and Noise-regularized preference optimization for Recommendation (HaNoRec). Specifically, HaNoRec dynamically adjusts optimization weights based on both the estimated hardness of each training sample and the policy model's real-time responsiveness, prioritizing harder examples. It further introduces Gaussian-perturbed distribution optimization on output logits to enhance cross-modal semantic consistency and reduce modality bias inherited from the reference model.",
        "entry_id": "http://arxiv.org/abs/2511.18740v1",
        "pub_date": "2025-11-24",
        "translated_summary": "大型语言模型（LLM）的最新进展通过实现对用户行为序列的自然语言推理，为序列推荐开辟了新途径。主流方法将推荐任务构建为语言建模问题：将交互历史转换为提示文本，通过监督微调学习用户偏好。然而这些方法仅基于文本模态，往往忽略用户的细粒度兴趣——尤其是当这些兴趣由商品图片、电影海报等丰富视觉信号塑造时。多模态大语言模型（MLLM）通过将文本与视觉对齐到共享语义空间，提供了更有前景的解决方案。当前主流训练范式采用监督微调（SFT）与直接偏好优化（DPO）相结合的方式来建模用户偏好，但仍存在两大核心挑战：1）样本难度失衡，随机负采样会导致模型过拟合简单样本而难以样本训练不足；2）跨模态语义偏差，DPO中固定的参考模型会阻碍策略模型修正模态未对齐问题（在长序列中尤为突出）。为此，我们提出HaNoRec多模态大语言模型框架，通过集成面向推荐的硬度感知与噪声正则化偏好优化方法，动态根据样本预估难度和策略模型实时响应度调整优化权重，优先处理困难样本；同时引入输出逻辑的高斯扰动分布优化，以增强跨模态语义一致性，降低参考模型继承的模态偏差。"
    },
    {
        "title": "When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation",
        "summary": "Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.",
        "entry_id": "http://arxiv.org/abs/2511.18717v1",
        "pub_date": "2025-11-24",
        "translated_summary": "序列推荐通过建模用户偏好来预测下一目标项目。现有研究大多采用被动推荐模式，仅在用户打开应用时进行响应，错失了应用关闭后的推荐机会。本文研究主动推荐范式，通过预测下次交互时间并主动推送项目。该模式面临两大挑战：如何精准预测兴趣时间点，以及如何基于预测时间生成兴趣项目。我们提出PASRec——基于扩散模型的推荐框架，通过联合优化目标实现兴趣时间与兴趣项目的协同对齐。在五个基准数据集上的实验表明，该方法在留一法和时间分割两种评估策略下均优于八种前沿基线模型。"
    },
    {
        "title": "A Recommender System Based on Binary Matrix Representations for Cognitive Disorders",
        "summary": "Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy.",
        "entry_id": "http://arxiv.org/abs/2511.18645v1",
        "pub_date": "2025-11-23",
        "translated_summary": "认知障碍（心理健康）诊断是一项精细而复杂的任务。如何从可能的障碍中筛选出最具信息量的待评估症状，构成了额外挑战。该过程需要掌握诊断标准与症状跨障碍重叠的全面知识，仅凭症状本身难以准确判断。本研究旨在开发一种基于二元矩阵表征的认知障碍诊断推荐系统，其核心算法通过构建障碍与症状组合的二元矩阵，根据患者当前症状对行列进行筛选，从而识别潜在障碍并推荐最具诊断价值的下阶段待查症状。研究采用Python实现了该推荐系统的原型，通过合成测试数据与部分真实数据验证，系统成功从初始症状集中识别出合理障碍，给出优化诊断的后续症状建议，并提供症状-障碍关联的补充信息。尽管目前仅为原型系统，但其已展现出作为临床辅助工具的潜力。该推荐系统的完整应用版本有望帮助心理健康从业者更高效地识别相关障碍，并通过针对性症状追踪调查提升诊断准确性。"
    },
    {
        "title": "General Agentic Memory Via Deep Research",
        "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
        "entry_id": "http://arxiv.org/abs/2511.18423v1",
        "pub_date": "2025-11-23",
        "translated_summary": "记忆对AI智能体至关重要，然而当前广泛采用的静态记忆系统试图预先构建现成可用的记忆，这不可避免地会导致严重的信息丢失。为突破这一局限，我们提出名为“通用智能记忆（GAM）”的创新框架。该框架遵循“即时编译”原则，在离线阶段仅保留简洁有效的记忆，而在运行时专注于为客户端生成优化上下文。为实现这一目标，GAM采用双模块设计：1）记忆模块通过轻量级记忆库提炼关键历史信息，同时在通用页面存储中保存完整历史记录；2）研究模块基于预构建记忆的指引，从页面存储中检索并整合在线请求所需的有效信息。这种设计使GAM能充分发挥前沿大语言模型的智能能力与测试时扩展性，同时通过强化学习实现端到端的性能优化。实验结果表明，在多种需要记忆支撑的任务场景中，GAM相较现有记忆系统均取得显著性能提升。"
    },
    {
        "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations",
        "summary": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.",
        "entry_id": "http://arxiv.org/abs/2511.18413v1",
        "pub_date": "2025-11-23",
        "translated_summary": "代理式推荐将推荐系统视为大型语言模型（LLM）智能体，这些智能体能够在网络应用中规划、推理、使用工具并与不同偏好的用户互动。然而，现有的大多数代理式推荐系统主要关注通用的单智能体规划执行流程或多智能体任务分解流程。由于缺乏面向推荐的设计，这些系统往往未能充分利用用户-物品交互历史中的协同信号，导致推荐结果不尽如人意。为解决这一问题，我们借鉴传统协同过滤算法与基于LLM的多智能体协作之间的相似性，提出了面向代理式推荐的多智能体协同过滤框架（MACF）。具体而言，针对目标用户和查询，我们将相似用户和相关物品实例化为具有独特配置文件的LLM智能体。每个智能体能够调用检索工具、推荐候选物品并与其他智能体交互。与传统协同过滤中静态的偏好聚合不同，MACF通过中央协调智能体，采用动态智能体招募和个性化协作指令的方式，自适应地管理用户与物品智能体之间的协作。在三个不同领域数据集上的实验结果表明，我们的MACF框架相较于强大的代理式推荐基线方法具有显著优势。"
    },
    {
        "title": "A Multimodal Conversational Agent for Tabular Data Analysis",
        "summary": "Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.",
        "entry_id": "http://arxiv.org/abs/2511.18405v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大型语言模型（LLM）能够通过与用户进行包含语音交互的情境感知对话，在保持高性能的同时处理数据分析、可视化和解读，从而重塑信息处理模式。本文提出Talk2Data——一个基于多模态LLM驱动的对话式智能体，用于实现直观的数据探索。该系统允许用户通过语音或文本指令查询数据集，并以图表、表格、统计量或语音解释形式获取答案。该架构以LLM为核心，整合了OpenAI Whisper自动语音识别系统、Qwen-coder代码生成模型、定制化沙箱执行工具以及Coqui文本转语音库，形成智能体协同工作回路。与纯文本分析工具不同，该系统支持跨模态响应适配，并能基于数据集上下文进行多轮对话。在三个数据集48项任务的评估中，原型系统实现95.8%准确率，纯模型生成时间低于1.7秒（不含语音识别与执行时间）。通过对五种参数量（1.5B-32B）LLM的对比实验，揭示了准确率-延迟-成本的平衡关系，其中7B模型在交互场景中表现最佳。通过在与用户对话和代码执行之间建立路由机制（约束于透明沙箱），同时将提示词锚定于模式级上下文，Talk2Data智能体在确保计算可验证的前提下，能可靠地从表格数据中提取可操作的洞察。除系统本身外，本文还探讨了该技术对人机数据交互、LLM驱动分析可信度的影响，以及面向大规模多模态助手的未来拓展方向。"
    },
    {
        "title": "Toward an AI-Native Internet: Rethinking the Web Architecture for Semantic Retrieval",
        "summary": "The rise of Generative AI Search is fundamentally transforming how users and intelligent systems interact with the Internet. LLMs increasingly act as intermediaries between humans and web information. Yet the web remains optimized for human browsing rather than AI-driven semantic retrieval, resulting in wasted network bandwidth, lower information quality, and unnecessary complexity for developers. We introduce the concept of an AI-Native Internet, a web architecture in which servers expose semantically relevant information chunks rather than full documents, supported by a Web-native semantic resolver that allows AI applications to discover relevant information sources before retrieving fine-grained chunks. Through motivational experiments, we quantify the inefficiencies of current HTML-based retrieval, and outline architectural directions and open challenges for evolving today's document-centric web into an AI-oriented substrate that better supports semantic access to web content.",
        "entry_id": "http://arxiv.org/abs/2511.18354v1",
        "pub_date": "2025-11-23",
        "translated_summary": "生成式AI搜索的兴起正在从根本上改变用户及智能系统与互联网的交互方式。大语言模型日益成为人类与网络信息之间的中介桥梁。然而当前网络仍以人类浏览体验为核心进行优化，而非面向AI驱动的语义检索，这导致网络带宽浪费、信息质量下降，并为开发者带来不必要的复杂性。我们提出“AI原生互联网”概念——该架构要求服务器提供语义关联的信息块而非完整文档，并辅以网络原生语义解析器，使AI应用能在检索细粒度信息块前先行发现相关信源。通过动机实验，我们量化了当前基于HTML检索模式的效能损耗，并勾勒出将以文档为核心的现有网络演进为AI导向基础设施的架构路径，以及实现网络内容语义化访问所需的开放挑战。"
    },
    {
        "title": "Time Matters: Enhancing Sequential Recommendations with Time-Guided Graph Neural ODEs",
        "summary": "Sequential recommendation (SR) is widely deployed in e-commerce platforms, streaming services, etc., revealing significant potential to enhance user experience. However, existing methods often overlook two critical factors: irregular user interests between interactions and highly uneven item distributions over time. The former factor implies that actual user preferences are not always continuous, and long-term historical interactions may not be relevant to current purchasing behavior. Therefore, relying only on these historical interactions for recommendations may result in a lack of user interest at the target time. The latter factor, characterized by peaks and valleys in interaction frequency, may result from seasonal trends, special events, or promotions. These externally driven distributions may not align with individual user interests, leading to inaccurate recommendations. To address these deficiencies, we propose TGODE to both enhance and capture the long-term historical interactions. Specifically, we first construct a user time graph and item evolution graph, which utilize user personalized preferences and global item distribution information, respectively. To tackle the temporal sparsity caused by irregular user interactions, we design a time-guided diffusion generator to automatically obtain an augmented time-aware user graph. Additionally, we devise a user interest truncation factor to efficiently identify sparse time intervals and achieve balanced preference inference. After that, the augmented user graph and item graph are fed into a generalized graph neural ordinary differential equation (ODE) to align with the evolution of user preferences and item distributions. This allows two patterns of information evolution to be matched over time. Experimental results demonstrate that TGODE outperforms baseline methods across five datasets, with improvements ranging from 10% to 46%.",
        "entry_id": "http://arxiv.org/abs/2511.18347v1",
        "pub_date": "2025-11-23",
        "translated_summary": "序列推荐系统在电商平台和流媒体服务等领域应用广泛，在提升用户体验方面展现出巨大潜力。然而现有方法往往忽略两个关键因素：交互间不规则的兴趣波动，以及随时间高度不均衡的物品分布。前者意味着用户真实偏好并非持续存在，长期历史交互可能与当前购买行为无关，仅依赖这些历史记录进行推荐可能导致目标时刻缺乏用户兴趣指向；后者表现为交互频率的峰谷波动，可能源于季节趋势、特殊事件或促销活动，这种外部驱动的分布模式若与个体兴趣不匹配将导致推荐失准。为解决这些问题，我们提出TGODE模型，通过增强与捕捉长期历史交互实现精准推荐。具体而言，我们首先构建分别融合用户个性化偏好和全局物品分布信息的用户时间图与物品演化图；针对不规则交互导致的时间稀疏性，设计时间引导的扩散生成器自动获取增强型时间感知用户图；同时开发用户兴趣截断因子，有效识别稀疏时间区间并实现均衡偏好推断。随后将增强的用户图与物品图输入广义图神经常微分方程，使其与用户偏好和物品分布的演化过程对齐，实现双模式信息随时间的动态匹配。实验结果表明，TGODE在五个数据集上均优于基线方法，性能提升幅度达10%至46%。"
    },
    {
        "title": "UFO: Unfair-to-Fair Evolving Mitigates Unfairness in LLM-based Recommender Systems via Self-Play Fine-tuning",
        "summary": "Large language model-based Recommender Systems (LRSs) have demonstrated superior recommendation performance by integrating pre-training with Supervised Fine-Tuning (SFT). However, this approach introduces item-side unfairness. Existing studies primarily attribute this issue to the absence of fairness constraints during SFT and attempt to mitigate unfairness via re-weighting and re-ranking methods. In this paper, we find that unfairness arises not only from SFT but also from pre-training, where inherent biases are further amplified during SFT. This finding underscores the failure of current methods to address the root causes of unfairness. Moreover, current methods struggle to preserve satisfactory recommendation performance. To tackle these issues, we propose an Unfair-to-Fair evOlving (UFO) framework using a self-play mechanism, formulating unfairness mitigation as a two-player game. UFO alternates between two player roles: the \\textit{judger}, which identifies unfairness from both pre-training and SFT, and the \\textit{corrector}, which adjusts the LRS to address identified unfairness while preserving recommendation performance. Iterative optimization between these roles enables UFO to completely resolve unfairness. Extensive experiments demonstrate that UFO effectively mitigates unfairness while improving recommendation performance.",
        "entry_id": "http://arxiv.org/abs/2511.18342v1",
        "pub_date": "2025-11-23",
        "translated_summary": "基于大语言模型的推荐系统（LRSs）通过将预训练与监督微调（SFT）相结合，展现出卓越的推荐性能。然而这种方法会引发项目侧的不公平问题。现有研究主要将该问题归因于SFT阶段缺乏公平性约束，并尝试通过重加权和重排序方法缓解不公平性。本文发现，不公平性不仅源于SFT阶段，预训练阶段固有的偏见也会在SFT过程中被进一步放大。这一发现揭示了现有方法未能解决不公平性根本原因的局限性。此外，现有方法难以保持令人满意的推荐性能。为解决这些问题，我们提出基于自我博弈机制的“不公平-公平演化”（UFO）框架，将不公平缓解问题构建为双玩家博弈。UFO交替执行两种角色：\\textit{评判者}（从预训练和SFT中识别不公平现象）与\\textit{校正者}（在保持推荐性能的同时调整LRS以解决已识别的不公平问题）。通过角色间的迭代优化，UFO能彻底消除不公平性。大量实验表明，UFO在有效缓解不公平性的同时，还能提升推荐性能。"
    },
    {
        "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search",
        "summary": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.",
        "entry_id": "http://arxiv.org/abs/2511.18313v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大型语言模型智能体在从知识库中检索上下文时，常因知识库结构与当前推理状态缺乏一致性而导致推理链条断裂。我们提出路径约束检索方法，通过将图结构约束与语义搜索相结合，确保检索信息在知识图谱中保持逻辑关联。该方法将搜索范围限定在锚点节点可达的节点子集，从根源上避免因检索结构断裂信息而引发的推理不一致问题。我们在PathRAG-6基准上验证该方法，该基准覆盖六大领域、包含180个节点和360条边。实验表明：相较于基线方法24%-32%的结构一致性，PCR实现了完全结构一致性，同时保持优异的相关性评分；在技术领域，PCR在排序10位时实现100%相关性且保持完全结构一致性，显著优于向量搜索与混合检索；与基线相比，PCR使检索上下文的平均图距离降低78%，证明其能获取结构更一致的信息。这些发现表明，路径约束检索能有效提升LLM智能体推理系统的可靠性与连贯性。"
    },
    {
        "title": "Large Language Model Enhanced Graph Invariant Contrastive Learning for Out-of-Distribution Recommendation",
        "summary": "Out-of-distribution (OOD) generalization has emerged as a significant challenge in graph recommender systems. Traditional graph neural network algorithms often fail because they learn spurious environmental correlations instead of stable causal relationships, leading to substantial performance degradation under distribution shifts. While recent advancements in Large Language Models (LLMs) offer a promising avenue due to their vast world knowledge and reasoning capabilities, effectively integrating this knowledge with the fine-grained topology of specific graphs to solve the OOD problem remains a significant challenge. To address these issues, we propose {$\\textbf{Inv}$ariant $\\textbf{G}$raph $\\textbf{C}$ontrastive Learning with $\\textbf{LLM}$s for Out-of-Distribution Recommendation (InvGCLLM)}, an innovative causal learning framework that synergistically integrates the strengths of data-driven models and knowledge-driven LLMs. Our framework first employs a data-driven invariant learning model to generate causal confidence scores for each user-item interaction. These scores then guide an LLM to perform targeted graph refinement, leveraging its world knowledge to prune spurious connections and augment missing causal links. Finally, the structurally purified graphs provide robust supervision for a causality-guided contrastive learning objective, enabling the model to learn representations that are resilient to spurious correlations. Experiments conducted on four public datasets demonstrate that InvGCLLM achieves significant improvements in out-of-distribution recommendation, consistently outperforming state-of-the-art baselines.",
        "entry_id": "http://arxiv.org/abs/2511.18282v1",
        "pub_date": "2025-11-23",
        "translated_summary": "分布外泛化已成为图推荐系统面临的重大挑战。传统图神经网络算法因学习虚假的环境相关性而非稳定的因果关系，在分布变化下会出现显著性能衰退。尽管大语言模型凭借其丰富的世界知识和推理能力为此提供了新思路，但如何有效融合其知识体系与具体图谱的细粒度拓扑结构以解决分布外问题仍具挑战。为此，我们提出基于大语言模型的因果不变图对比学习框架InvGCLLM，该创新性因果学习框架实现了数据驱动模型与知识驱动大语言模型的协同融合。该框架首先通过数据驱动的因果不变学习模型生成用户-物品交互的因果置信度，进而引导大语言模型基于世界知识执行定向图结构优化——剪除虚假连接并补全缺失因果边。最终，经结构纯化的图谱为因果引导的对比学习目标提供鲁棒监督信号，使模型能够学习抵御虚假相关性的表征。在四个公开数据集上的实验表明，InvGCLLM在分布外推荐任务中实现显著提升，持续超越现有最优基线模型。"
    },
    {
        "title": "Democratic Recommendation with User and Item Representatives Produced by Graph Condensation",
        "summary": "The challenges associated with large-scale user-item interaction graphs have attracted increasing attention in graph-based recommendation systems, primarily due to computational inefficiencies and inadequate information propagation. Existing methods provide partial solutions but suffer from notable limitations: model-centric approaches, such as sampling and aggregation, often struggle with generalization, while data-centric techniques, including graph sparsification and coarsening, lead to information loss and ineffective handling of bipartite graph structures. Recent advances in graph condensation offer a promising direction by reducing graph size while preserving essential information, presenting a novel approach to mitigating these challenges. Inspired by the principles of democracy, we propose \\textbf{DemoRec}, a framework that leverages graph condensation to generate user and item representatives for recommendation tasks. By constructing a compact interaction graph and clustering nodes with shared characteristics from the original graph, DemoRec significantly reduces graph size and computational complexity. Furthermore, it mitigates the over-reliance on high-order information, a critical challenge in large-scale bipartite graphs. Extensive experiments conducted on four public datasets demonstrate the effectiveness of DemoRec, showcasing substantial improvements in recommendation performance, computational efficiency, and robustness compared to SOTA methods.",
        "entry_id": "http://arxiv.org/abs/2511.18279v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大规模用户-物品交互图带来的挑战在基于图的推荐系统中日益受到关注，主要源于计算效率低下与信息传播不足。现有方法虽提供部分解决方案，但存在明显局限：以模型为中心的方法（如采样与聚合）常面临泛化能力不足，而以数据为中心的技术（如图稀疏化与粗化）则会导致信息丢失及对二分图结构处理失效。图压缩技术的最新进展通过缩减图规模同时保留关键信息，为应对这些挑战提供了新思路。受民主原则启发，我们提出\\textbf{DemoRec}框架，利用图压缩生成用户与物品代表节点以完成推荐任务。通过构建紧凑交互图并聚类原图中具有共同特征的节点，DemoRec显著降低图规模与计算复杂度，同时有效缓解大规模二分图中过度依赖高阶信息的关键问题。在四个公开数据集上的大量实验表明，DemoRec在推荐性能、计算效率和鲁棒性方面均优于现有最优方法，展现出显著优势。"
    },
    {
        "title": "LLM Reasoning for Cold-Start Item Recommendation",
        "summary": "Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.",
        "entry_id": "http://arxiv.org/abs/2511.18261v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大型语言模型凭借其内在的推理能力与海量知识库，在改进推荐系统方面展现出巨大潜力。然而现有研究主要聚焦于用户-物品交互数据充足的暖启动场景，对于交互数据稀疏、传统协同过滤方法难以奏效的冷启动场景探索不足。为突破这一局限，我们针对Netflix领域的冷启动物品推荐提出创新推理策略。该方法利用大型语言模型的先进推理能力，有效推断用户偏好，尤其适用于新上架或交互极少的物品。我们系统评估了监督微调、基于强化学习的微调以及融合两种方法的混合方案，以优化推荐性能。基于真实数据的大规模实验表明，该方案在冷启动推荐场景中实现了方法论效能与实际性能的双重提升。值得注意的是，基于推理的微调模型在特定情况下比Netflix现行排序模型性能提升最高达8%。"
    }
]