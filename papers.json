[
    {
        "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
        "summary": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems.",
        "entry_id": "http://arxiv.org/abs/2511.04541v1",
        "pub_date": "2025-11-06",
        "translated_summary": "跨领域建模用户偏好始终是板面推荐（即推荐有序项目序列）研究中的核心挑战。我们探索了大型语言模型如何通过板面对比推理，有效构建用户偏好的世界模型。我们在涵盖三个不同数据集的实验任务中，对多种大语言模型进行了实证研究。结果表明，任务表现与大语言模型所捕捉的偏好函数特性存在关联性，这为改进方向提供了线索，同时彰显了大语言模型作为推荐系统世界模型的潜力。"
    },
    {
        "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables",
        "summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research.",
        "entry_id": "http://arxiv.org/abs/2511.04491v1",
        "pub_date": "2025-11-06",
        "translated_summary": "现有的表格推理基准大多基于小型统一表格测试模型，未能充分体现现实数据的复杂性，导致对大型语言模型推理能力的评估存在局限。真实场景中的表格往往具有长篇幅、异构性和领域专有特征，既包含结构化字段又混合自由文本，需要模型在数千个标记范围内进行多跳推理。为弥补这一空白，我们推出RUST-BENCH基准数据集，涵盖2031个真实世界表格中的7966个问题，涉及两大领域：i) RB-Science（美国国家科学基金会资助记录）和ii) RB-Sports（NBA统计数据）。与先前研究不同，RUST-BENCH从数据规模、异构性、领域专有性和推理复杂性四个维度对LLMs进行联合评估。开源与商业模型的实验表明，当前LLMs在处理异构表格结构和复杂多跳推理时存在明显困难，暴露出架构设计和提示策略的持续缺陷。RUST-BENCH为推进表格推理研究建立了一个具有挑战性的新测试平台。"
    },
    {
        "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs",
        "summary": "Retrieval of information from graph-structured knowledge bases represents a\npromising direction for improving the factuality of LLMs. While various\nsolutions have been proposed, a comparison of methods is difficult due to the\nlack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a framework for generating high-quality synthetic\nKnowledge Graph Question Answering datasets from any Knowledge Graph, providing\nthe full set of ground-truth facts in the KG to reason over each question. We\nshow how, in addition to enabling more informative benchmarking of KG\nretrievers, the data produced with SynthKGQA also allows us to train better\nmodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset\ndesigned to test zero-shot generalization abilities of KG retrievers with\nrespect to unseen graph structures and relation types, and benchmark popular\nsolutions for KG-augmented LLMs on it.",
        "entry_id": "http://arxiv.org/abs/2511.04473v1",
        "pub_date": "2025-11-06",
        "translated_summary": "从图结构知识库中检索信息是提升大语言模型事实准确性的重要方向。虽然已有多种解决方案被提出，但由于缺乏具有图检索真值标注的挑战性问答数据集，不同方法间的比较仍存在困难。我们推出了SynthKGQA框架，该框架能够基于任意知识图谱生成高质量的知识图谱问答合成数据集，并为每个问题提供知识图谱中完整的真值事实链以供推理验证。研究表明，通过SynthKGQA生成的数据不仅能实现更具信息量的知识图谱检索器基准测试，还能训练出更优质的模型。我们将该框架应用于维基数据知识图谱，构建了GTSQA数据集——该数据集专门用于测试知识图谱检索器在面对未见图结构和关系类型时的零样本泛化能力，并在此数据集上对当前主流的知识图谱增强型大语言模型解决方案进行了基准评估。"
    },
    {
        "title": "On the Brittleness of CLIP Text Encoders",
        "summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy.",
        "entry_id": "http://arxiv.org/abs/2511.04247v1",
        "pub_date": "2025-11-06",
        "translated_summary": "近年来，多模态协同嵌入模型（特别是CLIP）通过将图像与文本在共享表征空间中对齐，推动了零样本分类和多媒体信息检索领域的发展。然而，基于对比对齐训练的此类模型对输入微小扰动缺乏稳定性。尤其在处理人工表述的查询时，查询语句的细微变化可能导致最佳匹配结果的排序产生显著差异。本文系统分析了多媒体信息检索场景中多类非语义查询扰动的影响，基于TRECVID特设视频搜索查询和V3C1视频数据集，对多种CLIP变体进行了词汇、句法和语义层面的扰动测试。研究发现：所有模型均对句法和语义扰动最为敏感，而鲁棒性薄弱环节主要集中在标点符号、大小写等表层文本编辑。这一结果表明，在基准准确率之外，鲁棒性应成为评估视觉语言模型的关键维度。"
    },
    {
        "title": "Denoised Recommendation Model with Collaborative Signal Decoupling",
        "summary": "Although the collaborative filtering (CF) algorithm has achieved remarkable\nperformance in recommendation systems, it suffers from suboptimal\nrecommendation performance due to noise in the user-item interaction matrix.\nNumerous noise-removal studies have improved recommendation models, but most\nexisting approaches conduct denoising on a single graph. This may cause\nattenuation of collaborative signals: removing edges between two nodes can\ninterrupt paths between other nodes, weakening path-dependent collaborative\ninformation. To address these limitations, this study proposes a novel\nGNN-based CF model called DRCSD for denoising unstable interactions. DRCSD\nincludes two core modules: a collaborative signal decoupling module (decomposes\nsignals into distinct orders by structural characteristics) and an order-wise\ndenoising module (performs targeted denoising on each order). Additionally, the\ninformation aggregation mechanism of traditional GNN-based CF models is\nmodified to avoid cross-order signal interference until the final pooling\noperation. Extensive experiments on three public real-world datasets show that\nDRCSD has superior robustness against unstable interactions and achieves\nstatistically significant performance improvements in recommendation accuracy\nmetrics compared to state-of-the-art baseline models.",
        "entry_id": "http://arxiv.org/abs/2511.04237v1",
        "pub_date": "2025-11-06",
        "translated_summary": "尽管协同过滤算法在推荐系统中取得了显著成效，但由于用户-物品交互矩阵中的噪声干扰，其推荐性能仍存在优化空间。现有大量去噪研究通过提升推荐模型性能，但多数方法仅在单一图结构上进行去噪操作，这可能导致协同信号衰减：移除两个节点间的边会中断其他节点间的路径，从而削弱依赖路径的协同信息。针对这些局限性，本研究提出名为DRCSD的新型图神经网络协同过滤模型，专门用于处理不稳定交互的噪声问题。该模型包含两个核心模块：协同信号解耦模块（根据结构特征将信号分解为不同阶数）和分阶去噪模块（对各阶信号进行针对性去噪）。此外，本研究改进了传统基于图神经网络的协同过滤模型的信息聚合机制，避免跨阶信号干扰直至最终池化操作。在三个真实公开数据集上的大量实验表明，DRCSD对不稳定交互具有卓越的鲁棒性，且在推荐准确性指标上相比现有最优基线模型实现了统计学意义上的显著提升。"
    },
    {
        "title": "Coordination-Free Lane Partitioning for Convergent ANN Search",
        "summary": "Production vector search systems often fan out each query across parallel\nlanes (threads, replicas, or shards) to meet latency service-level objectives\n(SLOs). In practice, these lanes rediscover the same candidates, so extra\ncompute does not increase coverage. We present a coordination-free lane\npartitioner that turns duplication into complementary work at the same cost and\ndeadline. For each query we (1) build a deterministic candidate pool sized to\nthe total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)\nassign each lane a disjoint slice of positions. Lanes then return different\nresults by construction, with no runtime coordination.\n  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT\nfeature vectors) with Hierarchical Navigable Small World graphs (HNSW)\nrecall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%\nto 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to\n0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted\nfile (IVF) indexes we see smaller but consistent gains (for example, +11% on MS\nMARCO) by de-duplicating list routing. A microbenchmark shows planner overhead\nof ~37 microseconds per query (mean at the main setting) with linear growth in\nthe number of merged candidates.\n  These results yield a simple operational guideline: size the per-query pool\nto the total budget, deterministically partition positions across lanes, and\nturn redundant fan-out into complementary coverage without changing budget or\ndeadline.",
        "entry_id": "http://arxiv.org/abs/2511.04221v1",
        "pub_date": "2025-11-06",
        "translated_summary": "生产环境中的向量检索系统通常会将每个查询并行分发至多个执行通道（线程、副本或分片）以满足延迟服务等级目标。实践中这些通道往往会重复发现相同候选结果，导致额外计算资源无法提升结果覆盖度。我们提出一种无协调机制的通道分区策略，在保持相同成本与截止时间的前提下，将重复计算转化为互补性工作。针对每个查询，我们：（1）构建确定性的候选池，其容量与总top-k预算相匹配；（2）施加每查询专属的伪随机排列；（3）为每个通道分配互不重叠的位置区间。通过这种构造方式，各通道无需运行时协调即可返回差异化结果。\n\n在四通道配置（总候选预算64）的同等成本下，基于分层导航小世界图的SIFT1M数据集（100万SIFT特征向量）实验中，召回率@10从0.249提升至0.999，同时通道重叠率从近100%降至0%。在MS MARCO数据集（880万文本段）的HNSW实验中，命中率@10从0.200提升至0.601，平均倒数排名@10从0.133提升至0.330。对于倒排文件索引，通过列表路由去重实现了较小但稳定的增益（如在MS MARCO上提升11%）。微基准测试显示，规划器每查询开销约37微秒（主要配置下的均值），且随合并候选数线性增长。\n\n这些成果衍生出简明运维指南：将每查询候选池容量设置为总预算规模，通过确定性位置分区分配至各通道，即可在不改变预算与截止时间的前提下，将冗余分发转化为互补性覆盖。"
    },
    {
        "title": "Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance",
        "summary": "University students face immense challenges during their undergraduate lives,\noften being deprived of personalized on-demand guidance that mentors fail to\nprovide at scale. Digital tools exist, but there is a serious lack of\ncustomized coaching for newcomers. This paper presents an AI-powered chatbot\nthat will serve as a mentor for the students of BRAC University. The main\ncomponent is a data ingestion pipeline that efficiently processes and updates\ninformation from diverse sources, such as CSV files and university webpages.\nThe chatbot retrieves information through a hybrid approach, combining BM25\nlexical ranking with ChromaDB semantic retrieval, and uses a Large Language\nModel, LLaMA-3.3-70B, to generate conversational responses. The generated text\nwas found to be semantically highly relevant, with a BERTScore of 0.831 and a\nMETEOR score of 0.809. The data pipeline was also very efficient, taking 106.82\nseconds for updates, compared to 368.62 seconds for new data. This chatbot will\nbe able to help students by responding to their queries, helping them to get a\nbetter understanding of university life, and assisting them to plan better\nroutines for their semester in the open-credit university.",
        "entry_id": "http://arxiv.org/abs/2511.04172v1",
        "pub_date": "2025-11-06",
        "translated_summary": "大学生在本科阶段面临诸多挑战，往往难以获得导师大规模提供的个性化即时指导。虽然现有数字化工具，但针对新生的定制化辅导严重匮乏。本文提出一款人工智能聊天机器人，将为BRAC大学学生提供导师式服务。该系统的核心组件是数据摄取管道，能高效处理并实时更新来自CSV文件和大学网页等多源信息。通过结合BM25词汇排序与ChromaDB语义检索的混合检索机制，并基于LLaMA-3.3-70B大语言模型生成对话响应，该聊天机器人展现出优异的语义相关性——BERTScore达0.831，METEOR评分达0.809。数据管道处理效率显著，更新现有数据仅需106.82秒，而处理新数据也仅需368.62秒。这款智能助手将有效解答学生疑问，帮助其深入理解大学生活，并在开放学分制下规划更合理的学期安排。"
    },
    {
        "title": "E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce",
        "summary": "Finding relevant products given a user query plays a pivotal role in an\ne-commerce platform, as it can spark shopping behaviors and result in revenue\ngains. The challenge lies in accurately predicting the correlation between\nqueries and products. Recently, mining the cross-features between queries and\nproducts based on the commonsense reasoning capacity of Large Language Models\n(LLMs) has shown promising performance. However, such methods suffer from high\ncosts due to intensive real-time LLM inference during serving, as well as human\nannotations and potential Supervised Fine Tuning (SFT). To boost efficiency\nwhile leveraging the commonsense reasoning capacity of LLMs for various\ne-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation\nEnhancer (E-CARE). During inference, models augmented with E-CARE can access\ncommonsense reasoning with only a single LLM forward pass per query by\nutilizing a commonsense reasoning factor graph that encodes most of the\nreasoning schema from powerful LLMs. The experiments on 2 downstream tasks show\nan improvement of up to 12.1% on precision@5.",
        "entry_id": "http://arxiv.org/abs/2511.04087v1",
        "pub_date": "2025-11-06",
        "translated_summary": "在电子商务平台中，根据用户查询推荐相关商品具有关键作用，它能有效激发购物行为并带来收入增长。核心挑战在于如何准确预测查询与商品之间的关联性。近期研究显示，基于大语言模型的常识推理能力挖掘查询与商品间的交叉特征已展现出显著效果。然而，这类方法因需在服务期间进行密集的实时大语言模型推理，同时依赖人工标注及潜在的监督微调，导致成本高昂。为在提升效率的同时充分利用大语言模型的常识推理能力应对各类电商任务，我们提出高效常识增强推荐框架E-CARE。该框架通过构建常识推理因子图，将大部分来自强大语言模型的推理模式进行编码，使得增强后的模型在推理时仅需对每个查询执行单次大语言模型前向传播即可获得常识推理能力。在两项下游任务上的实验表明，该框架使精确率@5指标最高提升12.1%。"
    },
    {
        "title": "Publication Trend in DESIDOC Journal of Library and Information Technology during 2013-2017: A Scientometric Approach",
        "summary": "DESIDOC Journal of Library & Information Technology (DJLIT) formerly known as\nDESIDOC Bulletin of Information Technology is a peer-reviewed, open access,\nbimonthly journal. This paper presents a Scientometric analysis of the DESIDOC\nJournal. The paper analyses the pattern of growth of the research output\npublished in the journal, pattern of authorship, author productivity, and,\nsubjects covered to the papers over the period (2013-2017). It is found that\n227 papers were published during the period of study (2001-2012). The maximum\nnumbers of articles were collaborative in nature. The subject concentration of\nthe journal noted is Scientometrics. The maximum numbers of articles (65%) have\nranged their thought contents between 6 and 10 pages. The study applied\nstandard formula and statistical tools to bring out the factual result.",
        "entry_id": "http://arxiv.org/abs/2511.04082v1",
        "pub_date": "2025-11-06",
        "translated_summary": "《DESIDOC图书馆与信息科技杂志》（DJLIT）前身为《DESIDOC信息科技通报》，是一本经过同行评审的开放获取双月刊。本文对该期刊进行了科学计量学分析，重点研究了2013-2017年间刊载研究成果的产出增长模式、作者合作模式、作者生产力及论文主题分布。研究发现，在观测周期（2001-2012年）内共发表227篇论文，其中大多数论文为合作研究成果。该期刊的核心主题领域为科学计量学，65%的论文篇幅集中在6-10页范围内。本研究通过标准公式与统计工具得出了客观结论。"
    },
    {
        "title": "Caption Injection for Optimization in Generative Search Engine",
        "summary": "Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation\n(RAG) techniques and Large Language Models (LLMs) to integrate multi-source\ninformation and provide users with accurate and comprehensive responses. Unlike\ntraditional search engines that present results in ranked lists, GSEs shift\nusers' attention from sequential browsing to content-driven subjective\nperception, driving a paradigm shift in information retrieval. In this context,\nenhancing the subjective visibility of content through Generative Search Engine\nOptimization (G-SEO) methods has emerged as a new research focus. With the\nrapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)\ntechniques, GSEs can now efficiently integrate text, images, audio, and video,\nproducing richer responses that better satisfy complex information needs.\nExisting G-SEO methods, however, remain limited to text-based optimization and\nfail to fully exploit multimodal data. To address this gap, we propose Caption\nInjection, the first multimodal G-SEO approach, which extracts captions from\nimages and injects them into textual content, integrating visual semantics to\nenhance the subjective visibility of content in generative search scenarios. We\nsystematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under\nboth unimodal and multimodal settings. Experimental results show that Caption\nInjection significantly outperforms text-only G-SEO baselines under the G-Eval\nmetric, demonstrating the necessity and effectiveness of multimodal integration\nin G-SEO to improve user-perceived content visibility.",
        "entry_id": "http://arxiv.org/abs/2511.04080v1",
        "pub_date": "2025-11-06",
        "translated_summary": "生成式搜索引擎（GSEs）通过检索增强生成技术与大语言模型整合多源信息，为用户提供精准全面的回答。与传统搜索引擎呈现排序列表的方式不同，GSEs将用户注意力从顺序浏览转向内容驱动的主观感知，推动信息检索范式变革。在此背景下，如何通过生成式搜索引擎优化方法提升内容的主观可见性成为新兴研究热点。随着多模态检索增强生成技术的快速发展，GSEs已能高效整合文本、图像、音频和视频，生成更丰富的响应以满足复杂信息需求。然而现有G-SEO方法仍局限于文本优化，未能充分利用多模态数据。为此，我们提出首项多模态G-SEO方法——字幕注入，通过提取图像描述文本并将其注入文本内容，融合视觉语义以增强生成式搜索场景下内容的主观可见性。我们在多模态检索增强生成基准MRAMG上系统评估了该方法在单模态与多模态设置下的表现。实验结果表明，在G-Eval指标下，字幕注入方法显著优于纯文本G-SEO基线，验证了多模态整合在提升用户感知内容可见性方面的必要性与有效性。"
    },
    {
        "title": "Two Decades of Research at the University of Lagos (2004-2023): A Scientometric Analysis of Productivity, Collaboration, and Impact",
        "summary": "This paper presents a scientometric analysis of research output from the\nUniversity of Lagos, focusing on the two decades spanning 2004 to 2023. Using\nbibliometric data retrieved from the Web of Science, we examine trends in\npublication volume, collaboration patterns, citation impact, and the most\nprolific authors, departments, and research domains at the university. The\nstudy reveals a consistent increase in research productivity, with the highest\npublication output recorded in 2023. Health Sciences, Engineering, and Social\nSciences are identified as dominant fields, reflecting the university's\ninterdisciplinary research strengths. Collaborative efforts, both locally and\ninternationally, show a positive correlation with higher citation impact, with\nthe United States and the United Kingdom being the leading international\ncollaborators. Notably, open-access publications account for a significant\nportion of the university's research output, enhancing visibility and citation\nrates. The findings offer valuable insights into the university's research\nperformance over the past two decades, providing a foundation for strategic\nplanning and policy formulation to foster research excellence and global\nimpact.",
        "entry_id": "http://arxiv.org/abs/2511.04075v1",
        "pub_date": "2025-11-06",
        "translated_summary": "本文对拉各斯大学2004至2023二十年间的研究产出展开科学计量分析。基于Web of Science检索的文献计量数据，我们系统考察了该校的论文发表趋势、合作模式、引用影响力以及核心作者、院系和研究领域。研究表明：该校科研生产力持续增长，2023年达到发文峰值；健康科学、工程学与社会科学构成三大优势学科，彰显跨学科研究实力；本地与国际合作均与高引用影响力呈正相关，其中美国与英国为主要国际合作对象。值得关注的是，开放获取论文在全校研究成果中占比显著，有效提升了学术可见度与引用率。这些发现为评估该校近二十年的科研绩效提供了重要依据，可为优化学术发展战略与政策制定提供参考，从而进一步提升研究质量与全球影响力。"
    },
    {
        "title": "Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters",
        "summary": "Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest\nvectors for a query vector from a dataset. It enforces that a specified set of\ndiscrete labels $S$ for the query must be included in the labels of each\nretrieved vector. Existing graph-based methods typically incorporate filter\nawareness by assigning fixed penalties or prioritizing nodes based on filter\nsatisfaction. However, since these methods use fixed, data in- dependent\npenalties, they often fail to generalize across datasets with diverse label and\nvector distributions. In this work, we propose a principled alternative that\nlearns the optimal trade-off between vector distance and filter match directly\nfrom the data, rather than relying on fixed penalties. We formulate this as a\nconstrained linear optimization problem, deriving weights that better reflect\nthe underlying filter distribution and more effectively address the filtered\nANN search problem. These learned weights guide both the search process and\nindex construction, leading to graph structures that more effectively capture\nthe underlying filter distribution and filter semantics. Our experiments\ndemonstrate that adapting the distance function to the data significantly im-\nproves accuracy by 5-10% over fixed-penalty methods, providing a more flexible\nand generalizable framework for the filtered ANN search problem.",
        "entry_id": "http://arxiv.org/abs/2511.04073v1",
        "pub_date": "2025-11-06",
        "translated_summary": "过滤式近似最近邻搜索能够从数据集中为查询向量检索最接近的向量，其核心要求是查询向量指定的离散标签集$S$必须包含于每个被检索向量的标签集合中。现有基于图的方法通常通过固定惩罚值或基于过滤条件满足程度的节点优先级机制来实现过滤感知。然而，由于这些方法采用固定且与数据无关的惩罚机制，往往难以在具有不同标签和向量分布的数据集上保持泛化能力。本研究提出一种理论严谨的替代方案：直接从数据中学习向量距离与过滤匹配之间的最优权衡，而非依赖固定惩罚机制。我们将其构建为约束线性优化问题，推导出的权重能更好反映底层过滤分布，从而更有效解决过滤式近似最近邻搜索问题。这些学习得到的权重同时指导搜索过程和索引构建，形成的图结构能更有效地捕捉底层过滤分布与过滤语义。实验表明，相较于固定惩罚方法，这种根据数据自适应调整距离函数的方式将准确率显著提升5-10%，为过滤式近似最近邻搜索问题提供了更灵活且可泛化的解决方案。"
    },
    {
        "title": "KnowThyself: An Agentic Assistant for LLM Interpretability",
        "summary": "We develop KnowThyself, an agentic assistant that advances large language\nmodel (LLM) interpretability. Existing tools provide useful insights but remain\nfragmented and code-intensive. KnowThyself consolidates these capabilities into\na chat-based interface, where users can upload models, pose natural language\nquestions, and obtain interactive visualizations with guided explanations. At\nits core, an orchestrator LLM first reformulates user queries, an agent router\nfurther directs them to specialized modules, and the outputs are finally\ncontextualized into coherent explanations. This design lowers technical\nbarriers and provides an extensible platform for LLM inspection. By embedding\nthe whole process into a conversational workflow, KnowThyself offers a robust\nfoundation for accessible LLM interpretability.",
        "entry_id": "http://arxiv.org/abs/2511.03878v1",
        "pub_date": "2025-11-05",
        "translated_summary": "我们开发了KnowThyself智能助手，这是一个推动大语言模型可解释性研究的智能辅助系统。现有工具虽能提供有用见解，但存在功能碎片化和代码依赖性强的问题。KnowThyself通过基于聊天的交互界面整合这些能力，用户可上传模型、用自然语言提问，并获得带引导说明的交互式可视化结果。其核心架构包含三层处理：协调器大模型首先重构用户查询，智能路由代理进一步将问题分发至专业模块，最终输出结果会被整合成连贯的阐释。这种设计既降低了技术门槛，又构建了可扩展的大语言模型检测平台。通过将全流程嵌入对话式工作流，KnowThyself为普及大语言模型可解释性提供了坚实基础。"
    },
    {
        "title": "CLAX: Fast and Flexible Neural Click Models in JAX",
        "summary": "CLAX is a JAX-based library that implements classic click models using modern\ngradient-based optimization. While neural click models have emerged over the\npast decade, complex click models based on probabilistic graphical models\n(PGMs) have not systematically adopted gradient-based optimization, preventing\npractitioners from leveraging modern deep learning frameworks while preserving\nthe interpretability of classic models. CLAX addresses this gap by replacing\nEM-based optimization with direct gradient-based optimization in a numerically\nstable manner. The framework's modular design enables the integration of any\ncomponent, from embeddings and deep networks to custom modules, into classic\nclick models for end-to-end optimization. We demonstrate CLAX's efficiency by\nrunning experiments on the full Baidu-ULTR dataset comprising over a billion\nuser sessions in $\\approx$ 2 hours on a single GPU, orders of magnitude faster\nthan traditional EM approaches. CLAX implements ten classic click models,\nserving both industry practitioners seeking to understand user behavior and\nimprove ranking performance at scale and researchers developing new click\nmodels. CLAX is available at: https://github.com/philipphager/clax",
        "entry_id": "http://arxiv.org/abs/2511.03620v1",
        "pub_date": "2025-11-05",
        "translated_summary": "CLAX是一个基于JAX的库，它通过现代梯度优化方法实现了经典点击模型。尽管神经点击模型在过去十年中不断涌现，但基于概率图模型的复杂点击模型尚未系统性地采用梯度优化方法，这导致从业者无法在保持经典模型可解释性的同时利用现代深度学习框架。CLAX通过以数值稳定的方式将基于EM的优化替换为直接梯度优化，成功解决了这一局限。该框架采用模块化设计，允许将嵌入层、深度网络乃至自定义模块等任何组件集成到经典点击模型中，实现端到端优化。我们在包含超十亿用户会话的完整Baidu-ULTR数据集上验证了CLAX的效率，单GPU仅需约2小时即可完成实验，比传统EM方法快数个数量级。CLAX实现了十种经典点击模型，既服务于需要理解用户行为并提升大规模排序性能的行业从业者，也助力于开发新型点击模型的研究人员。项目地址：https://github.com/philipphager/clax"
    },
    {
        "title": "A Semantic Encoding of Object Centric Event Data",
        "summary": "The Object-Centric Event Data (OCED) is a novel meta-model aimed at providing\na common ground for process data records centered around events and objects.\nOne of its objectives is to foster interoperability and process information\nexchange. In this context, the integration of data from different providers,\nthe combination of multiple processes, and the enhancement of knowledge\ninference are novel challenges. Semantic Web technologies can enable the\ncreation of a machine-readable OCED description enriched through ontology-based\nrelationships and entity categorization. In this paper, we introduce an\napproach built upon Semantic Web technologies for the realization of\nsemantic-enhanced OCED, with the aim to strengthen process data reasoning,\ninterconnect information sources, and boost expressiveness.",
        "entry_id": "http://arxiv.org/abs/2511.03351v1",
        "pub_date": "2025-11-05",
        "translated_summary": "以对象为中心的事件数据是一种新型元模型，旨在为围绕事件和对象构建的流程数据记录提供统一标准。该模型的目标之一是促进互操作性与流程信息交换。在此背景下，如何整合多源数据、融合多重流程以及增强知识推理能力成为全新挑战。语义网技术能够通过基于本体的关系定义与实体分类，构建机器可读的增强型事件数据描述。本文提出一种基于语义网技术的实现方法，旨在构建语义增强的以对象为中心的事件数据模型，从而强化流程数据推理能力，打通信息源之间的连接通道，提升系统表达能力。"
    },
    {
        "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning",
        "summary": "The rapid growth of open-access (OA) publications has intensified the\nchallenge of identifying relevant scientific papers. Due to privacy constraints\nand limited access to user interaction data, recent efforts have shifted toward\ncontent-based recommendation, which relies solely on textual information.\nHowever, existing models typically treat papers as unstructured text,\nneglecting their discourse organization and thereby limiting semantic\ncompleteness and interpretability. To address these limitations, we propose\nOMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective,\nMethod, Result, Conclusion) summarization, multi-level contrastive learning,\nand structure-aware re-ranking for scholarly recommendation. The QA-style\nsummarization module converts raw papers into structured and\ndiscourse-consistent representations, while multi-level contrastive objectives\nalign semantic representations across metadata, section, and document levels.\nThe final re-ranking stage further refines retrieval precision through\ncontextual similarity calibration. Experiments on DBLP, S2ORC, and the newly\nconstructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses\nstate-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in\nPrecision@10 and Recall@10, respectively. Additional evaluations confirm that\nQA-style summarization produces more coherent and factually complete\nrepresentations. Overall, OMRC-MR provides a unified and interpretable\ncontent-based paradigm for scientific paper recommendation, advancing\ntrustworthy and privacy-aware scholarly information retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.03330v1",
        "pub_date": "2025-11-05",
        "translated_summary": "开放获取（OA）文献的快速增长加剧了科学论文精准筛选的挑战。由于用户交互数据存在隐私限制与获取壁垒，近期研究重点已转向仅依赖文本信息的内容推荐方法。然而现有模型通常将论文视为非结构化文本，忽略了其论述结构，导致语义完整性受限且可解释性不足。为此，我们提出OMRC-MR分层框架，该框架融合了问答式OMRC（目标、方法、结果、结论）摘要生成、多层级对比学习与结构感知重排序技术，用于学术论文推荐。问答式摘要模块将原始论文转化为结构化的论述一致性表征，而多层级对比学习目标则在元数据、章节和文档层面实现语义表征对齐。最终的重排序阶段通过上下文相似度校准进一步提升检索精度。在DBLP、S2ORC及新建的Sci-OMRC数据集上的实验表明，OMRC-MR始终优于现有最优基线模型，在Precision@10和Recall@10指标上分别最高提升7.2%和3.8%。额外评估证实问答式摘要能生成更具连贯性与事实完整性的表征。总体而言，OMRC-MR为科学论文推荐提供了统一且可解释的内容驱动范式，推动了可信赖且保护隐私的学术信息检索发展。"
    },
    {
        "title": "Two thousand years of the oracle problem. Insights from Ancient Delphi on the future of blockchain oracles",
        "summary": "The oracle problem refers to the inability of an agent to know if the\ninformation coming from an oracle is authentic and unbiased. In ancient times,\nphilosophers and historians debated on how to evaluate, increase, and secure\nthe reliability of oracle predictions, particularly those from Delphi, which\npertained to matters of state. Today, we refer to data carriers for automatic\nmachines as oracles, but establishing a secure channel between these oracles\nand the real world still represents a challenge. Despite numerous efforts, this\nproblem remains mostly unsolved, and the recent advent of blockchain oracles\nhas added a layer of complexity because of the decentralization of blockchains.\nThis paper conceptually connects Delphic and modern blockchain oracles,\ndeveloping a comparative framework. Leveraging blockchain oracle taxonomy,\nlexical analysis is also performed on 167 Delphic queries to shed light on the\nrelationship between oracle answer quality and question type. The presented\nframework aims first at revealing commonalities between classical and\ncomputational oracles and then at enriching the oracle analysis within each\nfield. This study contributes to the computer science literature by proposing\nstrategies to improve the reliability of blockchain oracles based on insights\nfrom Delphi and to classical literature by introducing a framework that can\nalso be applied to interpret and classify other ancient oracular mechanisms.",
        "entry_id": "http://arxiv.org/abs/2511.03319v1",
        "pub_date": "2025-11-05",
        "translated_summary": "神谕问题是指行为体无法判断来自神谕的信息是否真实无偏。古代哲学家与历史学家曾就如何评估、提升并确保神谕预测（尤其是涉及国家事务的德尔斐神谕）的可靠性展开辩论。如今，我们将自动机器的数据载体称为预言机，但在这些预言机与现实世界之间建立安全通道仍具挑战。尽管付出诸多努力，该问题仍悬而未决，而近期区块链预言机的出现更因区块链的去中心化特性增添了复杂性。本文从概念层面将德尔斐神谕与现代区块链预言机相联系，构建出比较研究框架。基于区块链预言机分类法，我们对167条德尔斐神谕问询进行词法分析，以揭示神谕应答质量与问题类型之间的关联。该框架旨在首先揭示古典神谕与计算型预言机之间的共性，进而深化各自领域内的神谕分析。本研究通过借鉴德尔斐经验提出提升区块链预言机可靠性的策略，为计算机科学文献作出贡献；同时通过引入可适用于其他古代神谕机制解读与分类的分析框架，为古典文献研究提供了新视角。"
    },
    {
        "title": "KScaNN: Scalable Approximate Nearest Neighbor Search on Kunpeng",
        "summary": "Approximate Nearest Neighbor Search (ANNS) is a cornerstone algorithm for\ninformation retrieval, recommendation systems, and machine learning\napplications. While x86-based architectures have historically dominated this\ndomain, the increasing adoption of ARM-based servers in industry presents a\ncritical need for ANNS solutions optimized on ARM architectures. A naive port\nof existing x86 ANNS algorithms to ARM platforms results in a substantial\nperformance deficit, failing to leverage the unique capabilities of the\nunderlying hardware. To address this challenge, we introduce KScaNN, a novel\nANNS algorithm co-designed for the Kunpeng 920 ARM architecture. KScaNN\nembodies a holistic approach that synergizes sophisticated, data aware\nalgorithmic refinements with carefully-designed hardware specific\noptimizations. Its core contributions include: 1) novel algorithmic techniques,\nincluding a hybrid intra-cluster search strategy and an improved PQ residual\ncalculation method, which optimize the search process at a higher level; 2) an\nML-driven adaptive search module that provides adaptive, per-query tuning of\nsearch parameters, eliminating the inefficiencies of static configurations; and\n3) highly-optimized SIMD kernels for ARM that maximize hardware utilization for\nthe critical distance computation workloads. The experimental results\ndemonstrate that KScaNN not only closes the performance gap but establishes a\nnew standard, achieving up to a 1.63x speedup over the fastest x86-based\nsolution. This work provides a definitive blueprint for achieving\nleadership-class performance for vector search on modern ARM architectures and\nunderscores",
        "entry_id": "http://arxiv.org/abs/2511.03298v1",
        "pub_date": "2025-11-05",
        "translated_summary": "近似最近邻搜索（ANNS）是信息检索、推荐系统和机器学习应用的基础算法。虽然基于x86的架构历来主导该领域，但工业界对ARM服务器日益广泛的应用，亟需针对ARM架构优化的ANNS解决方案。将现有x86平台ANNS算法简单移植到ARM平台会导致显著性能损失，无法充分利用底层硬件的独特能力。为此，我们提出KScaNN——专为鲲鹏920 ARM架构协同设计的新型ANNS算法。该算法采用整体优化思路，将精密的数据感知算法改进与精心设计的硬件专用优化技术相融合，其核心创新包括：1）新型算法技术，包含混合式簇内搜索策略与改进的PQ残差计算方法，从更高维度优化搜索流程；2）基于机器学习的自适应搜索模块，实现按查询动态调整搜索参数，消除静态配置的效率瓶颈；3）针对ARM架构深度优化的SIMD内核，最大限度提升关键距离计算任务的硬件利用率。实验结果表明，KScaNN不仅弥补了性能差距，更树立了新的性能标杆，相较最快的x86解决方案实现最高1.63倍加速比。本研究为在现代ARM架构上实现领先级向量搜索性能提供了完整技术蓝图，同时印证了算法-硬件协同设计在下一代检索系统中的关键价值。"
    },
    {
        "title": "Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval",
        "summary": "Machine Translation for English Retrieval of Information in Any Language\n(MATERIAL) is an IARPA initiative targeted to advance the state of\ncross-lingual information retrieval (CLIR). This report provides a detailed\ndescription of Information Sciences Institute's (ISI's) Summarization and\ndomain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.\nSpecifically, we outline our team's novel approach to handle CLIR with emphasis\nin developing an approach amenable to retrieve a query-relevant document\n\\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3\nevaluations, SARAL exceeded the performance of other teams in five out of six\nevaluation conditions spanning three different languages (Farsi, Kazakh, and\nGeorgian).",
        "entry_id": "http://arxiv.org/abs/2511.03228v1",
        "pub_date": "2025-11-05",
        "translated_summary": "机器翻译英语跨语言信息检索项目（简称MATERIAL）是美国情报高级研究计划署推动跨语言信息检索技术发展的专项计划。本报告详细介绍了信息科学研究所SARAL团队在该项目中的研究成果，重点阐述了团队在跨语言检索方面的创新方法——突破传统排序文档列表模式，构建了一套能够检索查询相关文档集的解决方案。在MATERIAL第三阶段评估中，SARAL团队在涉及波斯语、哈萨克语和格鲁吉亚语三种语言的六项评测任务中，有五项性能表现超越其他参评团队。"
    },
    {
        "title": "Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification",
        "summary": "Large language models (LLMs) excel in generating fluent utterances but can\nlack reliable grounding in verified information. At the same time,\nknowledge-graph-based fact-checkers deliver precise and interpretable evidence,\nyet suffer from limited coverage or latency. By integrating LLMs with knowledge\ngraphs and real-time search agents, we introduce a hybrid fact-checking\napproach that leverages the individual strengths of each component. Our system\ncomprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid\none-hop lookups in DBpedia, 2) an LM-based classification guided by a\ntask-specific labeling prompt, producing outputs with internal rule-based\nlogic, and 3) a Web Search Agent invoked only when KG coverage is insufficient.\nOur pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the\nSupported/Refuted split without task-specific fine-tuning. To address Not\nenough information cases, we conduct a targeted reannotation study showing that\nour approach frequently uncovers valid evidence for claims originally labeled\nas Not Enough Information (NEI), as confirmed by both expert annotators and LLM\nreviewers. With this paper, we present a modular, opensource fact-checking\npipeline with fallback strategies and generalization across datasets.",
        "entry_id": "http://arxiv.org/abs/2511.03217v1",
        "pub_date": "2025-11-05",
        "translated_summary": "大型语言模型（LLM）在生成流畅文本方面表现出色，但可能缺乏对已验证信息的可靠依据。与此同时，基于知识图谱的事实核查系统虽能提供精确且可解释的证据，却存在覆盖范围有限或响应延迟的问题。通过将LLM与知识图谱及实时搜索智能体相结合，我们提出了一种混合式事实核查方法，充分发挥各组件的独特优势。该系统包含三个自动化步骤：1）知识图谱检索模块，用于在DBpedia中快速执行单跳查询；2）基于语言模型的分类器，通过任务特定的标注提示生成具有内部规则逻辑的输出；3）当知识图谱覆盖不足时触发的网络搜索代理。在FEVER基准测试的“支持/反驳”数据子集上，我们的流水线在不进行任务特定微调的情况下取得了0.93的F1分数。针对“信息不足”案例，我们开展了专项重标注研究，结果表明该方法能频繁为原标记为“信息不足”的声明找到有效证据，这一结论同时获得了专业标注者和LLM评审的确认。本文提出的模块化开源事实核查流水线具备故障应对策略，并展现出跨数据集的泛化能力。"
    },
    {
        "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework",
        "summary": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
        "entry_id": "http://arxiv.org/abs/2511.05385v1",
        "pub_date": "2025-11-07",
        "translated_summary": "检索增强生成（RAG）通过引入外部知识来提升大语言模型的可靠性。为实现灵活检索，智能体化RAG采用自主多轮检索与推理机制处理查询。尽管当前基于强化学习的智能体化RAG性能有所提升，但其搜索与推理过程常伴随显著的令牌消耗，形成以效率换取准确性的权衡。为此，本文提出TeaRAG——一个能同时压缩检索内容与推理步骤的高效令牌智能体化RAG框架：1）在检索内容压缩方面，通过在图检索中引入简洁三元组增强基于语块的语义检索，构建融合语义关联与共现关系的知识图谱，进而利用个性化PageRank算法聚焦核心知识，降低单次检索的令牌数；2）在推理步骤优化方面，提出迭代过程感知直接偏好优化（IP-DPO），其奖励函数通过知识匹配机制评估知识完备性，同时对冗余推理步骤施加惩罚。该设计可生成高质量偏好对数据集，支撑迭代式DPO训练以提升推理简洁性。在六个数据集上的实验表明，在Llama3-8B-Instruct和Qwen2.5-14B-Instruct模型上，TeaRAG将精确匹配率平均提升4%与2%，同时分别减少61%和59%的输出令牌数。代码已开源：https://github.com/Applied-Machine-Learning-Lab/TeaRAG。"
    },
    {
        "title": "QUESTER: Query Specification for Generative Retrieval",
        "summary": "Generative Retrieval (GR) differs from the traditional index-then-retrieve\npipeline by storing relevance in model parameters and directly generating\ndocument identifiers. However, GR often struggles to generalize and is costly\nto scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),\nwhich reframes GR as query specification generation - in this work, a simple\nkeyword query handled by BM25 - using a (small) LLM. The policy is trained\nusing reinforcement learning techniques (GRPO). Across in- and out-of-domain\nevaluations, we show that our model is more effective than BM25, and\ncompetitive with neural IR models, while maintaining a good efficiency",
        "entry_id": "http://arxiv.org/abs/2511.05301v1",
        "pub_date": "2025-11-07",
        "translated_summary": "生成式检索与传统“先索引后检索”流程不同，其将相关性信息存储于模型参数中，并直接生成文档标识符。然而该方法常面临泛化能力不足与扩展成本高昂的问题。我们提出QUESTER框架（查询规约生成式检索），通过（轻量级）大语言模型将生成式检索重构为查询规约生成任务——本研究中体现为可由BM25处理的简易关键词查询。该策略采用强化学习技术（GRPO）进行训练。在领域内外多项评估中，我们的模型不仅效果优于BM25，更能与神经信息检索模型保持竞争力，同时维持良好的检索效率。"
    },
    {
        "title": "Mapping Research Productivity of BRICS Countries with Special Reference to Coronary Artery Disease (CAD): A Scientometric Study",
        "summary": "This study presents a comprehensive scientometric analysis of research\nproductivity on Coronary Artery Disease (CAD) among the BRICS countries,\nBrazil, Russia, India, China, and South Africa, using data retrieved from the\nWeb of Science database for the period 1990 to 2019. A total of 50,036 records\nwere analyzed to assess publication growth trends, authorship patterns,\ncollaboration levels, and citation impact. The findings reveal a steady\nincrease in CAD-related publications, with China emerging as the leading\ncontributor, followed by Brazil, Russia, India, and South Africa. English\ndominated as the primary language of communication, accounting for over 93% of\npublications. Authorship and collaboration analysis indicate a high degree of\njoint research, with 97.91% of studies being co-authored and a degree of\ncollaboration of 0.98, underscoring the collective nature of scientific inquiry\nin this domain. The study validates the applicability of Lotkas Law for author\nproductivity, Bradfords Law for journal distribution, and Zipfs Law for keyword\nfrequency, while the Price Square Root Law was found inapplicable. The\npredominant publication format was journal articles (79.7%), and Kardiologiya\n(Russia) emerged as the most prolific journal. The results demonstrate\nsignificant growth in CAD research output and collaboration within BRICS,\nthough notable disparities persist among member nations. The study recommends\nenhancing individual author productivity, expanding international\ncollaboration, and supporting CAD research through strategic institutional and\ngovernmental initiatives. These findings provide valuable insights for\npolicymakers, funding agencies, and the academic community to strengthen\ncardiovascular research capacity within developing economies.",
        "entry_id": "http://arxiv.org/abs/2511.05211v1",
        "pub_date": "2025-11-07",
        "translated_summary": "本研究通过科学计量学方法，对1990-2019年间金砖国家（巴西、俄罗斯、印度、中国、南非）在冠状动脉疾病（CAD）领域的研究产出进行全面分析。基于Web of Science数据库提取的50,036条文献记录，系统评估了论文增长趋势、作者模式、合作水平及引用影响力。研究发现：CAD相关出版物呈稳定增长态势，中国位居发文量首位，其后依次为巴西、俄罗斯、印度和南非；英文为主要交流语言（占比93%以上）；作者与合作分析显示该领域具有高度协同性——合著率达97.91%，合作强度指数达0.98，凸显科研合作的集体性特征。研究验证了洛特卡定律（作者产出分布）、布拉德福定律（期刊分布）与齐普夫定律（关键词频次）的适用性，但普赖斯平方根定律在此不适用。期刊论文是主要成果形式（79.7%），其中俄罗斯《Kardiologiya》为最高产期刊。结果表明金砖国家CAD研究产出与合作显著增长，但成员国间仍存在明显差异。建议通过机构与政府的战略性支持，提升个体作者产出、拓展国际合作网络。本研究为政策制定者、资助机构及学术界加强发展中国家心血管研究能力建设提供了重要参考依据。"
    },
    {
        "title": "Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR",
        "summary": "In this paper, we present a novel series of Russian information retrieval\ndatasets constructed from the \"Did you know...\" section of Russian Wikipedia.\nOur datasets support a range of retrieval tasks, including fact-checking,\nretrieval-augmented generation, and full-document retrieval, by leveraging\ninteresting facts and their referenced Wikipedia articles annotated at the\nsentence level with graded relevance. We describe the methodology for dataset\ncreation that enables the expansion of existing Russian Information Retrieval\n(IR) resources. Through extensive experiments, we extend the RusBEIR research\nby comparing lexical retrieval models, such as BM25, with state-of-the-art\nneural architectures fine-tuned for Russian, as well as multilingual models.\nResults of our experiments show that lexical methods tend to outperform neural\nmodels on full-document retrieval, while neural approaches better capture\nlexical semantics in shorter texts, such as in fact-checking or fine-grained\nretrieval. Using our newly created datasets, we also analyze the impact of\ndocument length on retrieval performance and demonstrate that combining\nretrieval with neural reranking consistently improves results. Our contribution\nexpands the resources available for Russian information retrieval research and\nhighlights the importance of accurate evaluation of retrieval models to achieve\noptimal performance. All datasets are publicly available at HuggingFace. To\nfacilitate reproducibility and future research, we also release the full\nimplementation on GitHub.",
        "entry_id": "http://arxiv.org/abs/2511.05079v1",
        "pub_date": "2025-11-07",
        "translated_summary": "本文基于俄语维基百科\"你知道吗…\"栏目构建了一套新颖的俄语信息检索数据集。该数据集通过利用趣味性事实及其引用的维基百科文章（附带句子级分级相关性标注），支持事实核查、检索增强生成和全文档检索等多重任务。我们详细阐述了扩展现有俄语信息检索资源的数据集构建方法，并通过系列实验拓展了RusBEIR研究框架，系统比较了BM25等词汇检索模型与针对俄语优化的前沿神经架构及多语言模型。实验结果表明：在全文档检索任务中，词汇检索方法普遍优于神经模型，而在事实核查或细粒度检索等短文本场景中，神经方法更能有效捕捉词汇语义。基于新建数据集，我们进一步分析了文档长度对检索性能的影响，论证了神经重排序与检索结合的持续增效作用。本研究成果拓展了俄语信息检索的研究资源，强调精准评估检索模型对实现最优性能的重要性。所有数据集已发布于HuggingFace平台，并为确保可复现性及后续研究，我们在GitHub同步开放完整实现代码。"
    },
    {
        "title": "The use of social media among library professionals and patrons: A review of literature",
        "summary": "This paper focused on the utilization of social media by library\nprofessionals and library users. It provides an understanding of social media,\nthe most popular social media platforms utilized in the libraries. It also\nmentions the reasons for the adoption of social media in libraries be it\nacademic, public, school libraries and other types of libraries. This is a\nreview paper on the use of social media among library professionals and\npatrons. The findings reveal the contributions of social media to the\nlibraries. Social media makes things easy for library professionals and library\nusers. It enables them to connect, create awareness to new information,\ndisseminate information instantly, and helps to market the library resources\nand services. Therefore, it is recommended amongst others that the library\nmanagement board should encourage the use of social media in libraries.",
        "entry_id": "http://arxiv.org/abs/2511.05051v1",
        "pub_date": "2025-11-07",
        "translated_summary": "本文聚焦图书馆从业人员与用户对社交媒体的应用实践，系统阐释了社交媒体的概念内涵、图书馆界使用最普遍的社交平台类型，并剖析了高校图书馆、公共图书馆、中小学图书馆及各类专业图书馆采纳社交媒体技术的动因。作为针对图书馆从业者与使用者社交媒体应用的综述性研究，本文发现社交媒体为图书馆事业带来多重赋能：它显著提升馆员工作效率与用户服务体验，构建馆群与用户间的即时连接渠道，助推最新资讯的传播触达，实现馆藏资源与服务的精准推广。基于研究结论，建议图书馆管理委员会将社交媒体应用纳入发展战略，积极推动各类图书馆部署社会化媒体工具。"
    },
    {
        "title": "Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval",
        "summary": "As financial applications of large language models (LLMs) gain attention,\naccurate Information Retrieval (IR) remains crucial for reliable AI services.\nHowever, existing benchmarks fail to capture the complex and domain-specific\ninformation needs of real-world banking scenarios. Building domain-specific IR\nbenchmarks is costly and constrained by legal restrictions on using real\ncustomer data. To address these challenges, we propose a systematic methodology\nfor constructing domain-specific IR benchmarks through LLM-based query\ngeneration. As a concrete implementation of this methodology, our pipeline\ncombines single and multi-document query generation with an enhanced and\nreasoning-augmented answerability assessment method, achieving stronger\nalignment with human judgments than prior approaches. Using this methodology,\nwe construct KoBankIR, comprising 815 queries derived from 204 official banking\ndocuments. Our experiments show that existing retrieval models struggle with\nthe complex multi-document queries in KoBankIR, demonstrating the value of our\nsystematic approach for domain-specific benchmark construction and underscoring\nthe need for improved retrieval techniques in financial domains.",
        "entry_id": "http://arxiv.org/abs/2511.05000v1",
        "pub_date": "2025-11-07",
        "translated_summary": "随着大语言模型在金融领域的应用日益受到关注，精准的信息检索技术仍是确保人工智能服务可靠性的关键。然而，现有基准测试难以全面反映真实银行场景中复杂且具有领域特性的信息需求。构建领域专用信息检索基准不仅成本高昂，还受到使用真实客户数据的法律限制。为应对这些挑战，我们提出一种基于大语言模型的系统性领域检索基准构建方法。作为该方法的具体实践，我们的流程将单文档与多文档查询生成相结合，并引入增强型推理辅助可答性评估机制，相比现有方法更能契合人类判断标准。基于该方法构建的KoBankIR基准库包含从204份官方银行文件衍生的815条查询指令。实验表明，现有检索模型在应对KoBankIR中复杂的多文档查询时表现不佳，这既印证了我们系统性构建方法的有效性，也揭示了金融领域检索技术亟待提升的现实需求。"
    },
    {
        "title": "Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG",
        "summary": "Retrieval systems are essential to contemporary AI pipelines, although most\nconfuse two separate processes: finding relevant information and giving enough\ncontext for reasoning. We introduce the Search-Is-Not-Retrieve (SINR)\nframework, a dual-layer architecture that distinguishes between fine-grained\nsearch representations and coarse-grained retrieval contexts. SINR enhances the\ncomposability, scalability, and context fidelity of retrieval systems by\ndirectly connecting small, semantically accurate search chunks to larger,\ncontextually complete retrieve chunks, all without incurring extra processing\ncosts. This design changes retrieval from a passive step to an active one,\nmaking the system architecture more like how people process information. We\ndiscuss the SINR framework's conceptual foundation, formal structure,\nimplementation issues, and qualitative outcomes. This provides a practical\nfoundation for the next generation of AI systems that use retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.04939v1",
        "pub_date": "2025-11-07",
        "translated_summary": "检索系统是现代人工智能流程的核心组件，但多数系统混淆了两个独立过程：寻找相关信息与提供充分推理语境。我们提出\"搜索非检索\"（SINR）框架，该双层级架构明确区分细粒度搜索表征与粗粒度检索语境。通过将语义精准的小型搜索块与语境完整的宏观检索块直接关联，SINR在无需额外处理成本的前提下，显著提升了检索系统的可组合性、扩展性与语境保真度。这一设计使检索从被动步骤转变为主动过程，令系统架构更贴近人类信息处理模式。我们将深入探讨SINR框架的理论基础、形式化结构、实施要点与质性成效，为下一代基于检索的人工智能系统奠定实践基础。"
    },
    {
        "title": "Association via Entropy Reduction",
        "summary": "Prior to recent successes using neural networks, term frequency-inverse\ndocument frequency (tf-idf) was clearly regarded as the best choice for\nidentifying documents related to a query. We provide a different score, aver,\nand observe, on a dataset with ground truth marking for association, that aver\ndoes do better at finding assciated pairs than tf-idf. This example involves\nfinding associated vertices in a large graph and that may be an area where\nneural networks are not currently an obvious best choice. Beyond this one\nanecdote, we observe that (1) aver has a natural threshold for declaring pairs\nas unassociated while tf-idf does not, (2) aver can distinguish between pairs\nof documents for which tf-idf gives a score of 1.0, (3) aver can be applied to\nlarger collections of documents than pairs while tf-idf cannot, and (4) that\naver is derived from entropy under a simple statistical model while tf-idf is a\nconstruction designed to achieve a certain goal and hence aver may be more\n\"natural.\" To be fair, we also observe that (1) writing down and computing the\naver score for a pair is more complex than for tf-idf and (2) that the fact\nthat the aver score is naturally scale-free makes it more complicated to\ninterpret aver scores.",
        "entry_id": "http://arxiv.org/abs/2511.04901v1",
        "pub_date": "2025-11-07",
        "translated_summary": "在神经网络取得近期成功之前，词频-逆文档频率（tf-idf）被公认为检索关联文档的最佳方法。我们提出了一种新型评分指标aver，并在具有真实关联标注的数据集上验证了其在发现关联文档对方面确实优于tf-idf。本案例涉及大型图中的关联顶点发现，这或许是神经网络目前尚未显现明显优势的领域。除该实例外，我们还发现：（1）aver具有判定非关联对的天然阈值而tf-idf不具备；（2）对于tf-idf评分均为1.0的文档对，aver能有效区分其关联强度；（3）aver可扩展应用于多文档集而tf-idf仅适用于文档对；（4）aver基于简单统计模型中的熵推导得出，而tf-idf是为实现特定目标构建的指标，因此aver可能更具“自然性”。公允而言，我们也注意到：（1）aver的计算公式与过程较tf-idf更为复杂；（2）aver天然具备无量纲特性，这使其得分解读更具挑战性。"
    },
    {
        "title": "EMO100DB: An Open Dataset of Improvised Songs with Emotion Data",
        "summary": "In this study, we introduce Emo100DB: a dataset consisting of improvised\nsongs that were recorded and transcribed with emotion data based on Russell's\ncircumplex model of emotion. The dataset was developed by collecting improvised\nsongs that consist of melody, lyrics, and an instrumental accompaniment played,\nsung, and recorded by 20 young adults. Before recording each song, the\nparticipants were asked to report their emotional state, with the axes\nrepresenting arousal and valence based on Russell's circumplex model of\nemotions. The dataset is organized into four emotion quadrants, and it includes\nthe lyrics text and MIDI file of the melody extracted from the participant\nrecordings, along with the original audio in WAV format. By providing an\nintegrated composition of data and analysis, this study aims to offer a\ncomprehensive dataset that allows for a diverse exploration of the relationship\nbetween music and emotion.",
        "entry_id": "http://arxiv.org/abs/2511.04755v1",
        "pub_date": "2025-11-06",
        "translated_summary": "本研究推出Emo100DB数据集——一个收录即兴演唱歌曲的数据库，所有曲目均基于罗素情感环状模型进行录音、文本转写及情感标注。该数据集通过采集20位青年创作者即兴创作的歌曲构建而成，每首作品包含由参与者演奏录制的旋律、人声歌词与器乐伴奏。在录制前，参与者需根据罗素情感环状模型的双轴维度（唤醒度与效价）自评当前情感状态。数据集按情感象限分为四类，除原始WAV格式音频外，还提供从录音中提取的歌词文本与旋律MIDI文件。通过整合数据资源与分析维度，本研究旨在构建一个能够支持多角度探索音乐与情感关联的综合性数据集。"
    },
    {
        "title": "On the Brittleness of CLIP Text Encoders",
        "summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy.",
        "entry_id": "http://arxiv.org/abs/2511.04247v2",
        "pub_date": "2025-11-06",
        "translated_summary": "近年来，多模态协同嵌入模型（特别是CLIP）通过将图像与文本对齐至共享表征空间，在零样本分类和多媒体信息检索领域实现了重大突破。然而，基于对比对齐训练的此类模型对输入微小扰动缺乏稳定性。尤其在处理人工表述的查询时，查询语句的细微变化可能导致最佳匹配结果的排序产生显著差异。本文系统分析了多媒体信息检索场景中多类非语义查询扰动的影响，基于TRECVID Ad-Hoc视频搜索查询和V3C1视频数据集，对多种CLIP变体进行了词汇、句法和语义层面的扰动测试。研究发现：所有模型均对句法与语义扰动最为敏感，而鲁棒性薄弱环节集中在标点符号、大小写等表层文本编辑。这一结果表明，在基准准确率之外，鲁棒性应成为评估视觉语言模型的关键维度。"
    },
    {
        "title": "Collaborative residual learners for automatic icd10 prediction using prescribed medications",
        "summary": "Clinical coding is an administrative process that involves the translation of diagnostic data from episodes of care into a standard code format such as ICD10. It has many critical applications such as billing and aetiology research. The automation of clinical coding is very challenging due to data sparsity, low interoperability of digital health systems, complexity of real-life diagnosis coupled with the huge size of ICD10 code space. Related work suffer from low applicability due to reliance on many data sources, inefficient modelling and less generalizable solutions. We propose a novel collaborative residual learning based model to automatically predict ICD10 codes employing only prescriptions data. Extensive experiments were performed on two real-world clinical datasets (outpatient & inpatient) from Maharaj Nakorn Chiang Mai Hospital with real case-mix distributions. We obtain multi-label classification accuracy of 0.71 and 0.57 of average precision, 0.57 and 0.38 of F1-score and 0.73 and 0.44 of accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.",
        "entry_id": "http://arxiv.org/abs/2012.11327v1",
        "pub_date": "2020-12-16",
        "translated_summary": "临床编码是将诊疗过程中的诊断数据转换为ICD10等标准编码格式的行政流程，在医疗计费和病因学研究等领域具有重要应用。由于数据稀疏性、数字健康系统互操作性低、真实诊断复杂性及ICD10编码体系庞大，临床编码自动化面临巨大挑战。现有研究因依赖多数据源、建模效率低及解决方案普适性差，存在适用性不足的问题。本文提出一种基于协同残差学习的新型模型，仅通过处方数据即可实现ICD10编码的自动预测。我们在清迈大学马哈拉吉医院包含真实病例组合的门诊与住院数据集上开展了大量实验，结果显示：针对住院和门诊数据集，多标签分类的平均精确度分别达到0.71和0.57，F1分数分别为0.57和0.38，主要诊断预测准确率分别达到0.73和0.44。"
    },
    {
        "title": "Ensemble model for pre-discharge icd10 coding prediction",
        "summary": "The translation of medical diagnosis to clinical coding has wide range of applications in billing, aetiology analysis, and auditing. Currently, coding is a manual effort while the automation of such task is not straight forward. Among the challenges are the messy and noisy clinical records, case complexities, along with the huge ICD10 code space. Previous work mainly relied on discharge notes for prediction and was applied to a very limited data scale. We propose an ensemble model incorporating multiple clinical data sources for accurate code predictions. We further propose an assessment mechanism to provide confidence rates in predicted outcomes. Extensive experiments were performed on two new real-world clinical datasets (inpatient & outpatient) with unaltered case-mix distributions from Maharaj Nakorn Chiang Mai Hospital. We obtain multi-label classification accuracies of 0.73 and 0.58 for average precision, 0.56 and 0.35 for F1-scores and 0.71 and 0.4 accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.",
        "entry_id": "http://arxiv.org/abs/2012.11333v1",
        "pub_date": "2020-12-16",
        "translated_summary": "将医疗诊断转化为临床编码在医疗账单管理、病因分析和审计等领域具有广泛应用。当前编码工作主要依赖人工完成，而实现该任务的自动化面临诸多挑战：临床记录杂乱且存在噪声、病例复杂度高、ICD10编码体系庞大。既往研究主要依赖出院小结进行预测，且仅在极有限的数据规模上实施。我们提出一种融合多源临床数据的集成模型，以实现精准的编码预测，并建立评估机制为预测结果提供置信度评级。基于玛哈叻清迈医院未经筛选的真实临床数据集（住院与门诊），我们开展了大规模实验验证。实验结果显示：在住院与门诊数据集上，多标签分类的平均精确度分别达到0.73和0.58，F1分数分别为0.56和0.35，主要诊断预测准确率分别达到0.71和0.4。"
    },
    {
        "title": "Should I visit this place? Inclusion and Exclusion Phrase Mining from Reviews",
        "summary": "Although several automatic itinerary generation services have made travel planning easy, often times travellers find themselves in unique situations where they cannot make the best out of their trip. Visitors differ in terms of many factors such as suffering from a disability, being of a particular dietary preference, travelling with a toddler, etc. While most tourist spots are universal, others may not be inclusive for all. In this paper, we focus on the problem of mining inclusion and exclusion phrases associated with 11 such factors, from reviews related to a tourist spot. While existing work on tourism data mining mainly focuses on structured extraction of trip related information, personalized sentiment analysis, and automatic itinerary generation, to the best of our knowledge this is the first work on inclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000 reviews related to 1000 tourist spots, our broad level classifier provides a binary overlap F1 of $\\sim$80 and $\\sim$82 to classify a phrase as inclusion or exclusion respectively. Further, our inclusion/exclusion classifier provides an F1 of $\\sim$98 and $\\sim$97 for 11-class inclusion and exclusion classification respectively. We believe that our work can significantly improve the quality of an automatic itinerary generation service.",
        "entry_id": "http://arxiv.org/abs/2012.10226v1",
        "pub_date": "2020-12-18",
        "translated_summary": "尽管多项自动化行程生成服务已使旅行规划变得便捷，但游客在特殊情境下仍难以充分享受旅程。游客个体差异显著——或身患残疾、或有特殊饮食偏好、或需携带幼童同行等。虽然多数旅游景点具有普适性，但部分场所却无法满足所有人群需求。本文重点研究从旅游景点相关评论中挖掘涉及11类特殊因素的包容性与排斥性短语。现有旅游数据挖掘研究主要集中于行程信息的结构化提取、个性化情感分析及自动化行程生成，而本研究首次针对旅游评论中的包容/排斥短语进行挖掘。基于涵盖1000个旅游景点的2000条评论数据集，我们的广义分类器在判断短语属于包容类或排斥类时，分别获得约80和约82的二元重叠F1值。进一步地，针对11类细分场景的包容与排斥分类，我们的专用分类器分别取得了约98和约97的F1值。我们相信这项研究将显著提升自动化行程生成服务的质量。"
    },
    {
        "title": "Intelligent Vector-based Customer Segmentation in the Banking Industry",
        "summary": "Customer Segmentation is the process of dividing customers into groups based on common characteristics. An intelligent Customer Segmentation will not only enable an organization to effectively allocate marketing resources (e.g., Recommender Systems in the Banking sector) but also it will enable identifying the customer cohorts that are most likely to benefit from a specific policy (e.g., to discover diverse patient groups in the Health sector). While there has been a significant improvement in approaches to Customer Segmentation, the main challenge remains to be the understanding of the reasons behind the segmentation need. This task is challenging as it is subjective and depends on the goal of segmentation as well as the analyst's perspective. To address this challenge, in this paper, we present an intelligent vector-based customer segmentation approach. The proposed approach will leverage feature engineering to enable analysts to identify important features (from a pool of features such as demographics, geography, psychographics, behavioral, and more) and feed them into a neural embedding framework named Customer2Vec. The Customer2Vec combines the neural network classification and clustering methods as supervised and unsupervised learning techniques to embed the customer vector. We adopt a typical scenario in the Banking Sector to highlight how Customer2Vec significantly improves the quality of the segmentation and detecting customer similarities.",
        "entry_id": "http://arxiv.org/abs/2012.11876v1",
        "pub_date": "2020-12-22",
        "translated_summary": "客户细分是根据共同特征将客户划分为不同群体的过程。智能化的客户细分不仅能够帮助机构有效配置营销资源（例如银行业的推荐系统），还能识别最可能从特定政策中受益的客户群体（例如医疗领域中发现不同类型的患者群体）。尽管客户细分方法已取得显著进展，但核心挑战仍在于理解细分需求背后的动因。由于该任务具有主观性，且取决于细分目标和分析师视角，因此极具挑战性。为解决这一难题，本文提出了一种基于向量的智能客户细分方法。该方法通过特征工程，使分析师能够从人口统计、地理、心理特征、行为特征等特征池中识别重要特征，并将其输入名为Customer2Vec的神经嵌入框架。该框架结合神经网络分类（监督学习）与聚类（无监督学习）技术，构建客户向量嵌入模型。我们通过银行业典型场景验证了Customer2Vec能显著提升细分质量与客户相似度检测效果。"
    },
    {
        "title": "Dynamic-K Recommendation with Personalized Decision Boundary",
        "summary": "In this paper, we investigate the recommendation task in the most common scenario with implicit feedback (e.g., clicks, purchases). State-of-the-art methods in this direction usually cast the problem as to learn a personalized ranking on a set of items (e.g., webpages, products). The top-N results are then provided to users as recommendations, where the N is usually a fixed number pre-defined by the system according to some heuristic criteria (e.g., page size, screen size). There is one major assumption underlying this fixed-number recommendation scheme, i.e., there are always sufficient relevant items to users' preferences. Unfortunately, this assumption may not always hold in real-world scenarios. In some applications, there might be very limited candidate items to recommend, and some users may have very high relevance requirement in recommendation. In this way, even the top-1 ranked item may not be relevant to a user's preference. Therefore, we argue that it is critical to provide a dynamic-K recommendation, where the K should be different with respect to the candidate item set and the target user. We formulate this dynamic-K recommendation task as a joint learning problem with both ranking and classification objectives. The ranking objective is the same as existing methods, i.e., to create a ranking list of items according to users' interests. The classification objective is unique in this work, which aims to learn a personalized decision boundary to differentiate the relevant items from irrelevant items. Based on these ideas, we extend two state-of-the-art ranking-based recommendation methods, i.e., BPRMF and HRM, to the corresponding dynamic-K versions, namely DK-BPRMF and DK-HRM. Our experimental results on two datasets show that the dynamic-K models are more effective than the original fixed-N recommendation methods.",
        "entry_id": "http://arxiv.org/abs/2012.13569v1",
        "pub_date": "2020-12-25",
        "translated_summary": "本文研究基于隐式反馈（如点击、购买行为）的推荐任务。该领域的先进方法通常将问题转化为对项目集合（如网页、商品）进行个性化排序学习，并将前N个结果作为推荐内容提供给用户。其中N值通常是根据启发式标准（如页面尺寸、屏幕大小）预设的固定数值。这种固定数量推荐方案存在一个关键前提假设：系统总能找到足够多的符合用户偏好的相关项目。然而在实际场景中，该假设未必始终成立。某些应用场景中可推荐候选项目非常有限，部分用户对推荐内容的相关性要求极高，此时即使排名首位的项目也可能与用户偏好不匹配。因此我们提出动态K值推荐机制，其核心在于根据候选项目集和目标用户特性动态调整K值。我们将该任务形式化为包含排序与分类目标的联合学习问题：排序目标与现有方法一致，即根据用户兴趣生成项目排序列表；分类目标则是本研究的创新点，旨在通过学习个性化决策边界来区分相关与无关项目。基于此思路，我们拓展了两种先进排序推荐方法（BPRMF与HRM），提出对应的动态K版本DK-BPRMF和DK-HRM。在两个数据集上的实验结果表明，动态K模型较原始固定N值推荐方法具有显著优势。"
    },
    {
        "title": "Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision Approach",
        "summary": "The proliferation of massive open online courses (MOOCs) demands an effective way of course recommendation for jobs posted in recruitment websites, especially for the people who take MOOCs to find new jobs. Despite the advances of supervised ranking models, the lack of enough supervised signals prevents us from directly learning a supervised ranking model. This paper proposes a general automated weak supervision framework AutoWeakS via reinforcement learning to solve the problem. On the one hand, the framework enables training multiple supervised ranking models upon the pseudo labels produced by multiple unsupervised ranking models. On the other hand, the framework enables automatically searching the optimal combination of these supervised and unsupervised models. Systematically, we evaluate the proposed model on several datasets of jobs from different recruitment websites and courses from a MOOCs platform. Experiments show that our model significantly outperforms the classical unsupervised, supervised and weak supervision baselines.",
        "entry_id": "http://arxiv.org/abs/2012.14234v1",
        "pub_date": "2020-12-28",
        "translated_summary": "大规模开放在线课程(MOOC)的激增，要求招聘网站能为发布的职位提供有效的课程推荐服务，尤其对希望通过慕课求职的人群具有重要意义。尽管现有排序模型已取得长足发展，但监督信号的严重缺失制约了有监督排序模型的直接应用。为此，本文提出基于强化学习的通用自动化弱监督框架AutoWeakS：一方面通过无监督排序模型生成伪标签，进而训练多个有监督排序模型；另一方面通过强化学习自动搜索最优模型组合。我们系统化地在多个招聘网站职位数据集和慕课平台课程数据集上进行评估，实验表明该模型显著优于经典的无监督、有监督及弱监督基线方法。"
    },
    {
        "title": "Measuring University Impact: Wikipedia approach",
        "summary": "The impact of Universities on the social, economic and political landscape is one of the key directions in contemporary educational evaluation. In this paper, we discuss the new methodological technique that evaluates the impact of university based on popularity (number of page-views) of their alumni's pages on Wikipedia. It allows revealing the alumni popularity dynamics and tracking its state. Preliminary analysis shows that the number of page-views is higher for the contemporary persons that prove the perspectives of this approach. Then, universities were ranked based on the methodology and compared to the famous international university rankings ARWU and QS based only on alumni scales: for the top 10 universities, there is an intersection of two universities (Columbia University, Stanford University). The correlation coefficients between different university rankings are provided in the paper. Finally, the ranking based on the alumni popularity was compared with the ranking of universities based on the popularity of their webpages on Wikipedia: there is a strong connection between these indicators.",
        "entry_id": "http://arxiv.org/abs/2012.13980v1",
        "pub_date": "2020-12-27",
        "translated_summary": "高校对社会、经济及政治格局的影响是当代教育评估的核心方向之一。本文探讨了一种基于维基百科校友页面浏览量数据的高校影响力评估新方法。该方法能揭示校友知名度动态变化并追踪其状态。初步分析表明，当代人物的页面浏览量更高，这印证了该方法的应用前景。我们根据该方法对高校进行排名，并与仅基于校友规模的国际知名大学排名（ARWU和QS）进行比较：在前十名高校中，有两个大学重合（哥伦比亚大学、斯坦福大学）。文中提供了不同大学排名间的相关系数。最后，将基于校友知名度的排名与基于高校维基百科页面浏览量的排名进行对比，发现这两项指标之间存在显著关联。"
    },
    {
        "title": "Neural document expansion for ad-hoc information retrieval",
        "summary": "Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",
        "entry_id": "http://arxiv.org/abs/2012.14005v1",
        "pub_date": "2020-12-27",
        "translated_summary": "最近，Nogueira等人[2019]提出了一种基于神经序列到序列模型的新文档扩展方法，在短文本检索任务上展现出显著提升。然而，该方法需要大量领域内训练数据。本文证明，这种神经文档扩展方法能有效适配标准信息检索任务——这类任务通常面临标注稀缺且存在大量长文档的挑战。"
    },
    {
        "title": "Query Expansion for Cross-Language Question Re-Ranking",
        "summary": "Community question-answering (CQA) platforms have become very popular forums for asking and answering questions daily. While these forums are rich repositories of community knowledge, they present challenges for finding relevant answers and similar questions, due to the open-ended nature of informal discussions. Further, if the platform allows questions and answers in multiple languages, we are faced with the additional challenge of matching cross-lingual information. In this work, we focus on the cross-language question re-ranking shared task, which aims to find existing questions that may be written in different languages. Our contribution is an exploration of query expansion techniques for this problem. We investigate expansions based on Word Embeddings, DBpedia concepts linking, and Hypernym, and show that they outperform existing state-of-the-art methods.",
        "entry_id": "http://arxiv.org/abs/1904.07982v1",
        "pub_date": "2019-04-16",
        "translated_summary": "社区问答平台已成为日常提问与回答的热门论坛。尽管这些论坛是社区知识的丰富宝库，但由于开放式非正式讨论的特性，在寻找相关答案和类似问题时仍面临挑战。此外，若平台允许多语言提问与回答，我们还需应对跨语言信息匹配这一额外难题。本研究聚焦于跨语言问题重排序共享任务，旨在发现可能以不同语言表述的现存问题。我们的贡献在于针对该问题探索查询扩展技术，研究了基于词嵌入、DBpedia概念链接和上位词关系的扩展方法，并证明这些方法优于现有前沿技术。"
    },
    {
        "title": "How to define co-occurrence in different domains of study?",
        "summary": "This position paper presents a comparative study of co-occurrences. Some similarities and differences in the definition exist depending on the research domain (e.g. linguistics, NLP, computer science). This paper discusses these points, and deals with the methodological aspects in order to identify co-occurrences in a multidisciplinary paradigm.",
        "entry_id": "http://arxiv.org/abs/1904.08010v1",
        "pub_date": "2019-04-16",
        "translated_summary": "本立场文件对共现关系展开了一项比较研究。根据研究领域（如语言学、自然语言处理、计算机科学）的不同，其定义存在若干异同之处。本文通过多学科范式探讨这些要点，并论述识别共现关系的方法论层面。"
    },
    {
        "title": "Neural Message Passing for Multi-Label Classification",
        "summary": "Multi-label classification (MLC) is the task of assigning a set of target labels for a given sample. Modeling the combinatorial label interactions in MLC has been a long-haul challenge. We propose Label Message Passing (LaMP) Neural Networks to efficiently model the joint prediction of multiple labels. LaMP treats labels as nodes on a label-interaction graph and computes the hidden representation of each label node conditioned on the input using attention-based neural message passing. Attention enables LaMP to assign different importance to neighbor nodes per label, learning how labels interact (implicitly). The proposed models are simple, accurate, interpretable, structure-agnostic, and applicable for predicting dense labels since LaMP is incredibly parallelizable. We validate the benefits of LaMP on seven real-world MLC datasets, covering a broad spectrum of input/output types and outperforming the state-of-the-art results. Notably, LaMP enables intuitive interpretation of how classifying each label depends on the elements of a sample and at the same time rely on its interaction with other labels. We provide our code and datasets at https://github.com/QData/LaMP",
        "entry_id": "http://arxiv.org/abs/1904.08049v1",
        "pub_date": "2019-04-17",
        "translated_summary": "多标签分类任务旨在为给定样本分配一组目标标签，而如何建模标签间的组合交互关系一直是该领域的长期挑战。我们提出标签消息传递神经网络，通过基于注意力机制的神经消息传递技术，将标签视为标签交互图中的节点，并基于输入计算每个标签节点的隐表示。注意力机制使模型能够为每个标签的相邻节点分配不同权重，从而隐式学习标签间的交互规律。该模型结构简洁、预测精准、可解释性强，且不依赖特定图结构——由于具备高度并行化特性，尤其适用于密集标签预测场景。我们在七个真实多标签数据集上验证了LaMP的优越性，这些数据集覆盖多种输入/输出类型，实验结果表明其性能超越现有最优方法。值得注意的是，LaMP能直观展示每个标签的分类决策如何依赖于样本特征元素，同时揭示其与其他标签的交互依赖关系。代码与数据集已开源：https://github.com/QData/LaMP"
    },
    {
        "title": "Emotional Contribution Analysis of Online Reviews",
        "summary": "In response to the constant increase in population and tourism worldwide, there is a need for the development of cross-language market research tools that are more cost and time effective than surveys or interviews. Focusing on the Chinese tourism boom and the hotel industry in Japan, we extracted the most influential keywords in emotional judgement from Chinese online reviews of Japanese hotels in the portal site Ctrip. Using an entropy based mathematical model and a machine learning algorithm, we determined the words that most closely represent the demands and emotions of this customer base.",
        "entry_id": "http://arxiv.org/abs/1905.00185v1",
        "pub_date": "2019-05-01",
        "translated_summary": "针对全球人口与旅游业的持续增长，亟需开发比传统问卷和访谈更具成本与时间效益的跨语言市场调研工具。本研究聚焦中国游客赴日旅游热潮及日本酒店业，通过携程网中文评论数据，运用基于信息熵的数学模型与机器学习算法，从中国游客对日本酒店的在线评价中提取情感判断最具影响力的关键词，精准识别该客群的核心需求与情感倾向。"
    },
    {
        "title": "FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance",
        "summary": "Frequently Asked Question (FAQ) retrieval is an important task where the objective is to retrieve an appropriate Question-Answer (QA) pair from a database based on a user's query. We propose a FAQ retrieval system that considers the similarity between a user's query and a question as well as the relevance between the query and an answer. Although a common approach to FAQ retrieval is to construct labeled data for training, it takes annotation costs. Therefore, we use a traditional unsupervised information retrieval system to calculate the similarity between the query and question. On the other hand, the relevance between the query and answer can be learned by using QA pairs in a FAQ database. The recently-proposed BERT model is used for the relevance calculation. Since the number of QA pairs in FAQ page is not enough to train a model, we cope with this issue by leveraging FAQ sets that are similar to the one in question. We evaluate our approach on two datasets. The first one is localgovFAQ, a dataset we construct in a Japanese administrative municipality domain. The second is StackExchange dataset, which is the public dataset in English. We demonstrate that our proposed method outperforms baseline methods on these datasets.",
        "entry_id": "http://arxiv.org/abs/1905.02851v2",
        "pub_date": "2019-05-08",
        "translated_summary": "常见问题解答检索是一项重要任务，其目标是根据用户查询从数据库中检索出相应的问题-答案对。我们提出了一种FAQ检索系统，该系统同时考虑用户查询与问题的相似度，以及查询与答案的相关性。尽管构建标注数据进行训练是FAQ检索的常用方法，但标注成本较高。因此，我们采用传统无监督信息检索系统来计算查询与问题的相似度。另一方面，查询与答案的相关性可以通过使用FAQ数据库中的问答对进行学习。我们采用最新提出的BERT模型进行相关性计算。由于FAQ页面中的问答对数量不足以训练模型，我们通过利用与目标FAQ集相似的FAQ集合来解决这一问题。我们在两个数据集上评估了该方法：第一个是本地政务FAQ数据集，这是我们在日本行政市政领域构建的数据集；第二个是StackExchange公共英文数据集。实验证明，我们提出的方法在这两个数据集上均优于基线方法。"
    },
    {
        "title": "Who wrote this book? A challenge for e-commerce",
        "summary": "Modern e-commerce catalogs contain millions of references, associated with textual and visual information that is of paramount importance for the products to be found via search or browsing. Of particular significance is the book category, where the author name(s) field poses a significant challenge. Indeed, books written by a given author (such as F. Scott Fitzgerald) might be listed with different authors' names in a catalog due to abbreviations and spelling variants and mistakes, among others. To solve this problem at scale, we design a composite system involving open data sources for books as well as machine learning components leveraging deep learning-based techniques for natural language processing. In particular, we use Siamese neural networks for an approximate match with known author names, and direct correction of the provided author's name using sequence-to-sequence learning with neural networks. We evaluate this approach on product data from the e-commerce website Rakuten France, and find that the top proposal of the system is the normalized author name with 72% accuracy.",
        "entry_id": "http://arxiv.org/abs/1905.01973v1",
        "pub_date": "2019-04-19",
        "translated_summary": "现代电子商务目录包含数百万种商品，其关联的文本与视觉信息对于用户通过搜索或浏览找到产品至关重要。其中图书类目的作者名字段尤为特殊——同一作者（如F·斯科特·菲茨杰拉德）的著作可能因缩写、拼写变体或错误等原因在目录中呈现不同作者名称。为大规模解决该问题，我们设计了一套复合系统：既整合图书开放数据源，又采用基于深度学习的自然语言处理技术。具体通过孪生神经网络实现与已知作者名的近似匹配，并利用神经网络序列到序列学习直接校正现有作者名。基于法国乐天电商平台产品数据的测试显示，该系统首选建议的标准化作者名准确率达72%。"
    },
    {
        "title": "A Content-Based Approach to Email Triage Action Prediction: Exploration and Evaluation",
        "summary": "Email has remained a principal form of communication among people, both in enterprise and social settings. With a deluge of emails crowding our mailboxes daily, there is a dire need of smart email systems that can recover important emails and make personalized recommendations. In this work, we study the problem of predicting user triage actions to incoming emails where we take the reply prediction as a working example. Different from existing methods, we formulate the triage action prediction as a recommendation problem and focus on the content-based approach, where the users are represented using the content of current and past emails. We also introduce additional similarity features to further explore the affinities between users and emails. Experiments on the publicly available Avocado email collection demonstrate the advantages of our proposed recommendation framework and our method is able to achieve better performance compared to the state-of-the-art deep recommendation methods. More importantly, we provide valuable insight into the effectiveness of different textual and user representations and show that traditional bag-of-words approaches, with the help from the similarity features, compete favorably with the more advanced neural embedding methods.",
        "entry_id": "http://arxiv.org/abs/1905.01991v1",
        "pub_date": "2019-04-30",
        "translated_summary": "电子邮件始终是企业和社交场景中人们沟通的主要方式。面对每日涌入收件箱的海量邮件，智能邮件系统亟需实现重要邮件恢复与个性化推荐功能。本文以回复预测为例，研究用户对接收邮件的分类行为预测问题。与现有方法不同，我们将分类行为预测构建为推荐问题，聚焦于基于内容的研究方法——通过当前及历史邮件内容构建用户画像。通过引入额外相似性特征，进一步挖掘用户与邮件之间的关联性。在公开的Avocado邮件数据集上的实验表明，我们提出的推荐框架具有显著优势，相较当前最先进的深度推荐方法能获得更优性能。更重要的是，我们揭示了不同文本表征和用户表征方法的有效性，并证明传统词袋模型在相似性特征辅助下，可与更先进的神经嵌入方法相媲美。"
    },
    {
        "title": "Deep Landscape Forecasting for Real-time Bidding Advertising",
        "summary": "The emergence of real-time auction in online advertising has drawn huge attention of modeling the market competition, i.e., bid landscape forecasting. The problem is formulated as to forecast the probability distribution of market price for each ad auction. With the consideration of the censorship issue which is caused by the second-price auction mechanism, many researchers have devoted their efforts on bid landscape forecasting by incorporating survival analysis from medical research field. However, most existing solutions mainly focus on either counting-based statistics of the segmented sample clusters, or learning a parameterized model based on some heuristic assumptions of distribution forms. Moreover, they neither consider the sequential patterns of the feature over the price space. In order to capture more sophisticated yet flexible patterns at fine-grained level of the data, we propose a Deep Landscape Forecasting (DLF) model which combines deep learning for probability distribution forecasting and survival analysis for censorship handling. Specifically, we utilize a recurrent neural network to flexibly model the conditional winning probability w.r.t. each bid price. Then we conduct the bid landscape forecasting through probability chain rule with strict mathematical derivations. And, in an end-to-end manner, we optimize the model by minimizing two negative likelihood losses with comprehensive motivations. Without any specific assumption for the distribution form of bid landscape, our model shows great advantages over previous works on fitting various sophisticated market price distributions. In the experiments over two large-scale real-world datasets, our model significantly outperforms the state-of-the-art solutions under various metrics.",
        "entry_id": "http://arxiv.org/abs/1905.03028v2",
        "pub_date": "2019-05-07",
        "translated_summary": "在线广告实时竞价的出现，使得市场竞争建模（即竞价环境预测）受到广泛关注。该问题可表述为预测每次广告竞价市场价格的概率分布。针对第二价格拍卖机制导致的数据截断问题，众多研究者借鉴医学领域的生存分析方法开展竞价环境预测研究。然而现有解决方案大多聚焦于分段样本群的计数统计，或基于分布形式的启发式假设学习参数化模型，且均未考虑特征在价格空间上的序列模式。为在细粒度数据层面捕捉更复杂灵活的模式，我们提出深度融合竞价环境预测模型，将深度学习与生存分析相结合进行概率分布预测与截断数据处理。具体而言，我们利用循环神经网络灵活建模各出价价格对应的条件获胜概率，继而通过概率链式法则进行严格数学推导来实现竞价环境预测。以端到端方式，我们通过最小化两个具有综合动机的负似然损失函数来优化模型。该模型无需对竞价环境分布形式做特定假设，在拟合各类复杂市场价格分布方面较已有工作展现出显著优势。基于两个大规模真实数据集的实验表明，我们的模型在多项指标上均显著优于现有最优解决方案。"
    },
    {
        "title": "A Novel Fuzzy Search Approach over Encrypted Data with Improved Accuracy and Efficiency",
        "summary": "As cloud computing becomes prevalent in recent years, more and more enterprises and individuals outsource their data to cloud servers. To avoid privacy leaks, outsourced data usually is encrypted before being sent to cloud servers, which disables traditional search schemes for plain text. To meet both end of security and searchability, search-supported encryption is proposed. However, many previous schemes suffer severe vulnerability when typos and semantic diversity exist in query requests. To overcome such flaw, higher error-tolerance is always expected for search-supported encryption design, sometimes defined as 'fuzzy search'. In this paper, we propose a new scheme of multi-keyword fuzzy search over encrypted and outsourced data. Our approach introduces a new mechanism to map a natural language expression into a word-vector space. Compared with previous approaches, our design shows higher robustness when multiple kinds of typos are involved. Besides, our approach is enhanced with novel data structures to improve search efficiency. These two innovations can work well for both accuracy and efficiency. Moreover, these designs will not hurt the fundamental security. Experiments on a real-world dataset demonstrate the effectiveness of our proposed approach, which outperforms currently popular approaches focusing on similar tasks.",
        "entry_id": "http://arxiv.org/abs/1904.12111v2",
        "pub_date": "2019-04-27",
        "translated_summary": "近年来，随着云计算的普及，越来越多的企业和个人将数据外包至云服务器。为防止隐私泄露，外包数据通常会在上传至云端前进行加密处理，但这使得传统明文搜索方案无法适用。为实现安全性与可搜索性的统一，可搜索加密技术应运而生。然而，现有方案在查询请求存在拼写错误或语义多样性时存在明显缺陷。为克服这一不足，可搜索加密设计需要具备更高的容错能力，即实现\"模糊搜索\"。本文提出一种支持多关键词模糊搜索的加密外包数据查询方案。该方案创新性地通过词向量空间映射自然语言表达，相较于现有方案，在应对多种拼写错误时展现出更强的鲁棒性。同时，我们采用新型数据结构提升搜索效率，这两项创新在保证准确率的同时显著提升性能，且不会损害基础安全性。在真实数据集上的实验表明，本方案在同等任务中的表现优于当前主流方案。"
    },
    {
        "title": "Topic Classification Method for Analyzing Effect of eWOM on Consumer Game Sales",
        "summary": "Electronic word-of-mouth (eWOM) has become an important resource for the analysis of marketing research. In this study, in order to analyze user needs for consumer game software, we focus on tweet data. And we proposed topic extraction method using entropy-based feature selection based feature expansion. We also applied it to the classification of the data extracted from tweet data by using SVM. As a result, we achieved a 0.63 F-measure.",
        "entry_id": "http://arxiv.org/abs/1904.13213v1",
        "pub_date": "2019-04-23",
        "translated_summary": "电子口碑已成为营销调研分析的重要资源。为探究消费者对游戏软件的需求特性，本研究以推文数据为分析对象，提出基于熵特征选择与特征扩展的主题挖掘方法，并采用支持向量机对推文数据进行分类处理。实验结果显示，该方法的F值评估指标达到0.63。"
    },
    {
        "title": "Advanced Customer Activity Prediction based on Deep Hierarchic Encoder-Decoders",
        "summary": "Product recommender systems and customer profiling techniques have always been a priority in online retail. Recent machine learning research advances and also wide availability of massive parallel numerical computing has enabled various approaches and directions of recommender systems advancement. Worth to mention is the fact that in past years multiple traditional \"offline\" retail business are gearing more and more towards employing inferential and even predictive analytics both to stock-related problems such as predictive replenishment but also to enrich customer interaction experience. One of the most important areas of recommender systems research and development is that of Deep Learning based models which employ representational learning to model consumer behavioral patterns. Current state of the art in Deep Learning based recommender systems uses multiple approaches ranging from already classical methods such as the ones based on learning product representation vector, to recurrent analysis of customer transactional time-series and up to generative models based on adversarial training. Each of these methods has multiple advantages and inherent weaknesses such as inability of understanding the actual user-journey, ability to propose only single product recommendation or top-k product recommendations without prediction of actual next-best-offer. In our work we will present a new and innovative architectural approach of applying state-of-the-art hierarchical multi-module encoder-decoder architecture in order to solve several of current state-of-the-art recommender systems issues. Our approach will also produce by-products such as product need-based segmentation and customer behavioral segmentation - all in an end-to-end trainable approach. Finally, we will present a couple methods that solve known retail & distribution pain-points based on the proposed architecture.",
        "entry_id": "http://arxiv.org/abs/1904.07687v4",
        "pub_date": "2019-04-11",
        "translated_summary": "商品推荐系统与用户画像技术始终是在线零售领域的关注焦点。随着机器学习研究的最新进展以及大规模并行数值计算的广泛普及，推荐系统的发展呈现出多元化趋势。值得注意的是，近年来众多传统线下零售企业正越来越多地运用推断性甚至预测性分析技术，不仅将其应用于库存管理（如预测性补货），更致力于提升客户交互体验。基于深度学习的推荐模型通过表征学习来构建消费者行为模式，已成为该领域的重要研究方向。当前最先进的深度学习推荐系统融合了多种技术路径：既包含基于商品表征向量学习的经典方法，也涵盖客户交易时间序列的循环分析，更延伸至基于对抗训练的生成模型。这些方法虽各具优势，却也存在固有缺陷——例如无法真正理解用户行为路径、仅能推荐单一商品或Top-K商品列表而无法预测真正意义上的\"下一个最佳优惠\"。本研究提出了一种创新的层次化多模块编码器-解码器架构，旨在解决现有推荐系统的若干痛点。该端到端可训练架构不仅能实现核心推荐功能，还将自然衍生出基于需求的产品细分和客户行为细分等副产品。最后，我们将基于该架构提出若干解决零售分销领域典型痛点的方法论。"
    },
    {
        "title": "Short Text Topic Modeling Techniques, Applications, and Performance: A Survey",
        "summary": "Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.",
        "entry_id": "http://arxiv.org/abs/1904.07695v1",
        "pub_date": "2019-04-13",
        "translated_summary": "短文本分析旨在推断出具有判别力且连贯的潜在主题，这是一项关键的基础性任务，因为众多实际应用都需要对短文本进行语义理解。基于词语共现的传统长文本主题建模算法（如PLSA和LDA）难以有效解决该问题，因为短文本中可用的词语共现信息极其有限。因此，旨在克服短文本稀疏性问题的主题建模技术近年来备受机器学习研究界关注。本文系统综述了文献中提出的各类短文本主题建模方法，将其划分为基于狄利克雷多项混合、全局词语共现和自聚合三大类方法，通过典型算法示例分析其在各任务中的性能表现。我们开发了首个综合性开源工具包STTM（基于Java语言），该工具集成了所有综述算法与基准数据集，采用统一接口以促进该研究领域新方法的拓展。最后，我们在多个真实数据集上评估了这些前沿方法，通过纵向对比与长文本主题建模算法的横向比较，全面验证其性能表现。"
    },
    {
        "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
        "summary": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
        "entry_id": "http://arxiv.org/abs/2511.10492v1",
        "pub_date": "2025-11-13",
        "translated_summary": "在推荐系统中，除了准确性之外，针对多样性、新颖性和个性化等目标的优化对长期用户满意度至关重要。工业实践者已积累了大量结构化领域知识，我们将其称为\"人类先验\"（如物品分类体系、时序模式）。这类知识通常通过排名阶段或后排名阶段的后期调整来应用，但这种方法始终与核心模型学习相分离——随着行业向端到端生成式推荐基础模型转型，这种分离尤为不利。另一方面，许多针对超准确性目标的方法往往需要针对特定架构进行修改，并以完全无监督的方式学习用户意图，从而丢弃了这些宝贵的人类先验。\n\n我们提出了一种与主干模型无关的框架，将多年实践积累的人类先验直接整合到生成式推荐器的端到端训练中，而非抛弃这些知识。受高效大语言模型解码策略启发，我们通过轻量级的先验条件适配头，引导模型沿着人类可理解的维度（如交互类型、长短期兴趣）解耦用户意图。同时引入了分层组合策略来建模不同先验类型间的复杂交互。在三个大规模数据集上的实验表明，我们的方法显著提升了准确性及超准确性目标。研究还证实，人类先验能使主干模型更有效地利用更长上下文和更大模型规模。"
    },
    {
        "title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.",
        "entry_id": "http://arxiv.org/abs/2511.10277v1",
        "pub_date": "2025-11-13",
        "translated_summary": "大型语言模型在生成类人文本方面展现出卓越能力，但在计算机游戏对话系统中的应用仍存在局限。这主要源于其较高的硬件需求、延迟限制，以及游戏场景中需保持明确知识边界的必要性。本文提出一种模块化非玩家角色对话系统，通过微调小型语言模型来编码特定角色特征，并与可实时切换的记忆模块相结合。这些记忆模块能够保存角色专属的对话上下文和世界知识，无需在游戏过程中重新训练或加载模型，即可实现富有表现力的交互和长期记忆功能。我们使用三种开源小型语言模型进行系统评估：DistilGPT-2、TinyLlama-1.1B-Chat和Mistral-7B-Instruct，这些模型基于合成的角色对齐数据训练，并在消费级硬件上进行基准测试。虽然该研究受游戏应用驱动，但其模块化设计和角色驱动的记忆架构，对于需要表现力丰富、可扩展且具备深度记忆的对话代理场景具有广泛适用潜力，例如虚拟助手、客服机器人或交互式教育系统等领域。"
    },
    {
        "title": "GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation",
        "summary": "As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR.",
        "entry_id": "http://arxiv.org/abs/2511.10138v1",
        "pub_date": "2025-11-13",
        "translated_summary": "作为连接用户与商业内容的智能枢纽，广告推荐系统在数字经济的信息流通与价值创造中占据核心地位。然而现有多阶段广告推荐系统存在目标错位与误差累积问题，难以实现全局最优；而统一生成式推荐模型在实际工业应用中仍面临诸多挑战。为此，我们提出GPR（生成式预训练推荐系统），首次通过单模型框架将广告推荐重新定义为端到端生成任务，以统一生成范式替代传统级联架构。为实现该框架，我们在统一表征、网络架构和训练策略三大维度实现创新突破：首先设计面向广告场景的统一输入范式与令牌化方法，将广告与自然内容映射至共享的多层级语义ID空间，增强异构数据的语义对齐与建模一致性；其次构建异质层级解码器（HHD），通过双流解码架构分离用户意图建模与广告生成路径，在保持强大建模能力的同时实现训练效率与推理灵活性的平衡；最终提出融合多令牌预测（MTP）、价值感知微调与层级增强策略优化（HEPO）算法的多阶段联合训练策略，形成统一兴趣建模、价值对齐与策略优化的完整生成式推荐链路。GPR已在腾讯微信视频号广告系统全面部署，在GMV、CTCVR等关键业务指标上取得显著提升。"
    },
    {
        "title": "Practical RAG Evaluation: A Rarity-Aware Set-Based Metric and Cost-Latency-Quality Trade-offs",
        "summary": "This paper addresses the guessing game in building production RAG. Classical rank-centric IR metrics (nDCG/MAP/MRR) are a poor fit for RAG, where LLMs consume a set of passages rather than a browsed list; position discounts and prevalence-blind aggregation miss what matters: whether the prompt at cutoff K contains the decisive evidence. Second, there is no standardized, reproducible way to build and audit golden sets. Third, leaderboards exist but lack end-to-end, on-corpus benchmarking that reflects production trade-offs. Fourth, how state-of-the-art embedding models handle proper-name identity signals and conversational noise remains opaque. To address these, we contribute: (1) RA-nWG@K, a rarity-aware, per-query-normalized set score, and operational ceilings via the pool-restricted oracle ceiling (PROC) and the percentage of PROC (%PROC) to separate retrieval from ordering headroom within a Cost-Latency-Quality (CLQ) lens; (2) rag-gs (MIT), a lean golden-set pipeline with Plackett-Luce listwise refinement whose iterative updates outperform single-shot LLM ranking; (3) a comprehensive benchmark on a production RAG (scientific-papers corpus) spanning dense retrieval, hybrid dense+BM25, embedding models and dimensions, cross-encoder rerankers, ANN (HNSW), and quantization; and (4) targeted diagnostics that quantify proper-name identity signal and conversational-noise sensitivity via identity-destroying and formatting ablations. Together, these components provide practitioner Pareto guidance and auditable guardrails to support reproducible, budget/SLA-aware decisions.",
        "entry_id": "http://arxiv.org/abs/2511.09545v1",
        "pub_date": "2025-11-12",
        "translated_summary": "本文针对生产环境RAG构建中的评估难题展开研究。传统以排序为核心的信息检索指标（nDCG/MAP/MRR）与RAG场景存在根本性错配——大语言模型处理的是段落集合而非浏览列表，位置衰减和忽略证据分布的聚合方式无法捕捉核心问题：截断点K处的提示是否包含决定性证据。其次，当前缺乏标准化、可复现的黄金集构建与审计方法。再者，现有排行榜缺少反映生产环境权衡的端到端全库基准。最后，前沿嵌入模型如何处理专有名词标识信号与会话噪声仍不透明。为此我们提出：(1) RA-nWG@K——一种稀有度感知的查询归一化集合评分指标，通过池限制理论上限(PROC)及其百分比(%PROC)在成本-延迟-质量(CLQ)框架下区分检索能力与排序潜力；(2) rag-gs(MIT授权)——采用Plackett-Luce列表优化的轻量级黄金集流程，其迭代更新优于单次LLM排序；(3) 基于科学论文库的生产级RAG综合基准，覆盖稠密检索、混合检索、嵌入模型与维度、交叉编码器重排序、近似最近邻(HNSW)及量化技术；(4) 通过身份标识破坏与格式消融实验，量化专有名词标识信号处理能力与会话噪声敏感性。这些组件共同为实践者提供帕累托决策指导与可审计防护，支持符合预算与服务等级协议的可复现决策。"
    },
    {
        "title": "Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction",
        "summary": "Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR community. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simulation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query predictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench.",
        "entry_id": "http://arxiv.org/abs/2511.09329v1",
        "pub_date": "2025-11-12",
        "translated_summary": "由于缺乏成熟的衡量标准与基准，用户模拟验证始终是一项困难的任务，这使得评估模拟器是否准确反映真实用户行为充满挑战。作为SIGIR 2025 Sim4IA研讨会微共享任务环节的一部分，我们推出首个信息检索领域针对下一查询与对话预测的仿真基准套件Sim4IA-Bench。该套件中的数据集包含来自CORE搜索引擎的160个真实搜索会话，其中70个会话提供多达62次模拟运行数据，分为任务A与任务B，分别对应不同方法对用户后续搜索查询或对话的预测。Sim4IA-Bench为评估比较用户模拟方法及开发新型模拟器效度指标奠定了基础。尽管规模适中，但这是首个公开连接真实搜索会话与模拟下一查询预测的基准套件。除作为下一查询预测测试平台外，它还可支持查询重构行为、意图漂移及交互感知检索评估的探索性研究。我们还为此任务引入新的下一查询预测评估指标。通过开源此套件，我们旨在推动可复现研究，并促进面向信息访问的逼真可解释用户模拟工作：https://github.com/irgroup/Sim4IA-Bench。"
    },
    {
        "title": "NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning",
        "summary": "Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality-level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.",
        "entry_id": "http://arxiv.org/abs/2511.09250v1",
        "pub_date": "2025-11-12",
        "translated_summary": "受脑启发人工智能领域的最新进展试图利用CLIP等多模态模型将神经信号与视觉语义对齐。然而现有方法通常将CLIP视为静态特征提取器，忽视了其对神经表征的适应性以及EEG-图像对齐中固有的生理-符号鸿沟。为解决这些挑战，我们提出NeuroCLIP——一个专为EEG-图像对比学习设计的提示调优框架。该框架包含三项核心创新：(1) 设计双流视觉嵌入管道，结合动态过滤与令牌级融合生成实例级自适应提示，通过图像内容指导图像块嵌入令牌的调整，实现神经约束下的视觉表征细粒度调制；(2) 首次将视觉提示令牌引入EEG-图像对齐，作为全局模态级提示与实例级调整协同工作，这些令牌被嵌入Transformer架构以促进神经感知的全局适应与参数优化；(3) 受人类视觉编码神经科学原理启发，提出改进的对比损失函数，更精准建模EEG信号中的语义歧义与跨模态噪声。在THINGS-EEG2数据集上，NeuroCLIP在零样本图像检索任务中达到63.2%的Top-1准确率，较此前最佳方法提升12.3%，并在跨被试条件下展现出强大泛化能力（Top-1提升4.6%），彰显了生理感知提示调优在连接大脑信号与视觉语义方面的巨大潜力。"
    },
    {
        "title": "Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning",
        "summary": "Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.",
        "entry_id": "http://arxiv.org/abs/2511.09109v2",
        "pub_date": "2025-11-12",
        "translated_summary": "检索增强生成技术虽已被证实能有效缓解大语言模型的幻觉问题，但在复杂多步推理场景中的效能仍显不足。近期研究将基于搜索的交互机制融入RAG，实现了实时检索的迭代推理。然而现有方法多依赖结果监督，未能对中间步骤提供明确指导，易导致奖励破解和回答质量下降。我们提出双向检索增强推理框架Bi-RAR，通过前向与后向联合评估每个推理步骤。为衡量各步骤的信息完备性，基于柯氏复杂度构建了双向信息距离指标，并借助语言模型生成概率进行近似计算。该量化方法既能评估当前推理与答案的距离，又可衡量其对问题的解答程度。在此双向信号优化方面，采用具有级联奖励结构的多目标强化学习框架，重点强化早期轨迹对齐。在七个问答基准测试中的实证结果表明，Bi-RAR不仅超越现有方法，还能在训练与推理过程中实现与搜索引擎的高效交互推理。"
    },
    {
        "title": "Efficient Model-Agnostic Continual Learning for Next POI Recommendation",
        "summary": "Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption.",
        "entry_id": "http://arxiv.org/abs/2511.08941v1",
        "pub_date": "2025-11-12",
        "translated_summary": "下一兴趣点推荐通过用户历史签到记录预测其下一个目的地，从而提升个性化基于位置的服务质量。然而现有方法大多依赖静态数据集和固定模型，难以适应用户行为随时间的动态变化。为突破这一局限，我们探索名为\"持续下一兴趣点推荐\"的新任务，使模型能通过持续更新机制动态适应用户兴趣的演变。该任务面临双重挑战：既要捕捉用户行为的动态变化，又需保留已学知识。同时，在实际部署中还需确保更新时效与内存使用效率。为此，我们提出GIRAM框架——基于生成式关键字的兴趣检索与自适应建模，这个高效且模型无关的框架将情境感知的持续兴趣与近期兴趣相融合。GIRAM包含四个核心组件：(1)用于保存历史偏好的兴趣记忆库；(2)实现统一兴趣关键字表征的情境感知编码模块；(3)基于生成式关键字的检索模块，用于识别多样化相关持续兴趣；(4)自适应兴趣更新与融合模块，负责更新兴趣记忆库并平衡持续兴趣与近期兴趣。该框架可与现有下一兴趣点推荐模型无缝集成。在三个真实数据集上的实验表明，GIRAM在保持高效更新时间与内存消耗的同时，持续优于现有最优方法。"
    },
    {
        "title": "Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding",
        "summary": "Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.",
        "entry_id": "http://arxiv.org/abs/2511.08480v1",
        "pub_date": "2025-11-11",
        "translated_summary": "视觉语言模型通过获取可迁移的语义嵌入推动多模态表征学习，从而显著提升跨模态检索、聚类与分类等视觉语言任务的性能。理想的嵌入向量应全面保留输入语义内容，同时突出下游任务所需的判别性特征。最新研究表明，通过大规模对比学习可将VLMs转化为具有竞争力的嵌入模型，实现两个互补目标的同步优化。我们认为上述两个目标可解耦：对输入的全面理解有助于嵌入模型通过对比学习在下游任务中获得更优表现。本文提出压缩式预训练阶段CoMa，作为对比学习的热身阶段。实验表明，仅需少量预训练数据即可将VLM转化为具有竞争力的嵌入模型。CoMa在MMEB基准测试中实现了同体量VLMs的最优性能，在效率与效果上达成双重突破。"
    },
    {
        "title": "Advancing Scientific Knowledge Retrieval and Reuse with a Novel Digital Library for Machine-Readable Knowledge",
        "summary": "Digital libraries for research, such as the ACM Digital Library or Semantic Scholar, do not enable the machine-supported, efficient reuse of scientific knowledge (e.g., in synthesis research). This is because these libraries are based on document-centric models with narrative text knowledge expressions that require manual or semi-automated knowledge extraction, structuring, and organization. We present ORKG reborn, an emerging digital library that supports finding, accessing, and reusing accurate, fine-grained, and reproducible machine-readable expressions of scientific knowledge that relate scientific statements and their supporting evidence in terms of data and code. The rich expressions of scientific knowledge are published as reborn (born-reusable) articles and provide novel possibilities for scientific knowledge retrieval, for instance by statistical methods, software packages, variables, or data matching specific constraints. We describe the proposed system and demonstrate its practical viability and potential for information retrieval in contrast to state-of-the-art digital libraries and document-centric scholarly communication using several published articles in research fields ranging from computer science to soil science. Our work underscores the enormous potential of scientific knowledge databases and a viable approach to their construction.",
        "entry_id": "http://arxiv.org/abs/2511.08476v1",
        "pub_date": "2025-11-11",
        "translated_summary": "诸如ACM数字图书馆或语义学者（Semantic Scholar）等研究型数字图书馆，尚无法实现科学知识的机器支持高效复用（例如在综述研究中）。这是因为这些图书馆基于以文档为中心的模型，其叙述性文本的知识表达方式需要人工或半自动化的知识提取、结构化与组织。我们推出“重生版开放研究知识图谱”（ORKG reborn），这一新兴数字图书馆支持查找、访问并复用精确、细粒度、可重现的机器可读科学知识表达，这些表达通过数据与代码将科学论断及其支撑证据相互关联。这种丰富的科学知识表达以“重生”（即可重复使用）论文的形式发布，为科学知识检索提供了全新可能，例如通过统计方法、软件包、变量或符合特定约束条件的数据进行检索。我们详细描述了该系统的设计，并通过计算机科学到土壤科学等多个研究领域已发表论文的实例，对比现有顶尖数字图书馆和以文档为中心的学术交流模式，论证了该系统在信息检索方面的实际可行性与潜力。我们的工作彰显了科学知识数据库的巨大潜力，并为其构建提供了可行路径。"
    },
    {
        "title": "Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents",
        "summary": "Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a \"see-saw\" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \\textbf{HID} (\\textbf{H}ybrid \\textbf{I}ntent-based \\textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional \"see-saw\" into \"win-win\" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \\textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \\textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \\textit{diversity} and \\textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems.",
        "entry_id": "http://arxiv.org/abs/2511.08378v1",
        "pub_date": "2025-11-11",
        "translated_summary": "基于会话的推荐旨在根据匿名用户的交互会话预测其下一次交互行为。在实际推荐场景中，低曝光商品构成了交互行为的主体，这种长尾分布严重影响了推荐多样性。现有方法试图通过提升尾部商品曝光来解决该问题，却导致推荐准确率下降，呈现出长尾性能与准确率之间的“跷跷板效应”。我们认为这一矛盾源于尾部商品中存在的会话无关噪声，而现有长尾处理方法未能有效识别和约束此类噪声。为解决这一根本矛盾，我们提出\\textbf{HID}框架——一种即插即用的混合意图双约束框架，通过引入面向长尾性能和准确率的混合意图双约束，将传统的“跷跷板”关系转化为“共赢”关系。该框架包含两大核心创新：(一) \\textit{混合意图学习}，通过采用属性感知谱聚类重构商品-意图映射关系，重新构建意图提取策略。此外，通过为每个会话分配目标意图和噪声意图，实现会话无关噪声的甄别；(二) \\textit{意图约束损失函数}，融合了面向\\textit{多样性}和\\textit{准确率}的两个创新约束范式，用以规范商品和会话的表示学习过程。通过严格的理论推导，这两个目标被统一到单个训练损失函数中。在多个SBR模型和数据集上的大量实验表明，HID能同时提升长尾性能和推荐准确率，在长尾推荐系统中确立了新的性能标杆。"
    },
    {
        "title": "TurkEmbed: Turkish Embedding Model on NLI & STS Tasks",
        "summary": "This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.",
        "entry_id": "http://arxiv.org/abs/2511.08376v1",
        "pub_date": "2025-11-11",
        "translated_summary": "本文介绍了TurkEmbed——一种新型土耳其语嵌入模型，其设计目标是在自然语言推理（NLI）和语义文本相似度（STS）任务中超越现有模型。当前土耳其语嵌入模型普遍依赖机器翻译数据集，这可能限制其准确性和语义理解能力。TurkEmbed通过融合多样化数据集与先进训练技术（包括套娃表示学习），实现了更鲁棒精准的嵌入表示。该方法使模型能适配各类资源受限环境，并提供更快速的编码能力。我们在土耳其语STS-b-TR数据集上采用皮尔逊与斯皮尔曼相关指标进行评估，结果显示该模型在语义相似度任务中取得显著提升。此外，TurkEmbed在All-NLI-TR和STS-b-TR基准测试中超越当前最优模型Emrecan，实现了1-4%的性能提升。TurkEmbed有望通过提供更精细的语言理解能力并推动下游应用发展，从而增强土耳其语自然语言处理生态系统。"
    },
    {
        "title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress",
        "summary": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.",
        "entry_id": "http://arxiv.org/abs/2511.08325v1",
        "pub_date": "2025-11-11",
        "translated_summary": "尽管大型语言模型（LLM）发展迅速，但在网络购物、浏览器导航等多轮决策任务（即智能体任务）中仍面临挑战。这类任务要求根据环境反馈进行一系列智能决策。以往针对LLM智能体的研究通常依赖精心设计的提示工程或通过专家轨迹进行微调来提升性能。本研究另辟蹊径：探索构建过程奖励模型（PRM）来评估每个决策步骤并指导智能体的决策过程。与LLM推理中每一步都基于正确性评分不同，智能体任务中的行为没有绝对的正确性标准，而应通过其与目标的接近程度及已取得的进展来评估。基于这一洞见，我们提出了面向智能体任务的重新定义的过程奖励模型AgentPRM，该模型能同时捕捉序列决策间的相互关联及其对最终目标的贡献，从而实现更优的进度跟踪与探索-利用平衡。为高效获取训练AgentPRM所需的标注数据，我们采用基于时序差分（TD）的估计方法结合广义优势估计（GAE），该方法被证明比现有方法更具样本效率。跨多个智能体任务的广泛实验表明，AgentPRM的计算效率较基线方法提升超过8倍，且在扩展测试时计算资源时展现出稳健的性能提升。此外，我们通过详细分析揭示了该方法的作用机制，并提供了更多洞见，例如将AgentPRM应用于LLM智能体的强化学习。"
    },
    {
        "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System",
        "summary": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag",
        "entry_id": "http://arxiv.org/abs/2511.08181v1",
        "pub_date": "2025-11-11",
        "translated_summary": "当前，推荐系统领域正致力于通过利用模态信息或引入基于大语言模型卓越推理能力的智能体概念，来缓解冷启动场景下的局限性。与此同时，由于餐饮领域独特的数据属性与关系特征，传统餐饮推荐系统多采用知识图谱与本体论概念。在此背景下，我们提出MARC——一个基于智能体检索增强生成的多模态多任务鸡尾酒推荐系统，该系统在冷启动条件下利用图数据库实现推荐。该体系通过任务识别路由器和反思机制两大核心流程，生成高质量且符合情境的推荐结果。我们通过处理Kaggle平台的鸡尾酒数据构建图数据库，并采用200道人工编制的问题进行效果评估。评估过程综合运用大语言模型即评判与人工评估两种方式，结果表明基于图数据库生成的答案质量显著优于简单向量数据库方案。代码已开源：https://github.com/diddbwls/cocktail_rec_agentrag"
    },
    {
        "title": "DiffuGR: Generative Document Retrieval with Diffusion Language Models",
        "summary": "Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.08150v1",
        "pub_date": "2025-11-11",
        "translated_summary": "生成式检索将文档检索重新定义为基于序列的文档标识符生成任务，通过模型参数记忆文档，实现无需显式索引的端到端检索。现有生成式检索方法均基于自回归生成模型，即从左到右顺序生成标识符。然而这类方法存在两大局限：（1）文档标识符生成与自然语言生成存在本质差异，早期生成的错误标识符会导致完全错误的检索结果；（2）无法动态平衡检索效率与准确率之间的权衡关系，而这在实际应用中至关重要。为克服这些局限，我们提出基于扩散语言模型的生成式检索方法DiffuGR。该方法将文档标识符生成建模为离散扩散过程：训练阶段通过随机掩码破坏文档标识符，并学习扩散语言模型在检索感知目标下恢复被破坏的标识符；推理阶段则尝试并行生成标识符，并通过可控的去噪步骤进行优化。相较于传统的自回归解码机制，DiffuGR首创了先生成高置信度标识符、再通过扩散去噪优化的新范式。此外，DiffuGR还能显式控制质量与延迟的权衡关系。在标准检索数据集上的大量实验表明，DiffuGR在保持与强基线自回归方法竞争力的同时，可通过调整去噪次数实现灵活的精度-速度权衡。我们的研究结果证明，非自回归扩散模型是生成式文档检索领域具有实用价值的有效替代方案。"
    },
    {
        "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives",
        "summary": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.",
        "entry_id": "http://arxiv.org/abs/2511.08029v1",
        "pub_date": "2025-11-11",
        "translated_summary": "硬负样本对于训练高效检索模型至关重要。硬负样本挖掘通常依赖于使用交叉编码器或基于余弦距离等相似性度量的静态嵌入模型对文档进行排序。在生物医学和科学领域，由于难以区分源文档与硬负样本文档，硬负样本挖掘变得颇具挑战。然而，被引文献天然与源文档具有上下文关联性但并非重复内容，这使其成为理想的硬负样本。本研究提出BiCA：基于引文感知硬负样本的生物医学稠密检索方法，通过利用20,000篇PubMed文献中的引文链接进行硬负样本挖掘，以改进领域专用的小型稠密检索器。我们使用这些引文指导的负样本对GTE_small和GTE_Base模型进行微调，在BEIR数据集的内域和外域任务中通过nDCG@10指标观察到零样本稠密检索的持续提升，并在LoTTE数据集的长尾主题上使用Success@5指标超越基线。我们的研究结果揭示了利用文档链接结构生成高信息量负样本的潜力，通过最小化微调即可实现最先进性能，为高数据效率的领域自适应开辟了新路径。"
    },
    {
        "title": "From IDs to Semantics: A Generative Framework for Cross-Domain Recommendation with Adaptive Semantic Tokenization",
        "summary": "Cross-domain recommendation (CDR) is crucial for improving recommendation accuracy and generalization, yet traditional methods are often hindered by the reliance on shared user/item IDs, which are unavailable in most real-world scenarios. Consequently, many efforts have focused on learning disentangled representations through multi-domain joint training to bridge the domain gaps. Recent Large Language Model (LLM)-based approaches show promise, they still face critical challenges, including: (1) the \\textbf{item ID tokenization dilemma}, which leads to vocabulary explosion and fails to capture high-order collaborative knowledge; and (2) \\textbf{insufficient domain-specific modeling} for the complex evolution of user interests and item semantics. To address these limitations, we propose \\textbf{GenCDR}, a novel \\textbf{Gen}erative \\textbf{C}ross-\\textbf{D}omain \\textbf{R}ecommendation framework. GenCDR first employs a \\textbf{Domain-adaptive Tokenization} module, which generates disentangled semantic IDs for items by dynamically routing between a universal encoder and domain-specific adapters. Symmetrically, a \\textbf{Cross-domain Autoregressive Recommendation} module models user preferences by fusing universal and domain-specific interests. Finally, a \\textbf{Domain-aware Prefix-tree} enables efficient and accurate generation. Extensive experiments on multiple real-world datasets demonstrate that GenCDR significantly outperforms state-of-the-art baselines. Our code is available in the supplementary materials.",
        "entry_id": "http://arxiv.org/abs/2511.08006v1",
        "pub_date": "2025-11-11",
        "translated_summary": "跨领域推荐对提升推荐准确性与泛化能力至关重要，但传统方法常受限于对共享用户/物品ID的依赖，而这类ID在现实场景中往往不可用。为此，研究者们多聚焦于通过多领域联合训练学习解耦表征以弥合领域差异。尽管基于大语言模型的新方法展现出潜力，仍面临两大核心挑战：（1）**物品ID标记化困境**，导致词表爆炸且无法捕获高阶协同知识；（2）对用户兴趣与物品语义复杂演化的**领域特异性建模不足**。针对这些局限，我们提出**GenCDR**——一个创新的**生成式跨领域推荐框架**。该框架首先通过**领域自适应标记化模块**，借助通用编码器与领域适配器间的动态路由生成解耦的物品语义ID；对称地，**跨领域自回归推荐模块**通过融合通用兴趣与领域特异性兴趣建模用户偏好；最后通过**领域感知前缀树**实现高效精准的生成。在多组真实数据集上的大量实验表明，GenCDR显著优于当前最先进的基线模型。代码已附于补充材料中。"
    },
    {
        "title": "TurkEmbed4Retrieval: Turkish Embedding Model for Retrieval Task",
        "summary": "In this work, we introduce TurkEmbed4Retrieval, a retrieval specialized variant of the TurkEmbed model originally designed for Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. By fine-tuning the base model on the MS MARCO TR dataset using advanced training techniques, including Matryoshka representation learning and a tailored multiple negatives ranking loss, we achieve SOTA performance for Turkish retrieval tasks. Extensive experiments demonstrate that our model outperforms Turkish colBERT by 19,26% on key retrieval metrics for the Scifact TR dataset, thereby establishing a new benchmark for Turkish information retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.07595v1",
        "pub_date": "2025-11-10",
        "translated_summary": "本研究推出了TurkEmbed4Retrieval——这是专为检索任务优化的TurkEmbed模型变体，原模型设计用于自然语言推理（NLI）与语义文本相似度（STS）任务。通过在MS MARCO TR数据集上采用先进训练技术对基础模型进行微调，包括套娃表示学习与定制化的多重负样本排序损失函数，我们实现了土耳其语检索任务的性能突破。大量实验表明，在Scifact TR数据集的关键检索指标上，本模型以19.26%的优势超越土耳其语colBERT模型，由此为土耳其语信息检索树立了全新基准。"
    },
    {
        "title": "Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models",
        "summary": "Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.",
        "entry_id": "http://arxiv.org/abs/2511.07581v1",
        "pub_date": "2025-11-10",
        "translated_summary": "高效的信息检索要求能够基于局部证据进行推理，并在信息出现时不断优化策略。然而现有方法存在明显局限：神经检索模型缺乏推理能力，大语言模型虽能提供语义深度但计算成本过高，而查询重写或分解方法仅能实现静态转换。这些方法均无法满足复杂用户查询所需的探索、反馈与修正的迭代动态过程。我们提出Orion训练框架，使轻量化模型（3.5-12亿参数）通过学习搜索策略实现迭代检索。该框架包含三大核心组件：（1）通过合成轨迹生成与监督微调激发模型多样化探索模式；（2）采用强化学习奖励有效的查询优化与回溯行为；（3）基于束搜索的推理算法，利用强化学习阶段习得的自反思能力。尽管仅使用3%的训练数据，我们的12亿参数模型在SciFact上达成77.6%成功率（优于原有72.6%），BRIGHT达到25.2%（原22.1%），NFCorpus提升至63.2%（原57.8%），并在FEVER、HotpotQA和MSMarco保持竞争力。在六项基准测试中有五项超越参数量200-400倍的检索模型。这表明当模型被训练具备搜索、反思与修正能力时，检索性能的提升可源于学习策略本身，而不仅依赖模型规模。"
    },
    {
        "title": "A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain",
        "summary": "Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.",
        "entry_id": "http://arxiv.org/abs/2511.07577v1",
        "pub_date": "2025-11-10",
        "translated_summary": "现有的检索增强生成系统通常采用集中式架构，这导致数据收集、整合与管理成本高昂，并引发隐私担忧。业界亟需一种去中心化的RAG系统，使基础模型能够直接利用数据所有者控制的信息源，同时确保数据所有者保持对数据的完全掌控。然而，去中心化架构面临关键挑战：大量独立数据源的可靠性差异显著，可能降低检索精度与响应质量。为此，我们提出的去中心化RAG系统创新性地引入了可靠性评分机制，该机制根据各数据源在生成响应过程中的贡献质量进行动态评估，并在检索时优先调用高质量数据源。为确保透明度与可信度，评分过程通过基于区块链的智能合约进行安全管理，建立可验证且防篡改的可靠性记录，无需依赖中心化机构。我们使用两个Llama模型（3B和8B参数）在模拟环境中对系统进行评估，该环境包含六个具有不同可靠性等级的数据源。在模拟真实世界不可靠数据环境时，本系统相较集中式系统实现了10.7%的性能提升。值得注意的是，在理想可靠数据环境下，其性能已逼近集中式系统的理论上限。该去中心化基础设施通过批量更新操作实现了约56%的边际成本节约，同时保障了评分管理的安全可信。我们的代码与系统已在github.com/yining610/Reliable-dRAG开源。"
    },
    {
        "title": "GRIN Transfer: A production-ready tool for libraries to retrieve digital copies from Google Books",
        "summary": "Publicly launched in 2004, the Google Books project has scanned tens of millions of items in partnership with libraries around the world. As part of this project, Google created the Google Return Interface (GRIN). Through this platform, libraries can access their scanned collections, the associated metadata, and the ongoing OCR and metadata improvements that become available as Google reprocesses these collections using new technologies. When downloading the Harvard Library Google Books collection from GRIN to develop the Institutional Books dataset, we encountered several challenges related to rate-limiting and atomized metadata within the GRIN platform. To overcome these challenges and help other libraries make more robust use of their Google Books collections, this technical report introduces the initial release of GRIN Transfer. This open-source and production-ready Python pipeline allows partner libraries to efficiently retrieve their Google Books collections from GRIN. This report also introduces an updated version of our Institutional Books 1.0 pipeline, initially used to analyze, augment, and assemble the Institutional Books 1.0 dataset. We have revised this pipeline for compatibility with the output format of GRIN Transfer. A library could pair these two tools to create an end-to-end processing pipeline for their Google Books collection to retrieve, structure, and enhance data available from GRIN. This report gives an overview of how GRIN Transfer was designed to optimize for reliability and usability in different environments, as well as guidance on configuration for various use cases.",
        "entry_id": "http://arxiv.org/abs/2511.11447v1",
        "pub_date": "2025-11-14",
        "translated_summary": "谷歌图书项目于2004年正式启动，已与全球多家图书馆合作扫描数千万册文献。作为该项目的重要组成部分，谷歌开发了谷歌回传接口（GRIN）。通过该平台，合作图书馆可获取其馆藏扫描文献、相关元数据，以及谷歌运用新技术重新处理文献时持续优化的OCR文本与元数据。当哈佛图书馆通过GRIN平台下载谷歌图书资源以构建机构图书数据集时，我们遇到了接口速率限制与元数据原子化等技术挑战。为突破这些限制并助力其他图书馆更高效地利用谷歌图书资源，本技术报告正式发布GRIN Transfer工具。这套开源即用的Python流水线系统，可帮助合作图书馆从GRIN平台快速获取图书资源。报告同时推出了机构图书1.0流水线的升级版本——该原始流水线最初用于分析、增强和整合机构图书1.0数据集。我们已对其进行了重构，使其兼容GRIN Transfer的输出格式。图书馆可组合使用这两套工具，构建端到端的谷歌图书资源处理流程，实现从GRIN平台的数据获取、结构化处理到质量增强的全链条操作。本报告详细阐述了GRIN Transfer如何针对不同环境优化可靠性及易用性的设计思路，并为多种应用场景提供了配置指南。"
    },
    {
        "title": "Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database",
        "summary": "Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.\n  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.\n  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.",
        "entry_id": "http://arxiv.org/abs/2511.11399v1",
        "pub_date": "2025-11-14",
        "translated_summary": "近年来，结合图数据库的图机器学习技术因其处理复杂互联数据的能力，以及运用图数据科学实施机器学习的特点而日益重要。然而，当前图数据库-图机器学习应用在数据分析方式上存在显著缺陷，尤其在知识图谱的知识补全环节表现得尤为突出。现有架构往往忽略知识补全环节，直接处理表面不完整或碎片化的数据集，尽管这些数据实则蕴含宝贵的隐藏知识。这种局限性可能导致在使用这些数据作为图机器学习模型输入时产生错误解读。\n\n本文提出了一种创新架构，将知识补全阶段整合至图数据库-图机器学习应用中，通过实证揭示了显性化隐藏知识如何深刻影响数据集表现与评估指标。为此，我们引入了可扩展的传递关系——这种通过衰减函数建模的关系链能在网络中传播信息，实现跨节点的确定性知识流动。\n\n实验结果表明，我们的创新构想能从根本上重塑数据集的拓扑结构与整体动态特征，这印证了采用新型图数据库-图机器学习架构的必要性：既能构建更优质的模型，又能充分释放图数据分析的全部潜力。"
    },
    {
        "title": "SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation",
        "summary": "LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.\n  To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop \"assess-validate-reflect\" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks.",
        "entry_id": "http://arxiv.org/abs/2511.11370v1",
        "pub_date": "2025-11-14",
        "translated_summary": "基于大语言模型的智能体正成为一种新兴范式，通过模拟用户行为来增强推荐系统性能。然而现有研究多聚焦于对单一物品评分的建模，这种点对点范式导致模型存在用户偏好理解失准和物品语义表征僵化等普遍问题。为突破这些局限，我们提出创新性的集合式反思学习框架。该框架通过构建\"评估-验证-反思\"的闭环流程，充分发挥大语言模型的上下文学习优势。与传统点对点评估不同，我们的框架对整组物品进行整体判断，通过综合分析物品间错综复杂的内部关联及其与用户偏好画像的集体契合度，实现集合层面的情境理解。这种方法使模型能捕捉用户行为中至关重要的复杂关系模式，从而显著提升序列推荐效能。大量实验验证了本方法的优越性，证实集合式视角对实现序列推荐任务最先进性能具有关键作用。"
    },
    {
        "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising",
        "summary": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.",
        "entry_id": "http://arxiv.org/abs/2511.11305v1",
        "pub_date": "2025-11-14",
        "translated_summary": "我们正式推出MOON——一套面向电商应用的多模态表征学习可持续迭代实践体系。该体系已全面部署于淘宝搜索广告系统的检索、相关性、排序等全链路环节，在点击率预测任务上取得显著效果，实现总点击率20.00%的大幅提升。这一历时三年的项目已完成五轮全链路迭代，成为点击率预测任务改进幅度最大的实践。在MOON体系的探索迭代过程中，我们积累了宝贵洞见与实践经验，现将其凝练为包含“预训练-后训练-应用”的三阶段训练范式，有效打通多模态表征与下游任务的衔接通道。值得注意的是，为弥合多模态表征学习目标与下游训练目标之间的错位，我们创新性地定义了“兑换率”指标，用以量化中间指标提升对下游收益的转化效能。通过该分析框架，我们成功锁定基于图像的搜索召回率作为指导多模态模型优化的关键中间指标。历经三年五轮迭代，MOON体系在数据处理、训练策略、模型架构与下游应用四大维度持续演进，相关经验教训与深度洞察将在本文分享。作为对电商领域规模化效应的探索延伸，我们进一步系统研究了多模态表征学习的规模法则，深入解析训练词元数量、负样本规模、用户行为序列长度等多重因素的协同影响规律。"
    },
    {
        "title": "SQuaD: The Software Quality Dataset",
        "summary": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
        "entry_id": "http://arxiv.org/abs/2511.11265v1",
        "pub_date": "2025-11-14",
        "translated_summary": "软件质量研究日益依赖于能够同时衡量软件系统产品维度与过程维度的大规模数据集。然而现有资源往往聚焦于有限维度（如代码异味、技术债或重构活动），从而制约了跨时间维度和质量维度的综合分析。为弥补这一空白，我们推出软件质量数据集SQuaD——这是一个从450个成熟开源项目（涵盖Apache、Mozilla、FFmpeg和Linux内核等多元生态系统）中提取的多维度、时间感知型软件质量指标集合。通过集成九种前沿静态分析工具（SonarQube、CodeScene、PMD、Understand、CK、JaSoMe、RefactoringMiner、RefactoringMiner++和PyRef），本数据集在方法、类、文件和项目层级统一了700余项独特指标。SQuaD覆盖总计63,586个经过分析的项目版本，同时提供版本控制与问题追踪历史、软件漏洞数据（CVE/CWE），以及被证实能增强即时缺陷预测的过程指标。该数据集支持在可维护性、技术债、软件演进和质量评估方面开展前所未有的实证研究。我们还规划了新兴研究方向，包括自动化数据集更新与跨项目质量建模，以支撑软件分析技术的持续演进。本数据集已在ZENODO平台公开发布（DOI: 10.5281/zenodo.17566690）。"
    },
    {
        "title": "Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation",
        "summary": "Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform.",
        "entry_id": "http://arxiv.org/abs/2511.11255v1",
        "pub_date": "2025-11-14",
        "translated_summary": "大语言模型在利用结构化世界知识与多步推理能力方面展现出显著优势，但由于语义和行为层面的错位，将其转化为现实推荐系统仍存在根本性挑战。为弥合这一差距，我们提出Align³GR创新框架，通过三层次对齐实现统一：融合用户-项目语义信号与协同信号的双重标记化机制；基于双向语义对齐的增强行为建模；结合自我博弈优化与真实反馈的渐进式直接偏好优化策略。实验表明，在公开数据集上Align³GR的Recall@10和NDCG@10指标分别超越现有最优基线17.8%和20.2%，在工业级推荐平台的在线A/B测试与全量部署中均取得显著效果提升。"
    },
    {
        "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition",
        "summary": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.",
        "entry_id": "http://arxiv.org/abs/2511.11172v1",
        "pub_date": "2025-11-14",
        "translated_summary": "随着团体活动的日益普及，如何基于群体成员的集体偏好为整个用户群提供推荐的需求不断增长。尽管已有多种团体推荐系统被提出，但这些方法常因实际应用中普遍存在的数据稀疏性和高维度问题而效果受限。本文提出了一种名为Group Soft-Impute SVD的团体推荐系统，通过软填充奇异值分解技术来增强团体推荐效果。该方法利用低秩矩阵补全技术应对稀疏高维数据的挑战。在Goodbooks、Movielens和合成数据集上的实验表明，与基于群体矩阵分解的方法相比，本方法在小规模用户群体的召回率指标上表现更优，同时在所有群体规模下均能取得相当的结果。此外，本方法能恢复比基线模型更低的矩阵秩，证明了其在处理高维数据方面的有效性。"
    },
    {
        "title": "GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs",
        "summary": "Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as \"redacted documents\" or \"pie charts.\" We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net.",
        "entry_id": "http://arxiv.org/abs/2511.11010v1",
        "pub_date": "2025-11-14",
        "translated_summary": "过去三十年的努力已建成包含数十亿网页快照和PB级数据的网络档案馆。仅\"任期结束网络档案馆\"就收录了联邦政府制作的数百万份PDF文件及其他类型文件。尽管网络存档在保存方面成果显著，但在访问和可发现性方面仍存在重大挑战。例如，当前对\"任期结束PDF\"的浏览功能仅限于下载和浏览单个PDF文件，以及执行基本关键词检索。本文推出GovScape公共检索系统，该系统支持对2020年\"任期结束\"网络抓取中10,015,993份联邦政府PDF文件（总计70,958,487页）进行多模态检索——据我们所知，这涵盖了2020年抓取中所有可渲染且不超过50页的PDF文件。GovScape为这千万量级PDF提供四种主要检索方式：除支持(1)基于域名和抓取日期等元数据面的筛选条件及(2)精确文本检索外，还提供(3)语义文本检索与(4)跨页视觉检索，使用户能构建\"经修订文件\"或\"饼状图\"等结构化查询。我们详细阐述了GovScape的构成组件，包括检索功能、嵌入流程、系统架构和开源代码库。值得注意的是，该系统预处理千万份PDF的总计算成本约为1,500美元，相当于每美元计算成本可处理47,000页PDF，展现出即时扩展的潜力。基于此，我们已着手推进亿级PDF规模的多模态检索研究。GovScape可通过https://www.govscape.net 访问。"
    },
    {
        "title": "LEMUR: Large scale End-to-end MUltimodal Recommendation",
        "summary": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.",
        "entry_id": "http://arxiv.org/abs/2511.10962v1",
        "pub_date": "2025-11-14",
        "translated_summary": "传统基于ID的推荐系统常面临冷启动和泛化性挑战。融合文本与视觉特征的多模态推荐系统为缓解这些问题提供了新思路。然而现有工业级方案通常采用两阶段训练范式：先预训练多模态模型，再将其冻结的特征表示应用于推荐模型训练。这种解耦框架存在多模态学习与推荐目标失配、无法动态适配新数据等缺陷。为解决这些问题，我们提出首个基于原始数据端到端训练的大规模多模态推荐系统LEMUR。通过联合优化多模态模块与推荐模块，该系统在实现实时参数更新的同时，能更紧密地对齐下游任务目标。从用户历史行为构建多模态序列表示往往伴随高昂计算成本，为此我们设计了一种新型记忆库机制，可在训练过程中渐进式累积历史多模态表示。在抖音搜索场景部署一个月后，LEMUR使查询变更率衰减降低0.843%，QAUC指标提升0.81%，同时在抖音广告核心离线指标上均取得显著收益。实验结果验证了端到端多模态推荐在真实工业场景中的优越性。"
    },
    {
        "title": "Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports",
        "summary": "Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.",
        "entry_id": "http://arxiv.org/abs/2511.13523v1",
        "pub_date": "2025-11-17",
        "translated_summary": "医疗记录数字化常依赖智能手机拍摄的印刷报告，但生成的图像常因模糊、阴影及其他噪点而质量受损。传统OCR系统虽针对洁净扫描文件优化，在此类现实场景中表现不佳。本研究评估了紧凑型多模态语言模型作为隐私保护方案，在转录含噪临床文档中的应用。通过采用印度医疗环境中常见的、带有地域特色医疗英语撰写的产科超声报告，我们从转录准确率、噪声敏感度、数字精确度及计算效率四个维度比较了八类系统。实验表明，紧凑型多模态模型持续优于传统与神经OCR流程。尽管其计算成本较高，但卓越的鲁棒性与语言适应性使其成为医疗本地化数字化可行的技术选择。"
    },
    {
        "title": "PolicyBot - Reliable Question Answering over Policy Documents",
        "summary": "All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.",
        "entry_id": "http://arxiv.org/abs/2511.13489v1",
        "pub_date": "2025-11-17",
        "translated_summary": "一国公民皆受政府制定的法律政策影响。这些法律政策承担着为公民赋予特定权利、规定相应义务等重要职能。然而此类文件往往篇幅冗长、内容复杂且难以检索，致使公民难以快速定位并理解相关信息。本文推出PolicyBot系统——一个注重透明度与可复现性的检索增强生成框架，专门用于基于政策文档的智能问答。该系统融合领域语义分块、多语言稠密嵌入、多阶段检索重排序及溯源感知生成技术，确保应答内容忠实于原始文献。我们通过实施引文溯源来减少幻觉生成并增强用户信任，同时评估了多种检索与生成方案以确定最优架构。该端到端系统完全基于开源工具构建，可轻松适配其他需基于文档的问答场景。本研究重点阐述了在政务相关场景中部署可信赖检索增强生成系统时的设计考量、实践挑战与经验总结。"
    },
    {
        "title": "Exploring Multi-Table Retrieval Through Iterative Search",
        "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
        "entry_id": "http://arxiv.org/abs/2511.13418v1",
        "pub_date": "2025-11-17",
        "translated_summary": "面向数据湖的开放域问答需从多表中检索并整合信息，这一具有挑战性的子任务既要求语义相关性又需保持结构连贯性（如可连接性）。虽然混合整数规划等精确优化方法能确保结构连贯，但其计算复杂度往往令人望而却步。相反，仅针对查询覆盖度进行优化的简单贪心启发式方法又难以找到这些具备连贯性的可连接表集合。本文将多表检索构建为迭代搜索过程，论证该方法在可扩展性、可解释性与灵活性方面的优势。我们提出通用框架及具体实现方案——一种快速高效的贪心连接感知检索算法，能够整体平衡相关性、覆盖度与可连接性。在5个NL2SQL基准测试中的实验表明，相较于基于混合整数规划的方法，我们的迭代检索方法在保持竞争力的同时，根据基准测试与搜索空间设置的不同，速度提升达4-400倍。这项研究揭示了迭代启发式方法在实现实用化、可扩展且具备组合感知能力的检索方面的潜力。"
    },
    {
        "title": "Attention Grounded Enhancement for Visual Document Retrieval",
        "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
        "entry_id": "http://arxiv.org/abs/2511.13415v1",
        "pub_date": "2025-11-17",
        "translated_summary": "视觉文档检索需理解异构多模态内容以满足信息需求。近期研究采用基于截图的文档编码与细粒度延迟交互机制，显著提升了检索性能。然而，检索模型仍使用粗粒度的全局相关性标签进行训练，未能揭示支持匹配的具体区域。这导致模型倾向于依赖表层线索，难以捕捉隐式语义关联，制约了处理非抽取式查询的能力。为缓解该问题，我们提出一种基于注意力定位的检索增强框架AGREE。该框架利用多模态大语言模型的跨模态注意力作为代理局部监督信号，引导模型识别相关文档区域。在训练过程中，AGREE将局部信号与全局信号结合以联合优化检索器，使其不仅能判断文档是否匹配，更能学习驱动相关性的具体内容。在具有挑战性的ViDoRe V2基准测试中，AGREE显著优于仅使用全局监督的基线模型。定量与定性分析进一步表明，AGREE促进了查询词与文档区域的深度对齐，实现了超越表层匹配的更精准、可解释的检索。代码已开源：https://anonymous.4open.science/r/AGREE-2025。"
    },
    {
        "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference",
        "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.",
        "entry_id": "http://arxiv.org/abs/2511.13389v1",
        "pub_date": "2025-11-17",
        "translated_summary": "提升工业铸造过程的能源效率是一项关键挑战，因为这类工序属于高能耗作业，且工艺变量间存在复杂的相互依存关系。基于相关性的分析方法往往难以区分真正的因果驱动因素与伪相关性，限制了其在决策中的应用。本文运用时间序列因果推断框架，识别感应炉熔炼过程中直接影响能源效率的操作因素。通过整合丹麦某铸造厂的生产数据，研究采用时间序列聚类将熔炼周期划分为不同操作模式，并运用前沿因果发现算法PCMCI+揭示各模式内的因果关系。跨集群分析表明：能耗、炉温与物料重量间的强因果关系构成了能效的核心驱动因素，而电压对冷却水温的影响则始终存在延迟响应。集群间差异进一步区分了操作机制——高效集群以稳定的因果结构为特征，低效集群则表现出强化反馈回路与非典型依赖关系。本研究的贡献具有双重意义：方法论层面提出了融合聚类与因果推断的分析流程，为能源密集型工艺研究提供了创新工具；实践层面则为铸造企业优化生产性能、降低能耗与排放提供了可操作的见解。"
    },
    {
        "title": "FLOWER: Flow-Oriented Entity-Relationship Tool",
        "summary": "Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.",
        "entry_id": "http://arxiv.org/abs/2511.13357v1",
        "pub_date": "2025-11-17",
        "translated_summary": "跨数据源的关系探索是实体识别优化的关键环节。由于数据库能够存储包含合成数据与自然数据的大体量信息，正确处理全部对象成为重要任务。然而实体关系模型的构建方式往往受到人为因素影响。本文提出面向流程的实体关系工具FLOWER，这是首个端到端解决方案，能够实时消除主流SQL方言在处理、创建及可视化显性与隐性依赖时的重复性资源消耗问题。启动后，FLOWER自动检测内置约束，通过动态采样与鲁棒数据分析技术构建正确且必要的约束体系。该方法可优化实体关系模型与数据叙事功能，帮助用户深入理解数据基础，通过SQL或自然语言从数据库源获取潜在洞察。在STATS前沿基准测试中，FLOWER在分布表征方面较水库采样提升2.4倍，约束学习效率提高2.6倍，并实现2.15倍加速。在数据叙事方面，相比大语言模型准确度提升1.19倍，上下文依赖减少1.86倍。该工具支持23种语言，兼容CPU与GPU架构。实验结果表明FLOWER能更高效处理现实数据，在不同应用场景中确保质量、可扩展性与适用性。"
    },
    {
        "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
        "summary": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
        "entry_id": "http://arxiv.org/abs/2511.13271v1",
        "pub_date": "2025-11-17",
        "translated_summary": "以ChatGPT为代表的生成式人工智能工具兴起，为计算机教育带来新机遇与挑战。现有研究多聚焦于其完成任务的能力及对学习成绩的影响，却常忽视其对知识获取的作用。本研究通过对照实验，比较了不同编程基础的学习者在使用生成式AI与传统网络资源时知识获取的差异。我们招募24名具有初级与中级编程经验的本科生，观察其在完成编程任务时与ChatGPT的互动过程，从任务表现、概念理解及交互行为三个维度进行分析。研究发现：借助生成式AI生成完整解决方案能显著提升任务完成度（尤其对初学者），但未必转化为知识增长；更重要的是，使用策略因经验水平而异——初学者往往过度依赖AI完成作业却未获得知识提升，中级学习者则更善于选择性利用。研究表明，无论过度依赖还是极少使用都会削弱知识获取效果。基于此，我们呼吁师生将生成式AI定位为学习工具而非解题手段，并强调在编程教育中亟需建立引导机制，以促进深度理解。"
    },
    {
        "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation",
        "summary": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.",
        "entry_id": "http://arxiv.org/abs/2511.13201v1",
        "pub_date": "2025-11-17",
        "translated_summary": "检索增强生成（RAG）通过引入外部知识来抑制大语言模型的幻觉现象，有效提升其响应质量与领域适应性。近期研究将图结构引入RAG以增强实体间语义关系捕获，但主要聚焦于低阶成对实体关系，难以建模多实体间的高阶关联。超图增强方法通过超边建模多实体交互突破此局限，但通常局限于语块内部的实体级表征，未能兼顾跨语块的全局主题组织与对齐机制。受人类思维自上而下认知过程的启发，我们提出主题对齐的双超图RAG框架（Cog-RAG），通过主题超图捕捉语块间主题结构，利用实体超图建模高阶语义关系。进一步设计认知启发的两阶段检索策略：先从主题超图激活查询相关主题内容，再引导实体超图进行细粒度召回与扩散，实现从全局主题到局部细节的语义对齐与连贯生成。大量实验表明，Cog-RAG显著优于现有主流基线方法。"
    },
    {
        "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
        "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
        "entry_id": "http://arxiv.org/abs/2511.13189v1",
        "pub_date": "2025-11-17",
        "translated_summary": "基础模型已在众多领域引发人工智能革命，然而其在极端多标签分类（XMC）领域的变革潜力仍待开发。XMC任务需从极大规模标签空间中为查询匹配相关标签，其核心挑战在于效率与性能的平衡。为此，近期研究多采用仅含编码器的小型Transformer架构学习嵌入向量，将XMC高效转化为最大内积搜索问题。本文聚焦XMC两大关键方向：如何有效利用仅含解码器的大型模型，以及如何在保持计算效率的同时整合视觉信息。我们证明这两个方向各自具有重要价值，且能协同提升性能。实验表明，数十亿参数的解码器可在可控计算开销下实现显著性能提升。进一步提出的视觉增强极端多标签学习框架（ViXML）通过单图像嵌入池化技术，在限制计算增长的同时成功融合基础视觉模型，解锁多模态能力。值得注意的是，采用小型编码器的ViXML在多数场景下优于纯文本解码器，印证“一图胜千言”的效能。此外，我们还将现有纯文本数据集扩展为包含视觉元数据的版本，为后续研究提供基准平台。在四个公开纯文本数据集及其图像增强版本上的综合实验验证了方案有效性，在最大数据集上P@1指标较之前最优成果提升达8.21%。ViXML代码已开源：https://github.com/DiegoOrtego/vixml。"
    },
    {
        "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
        "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
        "entry_id": "http://arxiv.org/abs/2511.13166v1",
        "pub_date": "2025-11-17",
        "translated_summary": "为更有效地利用互联网用户行为数据，本文提出一种新颖的协同过滤方法——局部协同过滤(LCF)。该方法通过挖掘用户间的局部相似性，运用大数定律整合用户数据，从而提升用户行为数据的利用率。在Steam游戏数据集上的实验表明，LCF方法的推荐效果符合现实需求。"
    },
    {
        "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning",
        "summary": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.",
        "entry_id": "http://arxiv.org/abs/2511.13125v1",
        "pub_date": "2025-11-17",
        "translated_summary": "近年来基于学习的方法虽然降低了传统轨迹相似性计算的时间复杂度，但现有最优方法仍未能充分利用轨迹信息的完整频谱进行相似性建模。为解决这一问题，我们提出\\textbf{RePo}方法，通过联合编码\\textbf{区域级}与\\textbf{点级}特征来同时捕捉空间上下文和细粒度移动模式。在区域级表征方面，首先将GPS轨迹映射为网格序列，通过结构特征捕捉空间上下文，并借助视觉特征增强语义上下文；在点级表征方面，三个轻量级专家网络分别从密集GPS序列中提取局部特征、关联特征和连续移动模式。随后通过路由网络自适应融合点级特征，再与区域级特征进行交叉注意力融合生成最终轨迹嵌入。我们采用包含困难负样本的对比损失函数进行模型训练，以提供相似度排序监督。实验结果表明，在所有评估指标上，RePo相较现有最优基线模型平均准确率提升22.2\\%。"
    },
    {
        "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation",
        "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.",
        "entry_id": "http://arxiv.org/abs/2511.13063v1",
        "pub_date": "2025-11-17",
        "translated_summary": "电子显微镜图像中神经结构的精确分割对神经科学研究至关重要。然而，该任务面临形态结构复杂、信噪比低及标注稀缺等挑战，限制了现有方法的准确性与泛化能力。为解决这些问题，我们尝试利用视觉基础模型在自然图像上学习到的先验知识来改进分割性能。具体而言，我们提出了一种创新框架，能够将基于自然图像预训练的SAM2模型的知识有效迁移至电子显微镜领域。该框架首先通过SAM2提取强泛化性的通用特征；为弥合领域差异，我们设计了特征引导注意力模块，利用SAM2的语义线索引导轻量级精细编码器聚焦于困难区域；最后通过双亲和度解码器同步生成粗粒度与精细化亲和力图。实验结果表明，在冻结SAM2权重的情况下，我们的方法已达到与现有最优方法相当的精度；当在电子显微镜数据上进行微调后，其性能显著超越当前最优方法。本研究证实：结合针对性领域自适应引导，迁移自然图像预训练表征能有效解决神经元分割中的特定挑战。"
    },
    {
        "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
        "summary": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
        "entry_id": "http://arxiv.org/abs/2511.13057v1",
        "pub_date": "2025-11-17",
        "translated_summary": "稠密检索模型已成为信息检索领域的最先进标准。然而其高维度、高精度（float32）的向量嵌入在实际部署中带来了显著的存储与内存挑战。为应对此问题，我们在BEIR SciFact基准上开展严格实证研究，评估两种主要压缩策略的平衡关系：（1）通过深度自编码器实现维度压缩，将原始384维向量降至12维潜在空间；（2）通过量化实现精度压缩（float16、int8与二值化）。我们通过全套检索指标（NDCG、MAP、MRR、召回率、精确度）在不同k值截断点上相对float32基线的“性能损失（或增益）”，对每种方法进行系统比较。实验结果表明：int8标量量化能提供最佳平衡点，在实现4倍压缩的同时，nDCG@10指标仅出现可忽略的[约1-2%]下降；而自编码器虽呈现平缓的性能衰减，但在同等4倍压缩比（AE-96）下会出现更显著的性能损失；二值化量化由于会导致性能急剧下降，被证明不适用于此任务。本研究为部署高效能检索系统提供了实用指南。"
    },
    {
        "title": "Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning",
        "summary": "Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.",
        "entry_id": "http://arxiv.org/abs/2511.13041v1",
        "pub_date": "2025-11-17",
        "translated_summary": "协同过滤技术在现代推荐系统中发挥着关键作用，它通过分析用户与项目的交互历史来实现个性化推荐。然而基于协同过滤的方法常因训练数据不平衡而产生偏差，这种倾向会导致系统优先推荐热门项目，而对非活跃用户的推荐效果欠佳。现有研究通过重平衡训练样本、重排推荐结果或建立对偏差具有鲁棒性的模型来解决这一问题。这些方法虽有效，但可能影响推荐准确性，或对权重策略过于敏感，导致模型训练困难。本文深入分析了推荐偏差的成因与影响，从表示分布视角提出去偏框架AURL，通过增强表征学习中的群组对齐与全局一致性来实现推荐去偏。具体而言，我们发现了用户与项目表示分布中存在的两个核心问题：群组差异与全局坍缩，这两者直接导致推荐结果产生偏差。为此，我们在表示空间中设计了两个简洁有效的正则化器：群组对齐器致力于拉近长尾实体与热门实体的表示分布，全局一致性器则通过均匀分布表示来最大限度保留实体信息。我们的方法通过同步优化群组对齐与全局一致性正则项来缓解推荐偏差。在三个真实数据集和多种推荐骨干网络上进行的广泛实验，验证了所提框架的优越性。"
    },
    {
        "title": "Personalized Federated Recommendation With Knowledge Guidance",
        "summary": "Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.",
        "entry_id": "http://arxiv.org/abs/2511.12959v1",
        "pub_date": "2025-11-17",
        "translated_summary": "联邦推荐系统已成为构建隐私保护推荐系统的关键范式。然而现有联邦推荐模型面临核心困境：内存高效的单一知识模型受限于次优的知识替换机制，导致有价值的个性化信息被丢弃；而高性能的双知识模型因内存占用过高难以在实际设备上部署。我们提出知识引导的联邦推荐框架FedRKG，这一模型无关的解决方案通过\"知识引导\"核心原则，避免完全替换而将全局知识融合到保留的本地嵌入中，在单一知识内存占用量下实现双知识模型的个性化优势。进一步提出自适应引导机制，通过细粒度动态调节每个用户-项目交互的引导强度，克服静态融合方法的局限性。在基准数据集上的大量实验表明，FedRKG显著优于现有最优方法，验证了本方法的有效性。代码已开源：https://github.com/Jaehyung-Lim/fedrkg。"
    },
    {
        "title": "Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior",
        "summary": "In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.\n  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.",
        "entry_id": "http://arxiv.org/abs/2511.12949v1",
        "pub_date": "2025-11-17",
        "translated_summary": "近年来，大语言模型在语言理解与生成任务中表现出色，推动了智能对话和推荐系统的发展。然而这些系统仍存在显著局限：往往以静态方式建模用户偏好，难以捕捉交互行为中动态连续的潜在规律。用户历史提问序列隐含着兴趣演变与认知模式的丰富信号，但由于语言建模与行为序列建模之间存在固有割裂，如何利用这类时序数据进行预测仍具挑战。\n\n为突破这一局限，我们提出协同过滤增强的提问预测框架。该框架通过融合个性化记忆模块与基于图的偏好传播机制，动态建模用户-问题交互的演变过程。这种双重机制使系统既能自适应学习用户特定历史，又能借助相似用户的协同信号优化预测。实验结果表明，我们的方法可有效生成模拟真实用户提问模式的智能体，展现了构建前瞻自适应对话系统的潜力。"
    },
    {
        "title": "A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation",
        "summary": "Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.",
        "entry_id": "http://arxiv.org/abs/2511.12947v1",
        "pub_date": "2025-11-17",
        "translated_summary": "本地生活推荐服务近年来快速发展，为用户获取日常所需提供了便捷途径。然而该领域面临两大核心挑战：（1）空间约束性——受本地生活场景特性影响，商品通常仅向有限地理范围内的用户展示，间接降低了商品曝光概率；（2）长尾稀疏性——少数热门商品占据大部分用户交互记录，而大量优质长尾商品因交互机会失衡被严重忽视。现有方法多采用用户中心视角，例如建模空间用户偏好或利用协同过滤信号增强长尾表征。但我们认为，基于商品中心的视角更能契合该领域特性，应聚焦于增强符合本地生活服务空间约束特征的长尾商品表征。为此，我们提出ReST框架——一种即插即用的空间约束表征增强框架。具体而言，我们首先设计元ID预热网络，通过注入基础属性层级语义信息来初始化ID表征；随后提出基于对比学习的空间约束ID表征增强网络（SIDENet），该网络融合两项高效策略：空间约束硬采样策略与动态表征对齐策略。该设计能在训练过程中基于属性信息自适应识别弱表征ID，通过捕捉本地生活服务空间约束特性下的潜在商品关联来增强其表征，同时保持与热门商品的兼容性。"
    },
    {
        "title": "AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking",
        "summary": "In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.",
        "entry_id": "http://arxiv.org/abs/2511.12934v1",
        "pub_date": "2025-11-17",
        "translated_summary": "在工业推荐系统中，基于深度神经网络（DNN）的粗排模型通常采用顺序执行框架：只有在接收到上游召回阶段的候选集后，才会触发特征获取和模型前向计算。这种设计存在固有瓶颈，包括对相同用户/项目的重复计算，以及严格顺序操作带来的延迟增加，这些因素共同制约了模型容量与系统效率。为突破这些限制，我们提出异步推理框架（AIF），这是一种高效经济的计算架构，其核心在于将交互无关组件（即仅涉及单一用户或项目内部运算的模块）从实时预测中解耦。AIF通过以下方式重构模型推理流程：用户侧计算与召回阶段并行执行，项目侧计算则采用近线处理模式。这意味着交互无关组件的计算仅需执行一次，并在粗排阶段实时预测开始前完成。该架构显著提升了计算效率并降低延迟，释放的资源使得交互无关组件的特征集与模型架构得以显著优化。此外，我们深入探索了AIF框架内的模型设计，对线上实时预测中的交互相关组件采用近似计算方法。通过框架与模型的协同设计，我们的解决方案在不显著增加计算与延迟成本的前提下实现了显著性能提升，目前已在淘宝展示广告系统成功部署。"
    },
    {
        "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation",
        "summary": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.",
        "entry_id": "http://arxiv.org/abs/2511.12922v1",
        "pub_date": "2025-11-17",
        "translated_summary": "基于大语言模型的推荐系统通过项目标记化技术弥合项目空间与语言空间之间的差异，实现了高质量的推荐性能。然而，现有的项目标记化方法通常需要为每个项目领域单独训练模型，限制了泛化能力。同时，不同项目领域间分布与语义的差异性使得构建能保留领域特异性信息的统一标记化面临挑战。为此，我们提出UniTok——一个统一的项目标记化框架，通过将混合专家架构与系列码本相结合，将项目转化为离散标记，在实现可扩展标记化的同时保留跨领域语义信息。具体而言，不同领域的项目首先通过共享编码器投射到统一潜在空间，随后被路由至领域特定专家以捕获独特语义，而始终保持激活状态的共享专家则负责编码可跨领域迁移的通用知识。此外，为缓解领域间语义不均衡问题，我们提出互信息校准机制，引导模型为各领域保留相近层级的语义信息。基于多领域真实数据集的综合实验表明，UniTok框架具有以下优势：（a）高效性：在强基准测试中实现最高51.89%的性能提升；（b）理论严谨性：架构设计与优化的分析有效性得到验证；（c）强泛化性：无需针对每个领域重新训练即可在多样化领域保持稳健性能，这是现有基线方法所不具备的能力。"
    },
    {
        "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy",
        "summary": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.",
        "entry_id": "http://arxiv.org/abs/2511.12920v1",
        "pub_date": "2025-11-17",
        "translated_summary": "谷歌搜索正通过\"AI概览\"和\"精选摘要\"等功能呈现越来越多AI生成内容，用户虽无法控制其呈现形式却日益依赖。本研究通过对1,508个真实育儿及孕期相关查询进行系统算法审计，评估了这些信息展示的质量与一致性。我们构建的稳健评估框架涵盖多维度质量指标：答案一致性、相关性、医疗安全措施、信息来源类型及情感倾向。研究结果显示，同一搜索结果页面上AI概览与精选摘要的信息不一致率高达33%。尽管相关性评分普遍较高，但两者均严重缺乏医疗安全提示（AI概览仅11%含警示，精选摘要仅7%）。在信息来源方面，健康类网站虽占主导，但精选摘要还频繁链接商业来源。这些发现对公共卫生信息获取具有重要意义，表明AI赋能的健康信息亟需更严格的质量管控。本研究方法论为审计高风险领域AI系统提供了可迁移框架，这些领域的信息质量直接关系用户福祉。"
    },
    {
        "title": "NeuCLIRBench: A Modern Evaluation Collection for Monolingual, Cross-Language, and Multilingual Information Retrieval",
        "summary": "To measure advances in retrieval, test collections with relevance judgments that can faithfully distinguish systems are required. This paper presents NeuCLIRBench, an evaluation collection for cross-language and multilingual retrieval. The collection consists of documents written natively in Chinese, Persian, and Russian, as well as those same documents machine translated into English. The collection supports several retrieval scenarios including: monolingual retrieval in English, Chinese, Persian, or Russian; cross-language retrieval with English as the query language and one of the other three languages as the document language; and multilingual retrieval, again with English as the query language and relevant documents in all three languages. NeuCLIRBench combines the TREC NeuCLIR track topics of 2022, 2023, and 2024. The 250,128 judgments across approximately 150 queries for the monolingual and cross-language tasks and 100 queries for multilingual retrieval provide strong statistical discriminatory power to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included with the collection so that developers of reranking algorithms are no longer reliant on BM25 as their first-stage retriever. NeuCLIRBench is publicly available.",
        "entry_id": "http://arxiv.org/abs/2511.14758v1",
        "pub_date": "2025-11-18",
        "translated_summary": "为有效衡量检索技术的进展，需要具备能够准确区分系统性能的相关性评估测试集。本文推出NeuCLIRBench——一个面向跨语言与多语言检索的评估数据集。该数据集包含中文、波斯语和俄语的原始文档，以及这些文档经机器翻译后的英文版本，支持多种检索场景：英语、中文、波斯语或俄语的单语检索；以英语为查询语言、其他三种语言之一为文档语言的跨语言检索；以及以英语为查询语言、三种语言文档均需检索的多语言检索。该数据集整合了TREC NeuCLIR赛道2022至2024年的全部主题，包含单语及跨语言任务约150个查询的250,128条评估结果，以及多语言检索100个查询的评估数据，具备强大的统计区分能力以甄别不同检索方法。数据集还提供了强神经检索系统的融合基线，使重排序算法开发者无需再依赖BM25作为首阶段检索器。NeuCLIRBench已面向公众开放。"
    },
    {
        "title": "LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation",
        "summary": "With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.",
        "entry_id": "http://arxiv.org/abs/2511.14531v1",
        "pub_date": "2025-11-18",
        "translated_summary": "随着检索增强生成(RAG)技术在生成式AI解决方案中的地位日益凸显，系统化评估其效能的需求也愈发迫切。我们推出LiveRAG基准测试——一个包含895组合成问答对的公开数据集，专为支持基于RAG的问答系统进行系统性评估而设计。该合成基准源自SIGIR'2025 LiveRAG挑战赛的竞赛题库，所有参赛者曾在严格时间限制下接受该题库的测试。当前版本新增了挑战赛期间未公开的关键信息，包括用于评估参赛答案的基准答案及其支撑依据。此外，通过将项目反应理论模型应用于参赛者答题数据，我们为每个问题标注了预估难度系数与区分度指数。分析表明，该基准测试兼具问题多样性、难度广谱性以及系统能力区分度三大特性。期待LiveRAG基准能助力学界推进RAG研究、实施系统化评估，并开发出更稳健的问答系统。"
    },
    {
        "title": "Effective Diversification of Multi-Carousel Book Recommendation",
        "summary": "Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.",
        "entry_id": "http://arxiv.org/abs/2511.14461v1",
        "pub_date": "2025-11-18",
        "translated_summary": "在当代大多数电影流媒体平台中，采用可循环滚动的多轮播列表已成为内容呈现的基本模式。这种设计通过类型、作者等分类维度，有效凸显用户兴趣的不同面向。尽管轮播界面能优化信息结构与导航效率，但单纯依靠这种形式并不能提升推荐内容的多样性——而多样性恰恰是维持用户参与度的关键要素。本研究基于协同过滤算法，提出多种有效提升图书推荐多样性的解决方案，旨在优化公共图书馆线上目录的荐书体系。我们同时建立了评估指标体系，通过实验证明该推荐系统能够在准确性与超准确性指标之间实现良好平衡。"
    },
    {
        "title": "Jasper-Token-Compression-600M Technical Report",
        "summary": "This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.",
        "entry_id": "http://arxiv.org/abs/2511.14405v1",
        "pub_date": "2025-11-18",
        "translated_summary": "本技术报告介绍了2025年11月发布的开源Jasper-Token-Compression-600M模型的训练方法与评估结果。基于先前英文Stella和Jasper模型的蒸馏方案，我们成功将该方法扩展至中英双语领域，并通过引入对比学习进一步提升了模型性能。本模型的核心创新在于引入基于一维卷积的令牌压缩模块，通过动态调整训练过程中的压缩率，使模型能够学习更鲁棒、更高效的压缩文本表示。通过将知识蒸馏与令牌压缩技术相结合，我们在嵌入质量和推理效率两方面均实现了显著提升。该模型在达到与80亿参数模型相当性能的同时，运行效率显著优于传统的6亿参数模型。更多模型发布信息请访问：https://huggingface.co/infgrad/Jasper-Token-Compression-600M。"
    },
    {
        "title": "Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction",
        "summary": "Generative models are increasingly being explored in click-through rate (CTR) prediction field to overcome the limitations of the conventional discriminative paradigm, which rely on a simple binary classification objective. However, existing generative models typically confine the generative paradigm to the training phase, primarily for representation learning. During online inference, they revert to a standard discriminative paradigm, failing to leverage their powerful generative capabilities to further improve prediction accuracy. This fundamental asymmetry between the training and inference phases prevents the generative paradigm from realizing its full potential. To address this limitation, we propose the Symmetric Masked Generative Paradigm for CTR prediction (SGCTR), a novel framework that establishes symmetry between the training and inference phases. Specifically, after acquiring generative capabilities by learning feature dependencies during training, SGCTR applies the generative capabilities during online inference to iteratively redefine the features of input samples, which mitigates the impact of noisy features and enhances prediction accuracy. Extensive experiments validate the superiority of SGCTR, demonstrating that applying the generative paradigm symmetrically across both training and inference significantly unlocks its power in CTR prediction.",
        "entry_id": "http://arxiv.org/abs/2511.14403v1",
        "pub_date": "2025-11-18",
        "translated_summary": "生成式模型在点击率预估领域日益受到关注，旨在突破传统判别式范式仅依赖简单二元分类目标的局限。然而现有生成模型通常将生成范式局限于训练阶段，主要用于表征学习。在线推理时它们仍回归标准判别式范式，未能利用强大的生成能力进一步提升预测精度。这种训练与推理阶段的根本性不对称阻碍了生成范式发挥全部潜力。为解决这一局限，我们提出对称掩码生成式点击率预估框架，构建训练与推理阶段的对称性。具体而言，在训练阶段通过学习特征依赖获得生成能力后，该框架在在线推理阶段运用这种生成能力迭代重构输入样本特征，从而有效缓解噪声特征影响并提升预测准确性。大量实验验证了该框架的优越性，证明在训练和推理阶段对称应用生成范式能显著释放其在点击率预估中的潜力。"
    },
    {
        "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models",
        "summary": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.",
        "entry_id": "http://arxiv.org/abs/2511.14256v1",
        "pub_date": "2025-11-18",
        "translated_summary": "知识图谱推理（KGR）是通过对知识图谱进行逻辑推演以推断新知识的任务。近年来，大语言模型（LLM）在复杂推理任务中展现出卓越性能。尽管取得显著进展，当前基于LLM的KGR方法仍面临两个关键局限：其一，现有方法往往不加区分地提取推理路径，未能评估路径的重要性差异，可能引入无关噪声误导LLM；其二，多数方法依赖LLM动态探索潜在推理路径，但需要高频次检索和大量LLM调用。为应对这些挑战，我们提出PathMind框架，通过选择性引导LLM关注重要推理路径，提升推理的可靠性与可解释性。该框架采用“检索-优先级排序-推理”范式：首先通过检索模块从知识图谱中获取查询子图；随后引入路径优先级机制，通过语义感知的路径优先级函数识别重要推理路径，该函数同步考量路径累积代价与抵达目标的预估未来代价；最后通过双阶段训练策略（包括任务特定指令微调与路径偏好对齐）生成精准且逻辑一致的答案。在基准数据集上的大量实验表明，PathMind通过识别关键推理路径，在输入标记更少的复杂推理任务中持续优于现有基线模型。"
    },
    {
        "title": "LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation",
        "summary": "Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLM's geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.",
        "entry_id": "http://arxiv.org/abs/2511.14221v1",
        "pub_date": "2025-11-18",
        "translated_summary": "大型语言模型的最新进展通过增强语义泛化能力，改进了基于文本的推荐系统，突破了传统基于ID方法的局限。现有文本方法通常通过提示设计编码物品文本信息，并借助物品标记化生成离散语义ID。然而在本地生活服务等垂直领域任务中，仅将位置信息注入提示模板难以捕捉细粒度空间特征及物品间真实距离感知。为此，我们提出LGSID——面向本地生活推荐的LLM对齐地理标记化框架，该框架包含两大核心组件：（1）基于强化学习的地理LLM对齐；（2）分层级地理物品标记化。在强化学习对齐模块中，我们首先训练列表式奖励模型以捕捉物品间真实空间关系，继而提出创新性G-DPO算法，利用预训练奖励模型向LLM注入泛化空间知识与协同信号，同时保持其语义理解能力。进一步提出分层级地理物品标记化策略：主标记源自离散化空间与内容属性，残差标记则通过对齐后LLM的地理表征向量进行精细化生成。在快手真实工业数据集上的大量实验表明，LGSID在各项指标上持续优于当前最先进的判别式与生成式推荐模型。消融实验、可视化分析与案例研究进一步验证了该框架的有效性。"
    },
    {
        "title": "WebRec: Enhancing LLM-based Recommendations with Attention-guided RAG from Web",
        "summary": "Recommender systems play a vital role in alleviating information overload and enriching users' online experience. In the era of large language models (LLMs), LLM-based recommender systems have emerged as a prevalent paradigm for advancing personalized recommendations. Recently, retrieval-augmented generation (RAG) has drawn growing interest to facilitate the recommendation capability of LLMs, incorporating useful information retrieved from external knowledge bases. However, as a rich source of up-to-date information, the web remains under-explored by existing RAG-based recommendations. In particular, unique challenges are posed from two perspectives: one is to generate effective queries for web retrieval, considering the inherent knowledge gap between web search and recommendations; another challenge lies in harnessing online websites that contain substantial noisy content. To tackle these limitations, we propose WebRec, a novel web-based RAG framework, which takes advantage of the reasoning capability of LLMs to interpret recommendation tasks into queries of user preferences that cater to web retrieval. Moreover, given noisy web-retrieved information, where relevant pieces of evidence are scattered far apart, an insightful MP-Head is designed to enhance LLM attentions between distant tokens of relevant information via message passing. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed web-based RAG methods in recommendation scenarios.",
        "entry_id": "http://arxiv.org/abs/2511.14182v1",
        "pub_date": "2025-11-18",
        "translated_summary": "推荐系统在缓解信息过载、丰富用户在线体验方面发挥着关键作用。在大语言模型时代，基于大语言模型的推荐系统已成为推进个性化推荐的主流范式。近年来，检索增强生成技术通过整合外部知识库中的有效信息，日益受到学界关注，以增强大语言模型的推荐能力。然而，作为实时信息的丰富来源，网络资源在当前基于检索增强生成的推荐系统中尚未得到充分探索。具体而言存在两大挑战：一是考虑到网络搜索与推荐任务之间固有的知识鸿沟，如何生成适用于网络检索的有效查询；二是如何有效利用包含大量噪声内容的在线网站资源。为突破这些局限，我们提出WebRec——一个创新的基于网络的检索增强生成框架，该框架利用大语言模型的推理能力，将推荐任务解析为符合网络检索需求的用户偏好查询。此外，针对网络检索信息中相关证据分散存在的噪声问题，我们设计了具有洞察力的MP-Head模块，通过消息传递机制增强大语言模型对分散相关信息的注意力聚焦。大量实验证明，我们提出的基于网络的检索增强生成方法在推荐场景中具有显著有效性。"
    },
    {
        "title": "Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions",
        "summary": "In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the \"fill-in-the-blank\" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.",
        "entry_id": "http://arxiv.org/abs/2511.14144v1",
        "pub_date": "2025-11-18",
        "translated_summary": "本研究将基于Transformer的关系抽取与知识图谱匹配相结合，应用于填空题形式的多项选择题解答，同时保持输出过程的可追溯性。知识图谱是由实体和关系构成的结构化事实知识表示。由于构建成本高昂，传统上被视为具有已验证链接的静态数据库。但近年来基于Transformer的关系抽取技术发展使我们能够通过输入自然文本来动态生成知识图谱，从而开创了用生成图谱表征输入语句语义的新可能。基于此，我们提出一种针对填空题的解答方法，重点关注当输入事实错误的文本时，关系抽取方法会生成包含虚假信息知识图谱的特性。我们通过以下方式评估问题陈述的真实性：(i)使用关系抽取方法将语句转换为关系图谱，(ii)在封闭世界假设下对照事实正确的知识图谱进行验证。实验结果表明，本方法在保持过程可追溯性的同时，能对约70%的问题给出正确答案。我们还发现问题类别对准确率存在显著影响。"
    },
    {
        "title": "PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval",
        "summary": "With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.",
        "entry_id": "http://arxiv.org/abs/2511.14130v1",
        "pub_date": "2025-11-18",
        "translated_summary": "随着大语言模型的快速发展，金融信息检索已成为关键的工业应用场景。从冗长的财务报告中提取任务相关信息对运营决策与分析决策都至关重要。FinAgentBench数据集通过文档排序和文本块排序两项任务将这一问题规范化。我们提出PRISM这一免训练框架，融合了精细化系统提示、上下文学习与轻量级多代理系统。通过深入剖析各组件间的协同机制：提示工程提供精准任务指令，上下文学习注入语义相关的少样本示例，多代理系统则模拟协同评分行为。我们的最优配置在受限验证集上取得了0.71818的NDCG@5评分。进一步实验表明，PRISM在生产级金融检索场景中兼具可行性与鲁棒性，其模块化、纯推理的设计特性更契合实际应用需求。源代码已发布于https://bit.ly/prism-ailens。"
    },
    {
        "title": "NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval",
        "summary": "Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.",
        "entry_id": "http://arxiv.org/abs/2511.14096v1",
        "pub_date": "2025-11-18",
        "translated_summary": "检索增强生成（RAG）技术显著提升了大型语言模型在知识密集型任务中的表现。然而，传统RAG方法在应对多跳问答任务时，由于难以捕捉文档间的复杂依赖关系而存在局限。近期研究尝试通过基于图结构的RAG方法建立文档关联，但这类方法在节点匹配和子图构建过程中易导致语义连贯性缺失并引入无关噪声。为此，我们受神经生物学中位置细胞路径导航机制的启发，提出NeuroPath——一种基于大语言模型的语义路径追踪RAG框架。该框架包含动态路径追踪与检索后补全两个核心阶段：动态路径追踪通过在构建的知识图谱上进行目标导向的语义路径追踪与剪枝，有效提升噪声抑制与语义连贯性；检索后补全则通过结合中间推理过程与原始查询进行二次检索，优化查询目标并补全推理路径中的缺失信息。在三个多跳问答数据集上的实验表明，NeuroPath相较当前最先进的图结构RAG方法，在召回率@2和@5指标上分别实现16.3%和13.5%的平均提升；与迭代式RAG方法相比，在获得更高准确率的同时降低22.8%的token消耗。此外，我们在四款轻量化大模型（Llama3.1、GLM4、Mistral0.3和Gemma3）上验证了NeuroPath的鲁棒性，并进一步证明了其在处理不同复杂度任务时的可扩展性。项目代码已开源：https://github.com/KennyCaty/NeuroPath。"
    },
    {
        "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees",
        "summary": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.",
        "entry_id": "http://arxiv.org/abs/2511.13942v1",
        "pub_date": "2025-11-17",
        "translated_summary": "基于规则的系统必须在严格的时间限制内解决复杂的匹配问题，才能在实时应用中保持高效，例如AI代理的规划与反应控制，以及低延迟关系数据库查询。模式匹配系统可能遇到这样的问题：当规则包含大量欠约束变量，或产生组合爆炸的中间部分匹配时（尽管其他方面约束良好），寻找匹配需要指数级的时间和空间。在线AI系统通过示例驱动归纳或代码合成自动生成规则时，很容易产生最坏情况下的匹配模式，导致程序因超出可用内存而减速或停止。在我们基于示例学习的认知系统研究中发现，采用激进的反泛化泛化方法极易引发此类情况。为使这些系统无需人工设计约束即可投入实用，同时避免不可预测的故障模式，我们提出名为CORGI（面向集合的关系图迭代）的新型匹配算法。与基于RETE的方法不同，CORGI在寻找单个满意匹配时具备二次时间/空间保证，并能通过迭代流式输出后续匹配，而无需将完整冲突集载入内存。CORGI的独特之处在于摒弃了传统用于收集部分匹配的$β$存储器，转而采用两步法：前向传递中构建/维护接地关系图，后向迭代器按需遍历该图生成匹配。这种方法消除了因填充完整冲突集导致的高延迟和内存溢出问题。性能评估显示，在简单组合匹配任务中，CORGI显著优于SOAR和OPS5的RETE实现。"
    },
    {
        "title": "TaoSearchEmb: A Multi-Objective Reinforcement Learning Framework for Dense Retrieval in Taobao Search",
        "summary": "Dense retrieval, as the core component of e-commerce search engines, maps user queries and items into a unified semantic space through pre-trained embedding models to enable large-scale real-time semantic retrieval. Despite the rapid advancement of LLMs gradually replacing traditional BERT architectures for embedding, their training paradigms still adhere to BERT-like supervised fine-tuning and hard negative mining strategies. This approach relies on complex offline hard negative sample construction pipelines, which constrain model iteration efficiency and hinder the evolutionary potential of semantic representation capabilities. Besides, existing multi-task learning frameworks face the seesaw effect when simultaneously optimizing semantic relevance and non-relevance objectives. In this paper, we propose Retrieval-GRPO, a multi-objective reinforcement learning-based dense retrieval framework designed to address these challenges. The method eliminates offline hard negative sample construction by dynamically retrieving Top-K candidate products for each query during training, while introducing a relevance LLM as a reward model to generate real-time feedback. Specifically, the retrieval model dynamically optimizes embedding representations through reinforcement learning, with reward signals combining LLM-generated relevance scores, product quality scores, and multi-way exclusivity metrics to achieve multi-objective user preference alignment and real-time error correction. This mechanism not only removes dependency on hard negatives but also mitigates the seesaw effect through collaborative multi-objective optimization, significantly enhancing the model's semantic generalization capability for complex long-tail queries. Extensive offline and online experiments validate the effectiveness of Retrieval-GRPO, which has been deployed on China's largest e-commerce platform.",
        "entry_id": "http://arxiv.org/abs/2511.13885v1",
        "pub_date": "2025-11-17",
        "translated_summary": "稠密检索作为电商搜索引擎的核心组件，通过预训练嵌入模型将用户查询与商品映射至统一语义空间，以实现大规模实时语义检索。尽管大语言模型的快速发展正逐步替代传统BERT架构进行嵌入，但其训练范式仍遵循类BERT的监督微调与难负例挖掘策略。该方法依赖复杂的离线难负例样本构建流程，制约模型迭代效率并阻碍语义表征能力的进化潜力。此外，现有多任务学习框架在同时优化语义相关性与非相关性目标时存在“跷跷板效应”。本文提出Retrieval-GRPO——基于多目标强化学习的稠密检索框架以应对上述挑战。该方法通过训练期间动态检索每个查询的Top-K候选商品，消除离线难负例构建环节，同时引入相关性大语言模型作为奖励模型生成实时反馈。具体而言，检索模型通过强化学习动态优化嵌入表征，其奖励信号融合了LLM生成的相关性分数、商品质量分及多路排他性指标，实现多目标用户偏好对齐与实时纠偏。该机制不仅消除对难负例的依赖，更通过多目标协同优化缓解跷跷板效应，显著增强模型对复杂长尾查询的语义泛化能力。大量离线与在线实验验证了Retrieval-GRPO的有效性，该方案已在中国头部电商平台完成部署。"
    },
    {
        "title": "CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search",
        "summary": "Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.",
        "entry_id": "http://arxiv.org/abs/2511.15443v1",
        "pub_date": "2025-11-19",
        "translated_summary": "稠密检索已成为现代搜索系统的基础范式，尤其在短视频平台中应用广泛。然而大多数工业系统采用自增强训练流程，依赖历史曝光用户交互作为监督信号。这种模式不可避免地导致信息茧房效应——潜在相关但未被用户接触的内容被排除在训练信号之外，使模型偏向于狭窄保守的检索。本文提出CroPS（跨视角正样本），一种通过多视角引入多样化语义正例的新型检索数据引擎。CroPS从用户查询重构行为（查询层面）、推荐流交互数据（系统层面）以及大语言模型合成的世界知识（知识层面）三个维度增强正信号训练。为有效利用这些异构信号，我们提出分层标签分配策略及其对应的H-InfoNCE损失函数，共同实现细粒度、相关性感知的优化。在大型商业短视频搜索平台快手搜索上的大量实验表明，CroPS在离线评估和在线A/B测试中均显著超越强基线模型，既提升了检索性能又降低了用户查询重构率。目前CroPS已在快手搜索全量部署，每日服务数亿用户。"
    },
    {
        "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation",
        "summary": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",
        "entry_id": "http://arxiv.org/abs/2511.15435v1",
        "pub_date": "2025-11-19",
        "translated_summary": "先进的多模态检索增强生成技术已被广泛应用于增强大型多模态模型的能力，但同时也带来了新的安全隐患。现有对抗性研究揭示了MRAG系统易受知识投毒攻击的脆弱性，此类攻击会诱使检索器召回被植入的污染内容。然而，本研究探索了一种全新攻击范式：仅通过在用户输入的图像中添加人眼难以察觉的扰动来实现对MRAG的视觉攻击，无需操控其他组件。由于微调后的检索器与大规模生成器具有较强鲁棒性，且视觉扰动在RAG链式传播中会持续衰减，该攻击极具挑战性。我们提出了一种创新的分层视觉攻击方法，通过错位干扰MRAG生成器的两个输入源（多模态查询与增强知识）来扰乱其生成过程。进一步设计了分层双阶段策略来获取错位的增强知识：首先破坏跨模态对齐，进而干扰多模态语义对齐，通过优化扰动使检索器从原始数据库中召回无关知识。我们在OK-VQA和InfoSeek两个主流MRAG数据集上进行了广泛实验，采用CLIP系列检索器及BLIP-2、LLaVA两种大型多模态模型作为生成器。实验结果表明，我们的视觉攻击能显著降低MRAG系统的检索与生成性能，验证了该攻击方法的有效性。"
    },
    {
        "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
        "summary": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
        "entry_id": "http://arxiv.org/abs/2511.15408v1",
        "pub_date": "2025-11-19",
        "translated_summary": "在多样化人工文本训练下，大语言模型（LLMs）释放了创意自然语言生成（CNLG）的潜力，为广告、故事创作等应用场景带来价值。然而CNLG仍面临两大核心挑战：（1）多目标灵活性：用户需求往往具有个性化、细粒度、多元化特征，LLMs难以同时满足；（2）阐释复杂性：创意不仅关乎生成，更需理解与诠释深层含义以提升用户感知。这些挑战严重制约现有方法在短文本生成中产出兼具创意与深度的内容。为此，我们聚焦中文起名这一典型短文本CNLG任务——需满足用户对长度、语义、命名学等显性约束，同时提供具有美学价值的阐释。提出NAMeGEn创新多智能体优化框架，通过目标提取、姓名生成、评估验证三阶段迭代循环，兼顾多元化需求并生成精准阐释。为此任务构建含1.7万首古诗的增强美学诗歌语料库，推出配备定制化评估指标的CBNames基准测试。大量实验表明，NAMeGEn无需训练即可在多种LLM基座上超越六类基线方法，有效生成符合个性化需求的创意姓名并产出有意义阐释。"
    },
    {
        "title": "Unveiling Inference Scaling for Difference-Aware User Modeling in LLM Personalization",
        "summary": "Large Language Models (LLMs) are increasingly integrated into users' daily lives, driving a growing demand for personalized outputs. Prior work has primarily leveraged a user's own history, often overlooking inter-user differences that are critical for effective personalization. While recent methods have attempted to model such differences, their feature extraction processes typically rely on fixed dimensions and quick, intuitive inference (System-1 thinking), limiting both the coverage and granularity of captured user differences. To address these limitations, we propose Difference-aware Reasoning Personalization (DRP), a framework that reconstructs the difference extraction mechanism by leveraging inference scaling to enhance LLM personalization. DRP autonomously identifies relevant difference feature dimensions and generates structured definitions and descriptions, enabling slow, deliberate reasoning (System-2 thinking) over user differences. Experiments on personalized review generation demonstrate that DRP consistently outperforms baseline methods across multiple metrics.",
        "entry_id": "http://arxiv.org/abs/2511.15389v1",
        "pub_date": "2025-11-19",
        "translated_summary": "大型语言模型正日益融入用户的日常生活，推动着对个性化输出的需求增长。现有研究主要利用用户自身的历史数据，往往忽视了对于实现有效个性化至关重要的用户间差异。虽然近期方法尝试对此类差异进行建模，但其特征提取过程通常依赖固定维度和快速直观推断（系统1思维），限制了所捕获用户差异的覆盖范围与精细度。为突破这些局限，我们提出差异感知推理个性化框架，该框架通过利用推理扩展重构差异提取机制，以增强大型语言模型的个性化能力。该框架能自主识别相关的差异特征维度，生成结构化定义与描述，从而实现对用户差异的慢速深度推理（系统2思维）。在个性化评论生成任务上的实验表明，该框架在多项指标上持续优于基线方法。"
    },
    {
        "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
        "summary": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
        "entry_id": "http://arxiv.org/abs/2511.15383v1",
        "pub_date": "2025-11-19",
        "translated_summary": "飞机维修技师目前需花费高达30%的工作时间查阅技术手册，这在必须确保每个操作步骤都可追溯至认证来源的航空维修场景中，已成为公认的效率瓶颈。我们开发了一套合规检索系统，通过适配大语言模型重排与语义搜索技术，使其在现有认证查阅系统旁并行运行而非替代原有系统。该系统基于ATA章节层级构建版本鲁棒性向量，并运用视觉语言解析技术对认证内容进行结构化处理，使技术人员既能预览排序任务列表，又能通过原有查阅器获取核验流程。在4.9万条合成查询测试中实现超90%检索准确率；针对10名持证技师的跨语言对照研究显示，系统达成90.9%的前十检索成功率，并将单任务查阅时间从6-15分钟压缩至18秒，降幅达95%。这些成果实证表明，语义检索技术能在严格监管框架下有效运行，并为多语言航空维修实践带来实质性效率提升。"
    },
    {
        "title": "Opinion Dynamics Models for Sentiment Evolution in Weibo Blogs",
        "summary": "Online social media platforms enable influencers to distribute content and quickly capture audience reactions, significantly shaping their promotional strategies and advertising agreements. Understanding how sentiment dynamics and emotional contagion unfold among followers is vital for influencers and marketers, as these processes shape engagement, brand perception, and purchasing behavior. While sentiment analysis tools effectively track sentiment fluctuations, dynamical models explaining their evolution remain limited, often neglecting network structures and interactions both among blogs and between their topic-focused follower groups. In this study, we tracked influential tech-focused Weibo bloggers over six months, quantifying follower sentiment from text-mined feedback. By treating each blogger's audience as a single \"macro-agent\", we find that sentiment trajectories follow the principle of iterative averaging -- a foundational mechanism in many dynamical models of opinion formation, a theoretical framework at the intersection of social network analysis and dynamical systems theory. The sentiment evolution aligns closely with opinion-dynamics models, particularly modified versions of the classical French-DeGroot model that incorporate delayed perception and distinguish between expressed and private opinions. The inferred influence structures reveal interdependencies among blogs that may arise from homophily, whereby emotionally similar users subscribe to the same blogs and collectively shape the shared sentiment expressed within these communities.",
        "entry_id": "http://arxiv.org/abs/2511.15303v1",
        "pub_date": "2025-11-19",
        "translated_summary": "在线社交媒体平台使意见领袖能够发布内容并快速获取受众反馈，这显著影响着他们的推广策略与广告合作。理解追随者群体中的情绪动态与情感传染机制对意见领袖和营销者至关重要，因为这些过程直接塑造着用户参与度、品牌认知与消费行为。尽管情感分析工具能有效追踪情绪波动，但解释其演变规律的动态模型仍相对匮乏，往往忽略了博客间的网络结构及其垂直领域粉丝群之间的互动关系。本研究通过持续六个月追踪科技领域微博大V，基于文本挖掘的粉丝反馈量化其情绪走向。当我们将每位博主的受众视为单一“宏观主体”时，发现情绪轨迹遵循迭代平均原则——这一观点形成动态模型中的基础机制，也是社交网络分析与动态系统理论交叉领域的核心框架。情绪演变规律与观点动力学模型高度吻合，尤其是融合延迟感知机制、区分公开与私人意见的改进版French-DeGroot模型。通过推导出的影响力结构，我们发现博客间存在由同质效应产生的相互依赖：情感倾向相似的用户会订阅相同博客，并共同塑造这些社群内显现的集体情绪。"
    },
    {
        "title": "Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing",
        "summary": "Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.",
        "entry_id": "http://arxiv.org/abs/2511.15241v1",
        "pub_date": "2025-11-19",
        "translated_summary": "计算机化自适应测试是在线教育平台中广泛使用的学习者能力评估技术。该技术通过利用先验能力估计值动态选题，并依据答题结果迭代更新估计值，实现个性化学习者建模，已引起广泛关注。然而现有研究大多聚焦于提升诊断精度，忽视了自适应过程中固有的选择偏差问题。由于选题策略受能力估计值显著影响（如向低能力者分配简单题目，向高能力者分配难题），而题目选择又依赖于先验估计，这种偏差会渗入诊断模型，并在迭代更新过程中不断放大，最终导致预测结果失准。此外，学习者历史交互数据的不平衡性往往加剧诊断模型的偏差。针对该问题，我们提出包含双重核心模块的去偏框架：跨属性考生检索与选择性混合正则化。首先，我们检索具有均衡正误答题分布的考生作为参照基准，以其作为存在偏差考生的中性参照。随后在保持标签一致性的前提下，对存在偏差的考生与其匹配的均衡参照对象实施混合增强。这种数据增强策略有效丰富了偏差冲突样本的多样性，并平滑了选择边界。最终在两大基准数据集上的实验表明，本方法基于多种先进诊断模型，显著提升了计算机化自适应测试中选题策略的泛化能力与公平性。"
    },
    {
        "title": "ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation",
        "summary": "Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.",
        "entry_id": "http://arxiv.org/abs/2511.15141v1",
        "pub_date": "2025-11-19",
        "translated_summary": "近年来，大型语言模型凭借其强大的推理能力和处理冷启动物品的有效性，被广泛用作推荐系统。为更好地适配推荐场景，检索增强生成技术被引入应用。现有RAG方法多基于用户视角，通过检索与目标用户相似的用户购买模式，并将其提供给大语言模型。本研究提出ItemRAG——一种基于物品的RAG方法，该方法从物品间协同购买历史中检索相关物品（而非用户），帮助大语言模型捕捉对推荐有益的物品协同购买模式。特别地，我们的检索策略融合语义相似的物品以优化冷启动物品处理，同时利用协同购买频率提升检索物品的相关性。大量实验表明，ItemRAG在以下方面表现卓越：（1）将基于大语言模型的零样本推荐系统的命中率提升最高达43%；（2）在常规场景与冷启动物品推荐场景下均优于基于用户的RAG基线方法。"
    },
    {
        "title": "Multi-Aspect Cross-modal Quantization for Generative Recommendation",
        "summary": "Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.",
        "entry_id": "http://arxiv.org/abs/2511.15122v1",
        "pub_date": "2025-11-19",
        "translated_summary": "生成式推荐作为推荐系统的新范式，通过量化表征对项目特征进行离散化处理，将用户历史交互行为建模为离散标记序列，并基于该序列采用下一标记预测方法进行项目推荐。该范式的核心挑战在于构建层次清晰、冲突率低且利于生成模型训练的高质量语义标识符。然而，现有方法在利用多模态信息捕捉深层跨模态交互关系方面仍存在局限，而这些特性对学习优质语义标识符和有效训练生成式推荐模型至关重要。为此，我们提出多视角跨模态量化生成式推荐框架MACRec，从多维度将跨模态信息融入语义标识符学习与生成模型训练全过程。具体而言，在标识符学习阶段引入跨模态量化机制，通过多模态信息的互补融合有效降低冲突率，提升码本可用性；同时构建包含显性与隐性对齐的多视角跨模态对齐策略，进一步增强生成式推荐模型的推理能力。我们在三个知名推荐数据集上的实验结果表明，该方法的性能显著优于现有主流方案。"
    },
    {
        "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
        "summary": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.\n  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.\n  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
        "entry_id": "http://arxiv.org/abs/2511.15061v1",
        "pub_date": "2025-11-19",
        "translated_summary": "基因组问答通常需要复杂推理和跨生物医学数据源的整合。GeneGPT通过将领域专用API与OpenAI的代码生成大模型相结合，实现了与基因组数据库的自然语言交互。然而，其依赖专有模型的特性限制了可扩展性，增加了运营成本，并引发了对数据隐私与泛化能力的担忧。\n\n本研究通过采用开源模型（包括Llama 3.1、Qwen2.5及Qwen2.5 Coder）在单体架构中复现GeneGPT，率先通过实验验证了该方案的局限性。在此基础上，我们开发了OpenBioLLM——采用模块化多智能体框架，通过引入工具路由、查询生成和响应验证的智能体分工机制，扩展了GeneGPT的协同推理与角色化任务执行能力。\n\n在超过90%的基准任务中，OpenBioLLM达到或超越了GeneGPT的性能，在Gene-Turing和GeneHop基准上分别取得0.849和0.830的平均分，且仅使用未经额外微调的小型开源模型。该框架的模块化多智能体设计使基准任务延迟降低40-50%，在保持模型能力的同时显著提升效率。综合评估结果凸显了开源多智能体系统在基因组问答领域的潜力。代码与资源已开源：https://github.com/ielab/OpenBioLLM。"
    },
    {
        "title": "SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs",
        "summary": "Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.\n  In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.\n  Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.",
        "entry_id": "http://arxiv.org/abs/2511.14881v1",
        "pub_date": "2025-11-18",
        "translated_summary": "基于深度学习的大规模推荐模型服务化部署面临诸多挑战。现有系统依赖基于CPU的近似最近邻索引与过滤服务，存在不可忽视的资源开销且丧失了联合优化机会。这种低效性导致系统难以支撑更复杂的模型架构，例如基于学习的相似度计算和多任务检索。\n\n本文提出SilverTorch——一种基于GPU的推荐模型服务化系统。该系统通过将独立索引与过滤服务替换为模型服务层，实现了模型服务的统一化。我们提出GPU上的布隆索引算法用于特征过滤，并开发了基于张量原生融合的Int8近似最近邻计算内核。通过协同设计近似最近邻搜索索引与过滤索引，有效降低了GPU显存占用并消除了冗余计算。受益于该服务范式，我们引入了顶层架构评分层与价值模型来实现多任务结果聚合，从而提升检索精度并为未来复杂模型服务研究奠定基础。在排序阶段，该系统通过预缓存服务模型内的物品嵌入向量，显著加速了嵌入计算过程。\n\n在工业级数据集上的评估表明，相较于现有最优方案，SilverTorch可实现延迟降低5.6倍、吞吐量提升23.7倍。实验同时证明，在通过部署更复杂模型提升精度的前提下，该方案比基于CPU的解决方案成本效益高出13.35倍。目前SilverTorch已在多个核心产品中在线服务数百个模型，为日均数十亿活跃用户提供内容推荐服务。"
    },
    {
        "title": "Jasper-Token-Compression-600M Technical Report",
        "summary": "This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.",
        "entry_id": "http://arxiv.org/abs/2511.14405v2",
        "pub_date": "2025-11-18",
        "translated_summary": "本技术报告介绍了2025年11月发布的开源Jasper-Token-Compression-600M模型的训练方法与评估结果。基于先前英文Stella与Jasper模型的蒸馏方案，我们成功将该方法扩展至双语（英文与中文）领域，并通过引入对比学习进一步提升了模型性能。本模型的核心创新在于提出基于一维卷积的令牌压缩模块，在训练过程中动态调整压缩率，使模型能够学习更鲁棒、更高效的压缩文本表示。通过将知识蒸馏与令牌压缩技术相结合，我们在嵌入质量与推理效率方面均实现显著提升。该模型在达到与80亿参数模型相当性能的同时，运行效率显著优于传统6亿参数模型。更多模型发布信息请访问：https://huggingface.co/infgrad/Jasper-Token-Compression-600M。"
    },
    {
        "title": "PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest Neighbor Search",
        "summary": "Similarity searches are a critical task in data mining. As data sets grow larger, exact nearest neighbor searches quickly become unfeasible, leading to the adoption of approximate nearest neighbor (ANN) searches. ANN has been studied for text data, images, and trajectories. However, there has been little effort to develop ANN systems for polygons in spatial database systems and geographic information systems. We present PolyMinHash, a system for approximate polygon similarity search that adapts MinHashing into a novel 2D polygon-hashing scheme to generate short, similarity-preserving signatures of input polygons. Minhash is generated by counting the number of randomly sampled points needed before the sampled point lands within the polygon's interior area, yielding hash values that preserve area-based Jaccard similarity. We present the tradeoff between search accuracy and runtime of our PolyMinHash system. Our hashing mechanism reduces the number of candidates to be processed in the query refinement phase by up to 98% compared to the number of candidates processed by the brute-force algorithm.",
        "entry_id": "http://arxiv.org/abs/2511.16576v1",
        "pub_date": "2025-11-20",
        "translated_summary": "相似性搜索是数据挖掘中的关键任务。随着数据集规模不断扩大，精确最近邻搜索迅速变得不可行，促使近似最近邻搜索技术得到广泛应用。目前针对文本数据、图像和轨迹的近似最近邻搜索已有深入研究，但在空间数据库系统和地理信息系统中，针对多边形的近似最近邻搜索系统开发却鲜有进展。我们提出了PolyMinHash系统，这是一种近似多边形相似性搜索方案，通过将MinHashing算法创新性地适配为二维多边形哈希方案，生成简洁且保持相似性的输入多边形签名。该哈希机制通过统计随机采样点落入多边形内部区域所需的采样次数来生成哈希值，从而保持基于面积的杰卡德相似性。我们展示了PolyMinHash系统在搜索精度与运行时间之间的权衡关系。实验表明，与暴力算法相比，我们的哈希机制在查询优化阶段需要处理的候选数据量最高可减少98%。"
    },
    {
        "title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation",
        "summary": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.\n  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.\n  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.",
        "entry_id": "http://arxiv.org/abs/2511.16543v1",
        "pub_date": "2025-11-20",
        "translated_summary": "将大语言模型（LLM）融入可解释推荐系统时，端到端架构常面临性能与效率的权衡——排序与解释的联合优化往往导致双重妥协。为此，我们提出Prism这一创新解耦框架，将推荐过程严格分离为独立排序阶段和解释生成阶段。\n\n受知识蒸馏启发，Prism引入强力教师LLM（如FLAN-T5-XXL）作为预言机，生成高保真解释性知识；随后由精调后的轻量级学生模型Prism（如BART-Base）专门将这些知识合成为个性化解释。这种解耦架构确保各组件专注于特定目标，消除了耦合模型的内在冲突。\n\n在基准数据集上的大量实验表明：仅1.4亿参数的Prism模型在忠实度与个性化的人类评估中显著优于110亿参数的教师模型，推理速度提升24倍，内存消耗降低10倍。这些结果验证了“解耦+定向蒸馏”能为高质量可解释推荐提供高效可行的技术路径。"
    },
    {
        "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
        "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5$\\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\\times$ faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
        "entry_id": "http://arxiv.org/abs/2511.16528v1",
        "pub_date": "2025-11-20",
        "translated_summary": "神经信息检索系统在高资源语言中表现优异，但在土耳其语等形态复杂、资源相对匮乏的语言中仍有待探索。目前稠密双编码器主导土耳其信息检索领域，而保留词元级表征进行细粒度匹配的延迟交互模型尚未得到系统评估。我们推出TurkColBERT——首个针对土耳其语检索的稠密编码器与延迟交互模型的综合基准。通过两阶段适配流程：先在土耳其自然语言推理/语义文本相似性任务上微调英语和多语言编码器，再利用基于MS MARCO-TR训练的PyLate将其转换为ColBERT风格检索器。我们在覆盖科学、金融及论证领域的五个土耳其BEIR数据集上评估10个模型，结果显示：参数量仅100万的colbert-hash-nano-tr比6亿参数的turkish-e5-large稠密编码器缩小600倍，却能保持其71%以上的平均平均精度。延迟交互模型体积比稠密编码器小3-5倍，性能却显著更优：ColmmBERT-base-TR在特定领域任务中平均精度提升高达13.8%。为满足生产需求，我们对比索引算法：MUVERA+重排序比PLAID快3.33倍，并带来1.7%的相对平均精度提升。这使得ColmmBERT-base-TR在MUVERA下实现0.54毫秒查询延迟。我们公开所有检查点、配置和评估脚本。当前局限包括依赖中等规模数据集（≤5万文档）及翻译基准，可能无法完全反映真实土耳其语检索环境；更大规模的MUVERA评估仍有待进行。"
    },
    {
        "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation",
        "summary": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.\n  This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.",
        "entry_id": "http://arxiv.org/abs/2511.16478v1",
        "pub_date": "2025-11-20",
        "translated_summary": "音乐推荐系统长期依赖信息检索框架，其进展主要通过检索导向子任务的准确度来衡量。这种简化范式虽有效，却难以回答\"何为优质推荐\"的核心问题，而通过用户研究或公平性分析来拓宽评估维度的尝试收效甚微。大语言模型的出现打破了这一框架：其生成式特性与基于排序的传统方法截然不同，使得标准准确度指标不再适用。同时，大语言模型也带来幻觉、知识截止期、非确定性及训练数据不透明等新挑战，导致传统训练/测试方案难以奏效。但另一方面，它们也创造了自然语言交互乃至让模型充当评估者的新机遇。\n\n本文主张，大语言模型驱动的音乐推荐系统需要重建评估体系。我们首先梳理大语言模型如何重塑用户建模、内容建模及自然语言推荐三个维度，继而考察自然语言处理领域的评估实践，提炼适用于音乐推荐系统的方法论与开放挑战。最后通过聚焦提示工程在音乐推荐系统的应用，我们构建出包含成功维度与风险维度的结构化评估框架。本研究旨在为音乐推荐领域提供与时俱进的、具有教学意义的跨学科评估视角。"
    },
    {
        "title": "ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports",
        "summary": "We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.",
        "entry_id": "http://arxiv.org/abs/2511.16438v1",
        "pub_date": "2025-11-20",
        "translated_summary": "我们推出ESGBench——一个基于企业可持续发展报告的ESG可解释问答系统评估基准数据集与评估框架。该基准包含跨越多维度ESG主题的领域扎根问题，并配备人工精校的参考答案与佐证依据，支持对模型推理过程进行细粒度评估。通过对前沿大语言模型在ESGBench上的表现分析，我们揭示了其在事实一致性、答案溯源性及领域适配性方面的核心挑战。该基准旨在推动面向ESG领域的透明可信人工智能系统研究进程。"
    },
    {
        "title": "An Efficient LLM-based Evolutional Recommendation with Locate-Forget-Update Paradigm",
        "summary": "Nowadays, Large Language Models (LLMs) have shown exceptional performance in sequential recommendations, and the adoption of LLM-based recommender systems (LLMRec) is becoming increasingly widespread in existing e-commerce platforms. Despite the impressive performance, the constant high volume of new user-item interactions makes it difficult to adapt to the evolution of user preference over time, especially for LLM-based recommender systems. The challenge arises from the large number of parameters in LLMs, which makes traditional evolution methods (i.e., Re-training or Fine-tuning) impractical. Specifically, Re-training with all interactions results in prohibitively high computational costs. On the other hand, fine-tuning with only new interactions leads to preference forgetting among inactive users, ultimately compromising overall performance. To tackle this problem, we propose EvoRec, an efficient Locate-Forget-Update framework designed for LLM-based recommender systems to model the evolution of user preferences. EvoRec identifies a small set of parameters associated with preference changes and updates them precisely, thereby saving computational resources while maintaining strong recommendation performance. Notably, the modified parameters account for only 30\\% of LoRA adapter parameters, with no additional parameters introduced. Extensive experiments on two real-world datasets demonstrate that, compared to existing methods, EvoRec not only efficiently evolves LLMRec to adapt to the preferences of active users, but also preserves the interests of inactive users from being disturbed during evolution.",
        "entry_id": "http://arxiv.org/abs/2511.16414v1",
        "pub_date": "2025-11-20",
        "translated_summary": "当前，大语言模型在序列推荐任务中展现出卓越性能，基于大语言模型的推荐系统在电商平台中的应用日益广泛。然而，尽管性能优异，持续涌入的新用户-物品交互数据使其难以适应用户偏好的动态演化，这一挑战对大语言模型推荐系统尤为突出。其根源在于大语言模型参数量庞大，使得传统演化方法（如重训练或微调）难以实施：若采用全量交互数据重训练，将产生难以承受的计算开销；若仅用新交互数据微调，又会导致非活跃用户的偏好遗忘，最终影响整体性能。为此，我们提出EvoRec——一个面向大语言模型推荐系统的高效“定位-遗忘-更新”框架，通过精准识别与偏好演化相关的极小参量子集并进行定向更新，在节约计算资源的同时保持强劲的推荐性能。值得注意的是，该方法仅需调整相当于LoRA适配器30%的参数量，且未引入任何额外参数。在两个真实数据集上的大量实验表明，相较于现有方法，EvoRec不仅能高效推动大语言模型推荐系统适应用户偏好演化，还能在演化过程中有效保护非活跃用户的兴趣不受干扰。"
    },
    {
        "title": "ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge this gap, we propose a novel fine-tuning framework that optimizes the retriever for Answer Alignment. Specifically, we first identify high-quality positive chunks by evaluating their sufficiency to generate the correct answer. We then employ a curriculum-based contrastive learning scheme to fine-tune the retriever. This curriculum leverages LLM-constructed Knowledge Graphs (KGs) to generate augmented queries, which in turn mine progressively challenging hard negatives. This process trains the retriever to distinguish the answer-sufficient positive chunks from these nuanced distractors, enhancing its generalization. Extensive experiments on 10 datasets from the Ultradomain and LongBench benchmarks demonstrate that our fine-tuned retriever achieves state-of-the-art performance, improving 14.5% over the base model without substantial architectural modifications and maintaining strong efficiency for long-context RAG. Our work presents a robust and effective methodology for building truly answer-centric retrievers.",
        "entry_id": "http://arxiv.org/abs/2511.16326v1",
        "pub_date": "2025-11-20",
        "translated_summary": "检索增强生成（RAG）已成为处理知识密集型任务的重要框架，但其在长文本场景中的效果常受限于检索器难以识别稀疏却关键的证据。传统检索器虽针对查询-文档相似度进行优化，却往往无法与生成精确答案的下游目标对齐。为弥补这一差距，我们提出了一个创新微调框架，通过答案对齐机制优化检索器。具体而言，我们首先通过评估文本块生成正确答案的充分性来筛选高质量正样本，随后采用基于课程学习的对比训练方案，利用大语言模型构建的知识图谱生成增强查询，进而挖掘难度渐增的困难负样本。该方法训练检索器从精妙设计的干扰项中识别足以支撑答案的正样本，从而提升其泛化能力。在Ultradomain和LongBench基准的10个数据集上的实验表明，经微调的检索器实现了最先进性能，较基础模型提升14.5%且无需重大结构改动，同时在长文本RAG场景中保持高效性。本研究为构建真正以答案为中心的检索器提供了稳健有效的解决方案。"
    },
    {
        "title": "Incorporating Token Importance in Multi-Vector Retrieval",
        "summary": "ColBERT introduced a late interaction mechanism that independently encodes queries and documents using BERT, and computes similarity via fine-grained interactions over token-level vector representations. This design enables expressive matching while allowing efficient computation of scores, as the multi-vector document representations could be pre-computed offline. ColBERT models distance using a Chamfer-style function: for each query token, it selects the closest document token and sums these distances across all query tokens.\n  In our work, we explore enhancements to the Chamfer distance function by computing a weighted sum over query token contributions, where weights reflect the token importance. Empirically, we show that this simple extension, requiring only token-weight training while keeping the multi-vector representations fixed, further enhances the expressiveness of late interaction multi-vector mechanism. In particular, on the BEIR benchmark, our method achieves an average improvement of 1.28\\% in Recall@10 in the zero-shot setting using IDF-based weights, and 3.66\\% through few-shot fine-tuning.",
        "entry_id": "http://arxiv.org/abs/2511.16106v1",
        "pub_date": "2025-11-20",
        "translated_summary": "ColBERT通过引入延迟交互机制，采用BERT分别编码查询和文档，并基于词元级向量表示进行细粒度交互计算相似度。该设计在实现高效表达匹配的同时，支持离线预计算多向量文档表示以提升评分效率。该模型采用Chamfer风格的距离函数：为每个查询词元选取最接近的文档词元，并将所有查询词元的距离求和。\n\n本研究针对Chamfer距离函数提出改进方案，通过计算查询词元贡献度的加权和来增强模型性能，其中权重反映词元重要性。实证研究表明，在保持多向量表示固定的前提下，仅需进行词元权重训练，这一简单扩展即可进一步提升延迟交互多向量机制的表达能力。具体而言，在BEIR基准测试中，基于IDF权重的零样本设置下Recall@10指标平均提升1.28%，而经过少量样本微调后，该指标提升幅度可达3.66%。"
    },
    {
        "title": "QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation",
        "summary": "We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym.",
        "entry_id": "http://arxiv.org/abs/2511.15996v1",
        "pub_date": "2025-11-20",
        "translated_summary": "我们推出QueryGym——一个轻量级、可扩展的Python工具包，专门支持基于大语言模型（LLM）的查询重构。这一工具的开发具有重要意义，因为近期研究表明基于LLM的查询重构能显著提升检索效果。然而，尽管不同研究者曾零散地分享过各自方法的实现，目前仍缺乏统一的工具包来提供标准化的实现方案，这阻碍了公平比较、快速实验、一致性基准测试和可靠部署。QueryGym通过提供统一的框架来解决这一痛点，支持基于LLM的重构方法的实施、执行与比较。该工具包具备五大特性：（1）提供应用多种LLM方法的Python API；（2）采用检索无关接口设计，支持与Pyserini、PyTerrier等后端系统集成；（3）建立集中式提示管理系统，支持版本控制与元数据追踪；（4）内置对BEIR、MS MARCO等基准测试的支持；（5）完全开源的扩展实现，向所有研究者开放。QueryGym已在https://github.com/radinhamidi/QueryGym 公开。"
    },
    {
        "title": "Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing",
        "summary": "Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.",
        "entry_id": "http://arxiv.org/abs/2511.15241v2",
        "pub_date": "2025-11-19",
        "translated_summary": "计算机化自适应测试是在线教育平台中广泛使用的学习者能力评估技术。该技术通过基于能力预估值动态选题，并依据答题结果迭代更新能力估计，实现个性化学习者建模，已引起广泛关注。然而现有研究大多聚焦于提升诊断精度，忽视了自适应过程中固有的选择偏差问题。由于选题策略受能力估计值强烈影响（如向低能力者分配简单题目、向高能力者分配难题），而题目选择又依赖于先验估计，这种偏差会渗入诊断模型，并在迭代更新过程中被不断放大，最终导致预测结果失准。此外，学习者历史交互数据的不平衡性往往会加剧诊断模型的偏差。为解决该问题，我们提出包含双重核心模块的去偏框架：跨属性考生检索与选择性混合正则化。首先检索具有均衡正误答题分布的考生作为偏斜考生的中性参照，随后在保持标签一致性的前提下对偏斜考生与其匹配的均衡参照实施混合增强。该方法有效丰富了偏差冲突样本的多样性，平滑了选择边界。最终在两大基准数据集上的实验表明，我们的方案能显著提升计算机化自适应测试中选题策略的泛化能力与公平性。"
    },
    {
        "title": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback",
        "summary": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.",
        "entry_id": "http://arxiv.org/abs/2511.17255v1",
        "pub_date": "2025-11-21",
        "translated_summary": "大型视觉语言模型（VLMs）能够通过自然语言查询实现直观的视觉搜索。然而，提升其性能通常需要对模型进行微调或扩展至更大规模。本研究受传统文本搜索机制启发，提出在推理阶段通过相关性反馈提升检索性能的方法。该模型无关的设计既可替代微调，也可与经过微调的VLMs协同使用。具体而言，我们针对基于VLM的检索提出并评估了四种反馈策略：首先改进经典伪相关性反馈（PRF），通过top-ranked结果优化查询嵌入；为克服其局限性，提出生成式相关性反馈（GRF），利用合成描述文本进行查询优化；进一步设计注意力反馈汇总器（AFS），这是一种基于Transformer的定制模型，可整合相关项目的多模态细粒度特征；最后通过真实标注文本模拟显式反馈作为性能上限基准。在Flickr30k和COCO数据集上的实验表明，相较于无反馈检索，GRF、AFS与显式反馈能使较小VLM的MRR@5提升3-5%，较大模型提升1-3%。值得注意的是，AFS与显式反馈类似，能有效抑制查询漂移现象，并在迭代式多轮检索中表现出比GRF更强的鲁棒性。本研究证实相关性反馈能持续增强不同VLM的检索性能，为交互式自适应视觉搜索开辟了新路径。"
    },
    {
        "title": "Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters",
        "summary": "Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \\textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.",
        "entry_id": "http://arxiv.org/abs/2511.17044v1",
        "pub_date": "2025-11-21",
        "translated_summary": "参数化检索增强生成（PRAG）是一种新型RAG范式，它通过LoRA适配器对文档进行参数化，将外部知识直接整合到大语言模型中，相比传统RAG方法显著降低了推理成本。然而，现有PRAG方法采用**一对一**的文档编码方案，为每个独立文档配备专属LoRA适配器。这种方案存在两大局限：首先，由于单个LoRA适配器的训练数据有限，会导致数据稀疏问题；其次，在推理过程中需要为每个候选文本段合并新的LoRA适配器与LLM权重，产生了高昂的计算开销。为突破这些限制，我们提出了一种新颖的PRAG文本编码范式——潜在路由编码机制（Poly-PRAG）。在离线编码阶段，我们将文档集的编码视为多任务学习过程，为每个文本段分配唯一任务标识符。通过路由函数调度，仅需少量潜在LoRA适配器即可覆盖整个文本空间的编码需求。在线推理时，该路由函数会根据输入查询动态激活对应的潜在专家子集。我们在多个知识密集型NLP任务上对Poly-PRAG进行了全面评估，大量实验证明该方法的有效性，在四个不同数据集上均取得了最先进的性能表现。"
    },
    {
        "title": "CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation",
        "summary": "The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.",
        "entry_id": "http://arxiv.org/abs/2511.17041v1",
        "pub_date": "2025-11-21",
        "translated_summary": "大规模在线开放课程(MOOC)的快速发展为个性化学习带来了重大挑战，其中概念推荐尤为关键。现有方法通常依赖异构信息网络或知识图谱来捕捉概念关系，并结合知识追踪模型评估学习者认知状态。然而，这些方法因依赖高质量结构化知识图谱而存在明显局限，而现实教育场景中此类图谱往往稀缺。为应对这一根本性挑战，本文提出CLLMRec创新框架，通过两大协同技术支柱利用大语言模型：语义对齐与先修知识蒸馏。语义对齐组件通过编码学习者与概念的非结构化文本描述，构建统一表征空间；先修知识蒸馏范式采用师生架构，由大型教师LLM（实现为先验知识感知组件）从其内化的世界知识中提取概念先修关系，并将其蒸馏为软标签以训练高效的学生排序器。在此基础上，我们的框架引入精细排序机制，通过深度知识追踪显式建模学习者实时认知状态，确保推荐既符合知识结构又契合认知水平。在两个真实MOOC数据集上的大量实验表明，CLLMRec在多项评估指标上显著优于现有基线方法，验证了该框架在不依赖显式结构先验的情况下，能有效生成真正具有认知意识且个性化的概念推荐。"
    },
    {
        "title": "RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers",
        "summary": "Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.",
        "entry_id": "http://arxiv.org/abs/2511.16943v1",
        "pub_date": "2025-11-21",
        "translated_summary": "生成式推荐系统通常利用语义标识符（SID），将每个物品表示为编码语义信息的标记序列。然而，使用多个SID表示物品ID会显著增加输入序列长度，这成为计算复杂度和内存消耗的主要制约因素。现有研究主要聚焦于优化注意力计算和KV缓存，而本文提出RASTP（表征感知语义标记剪枝）方法，直接对输入序列中信息量较低的标记进行剪枝。具体而言，RASTP通过结合语义显著性（通过表征幅度度量）和注意力中心性（源自累积注意力权重）来评估标记重要性。由于RASTP能动态剪枝低信息量或不相关的语义标记，在三个真实亚马逊数据集上的实验表明，该方法在保持或略微提升推荐性能的同时，将训练时间缩短了26.7%。相关代码已开源：https://github.com/Yuzt-zju/RASTP。"
    },
    {
        "title": "δ-EMG: A Monotonic Graph Index for Approximate Nearest Neighbor Search",
        "summary": "Approximate nearest neighbor (ANN) search in high-dimensional spaces is a foundational component of many modern retrieval and recommendation systems. Currently, almost all algorithms follow an $ε$-Recall-Bounded principle when comparing performance: they require the ANN search results to achieve a recall of more than $1-ε$ and then compare query-per-second (QPS) performance. However, this approach only accounts for the recall of true positive results and does not provide guarantees on the deviation of incorrect results. To address this limitation, we focus on an Error-Bounded ANN method, which ensures that the returned results are a $(1/δ)$-approximation of the true values. Our approach adopts a graph-based framework. To enable Error-Bounded ANN search, we propose a $δ$-EMG (Error-bounded Monotonic Graph), which, for the first time, provides a provable approximation for arbitrary queries. By enforcing a $δ$-monotonic geometric constraint during graph construction, $δ$-EMG ensures that any greedy search converges to a $(1/δ)$-approximate neighbor without backtracking. Building on this foundation, we design an error-bounded top-$k$ ANN search algorithm that adaptively controls approximation accuracy during query time. To make the framework practical at scale, we introduce $δ$-EMQG (Error-bounded Monotonic Quantized Graph), a localized and degree-balanced variant with near-linear construction complexity. We further integrate vector quantization to accelerate distance computation while preserving theoretical guarantees. Extensive experiments on the ANN-Benchmarks dataset demonstrate the effectiveness of our approach. Under a recall requirement of 0.99, our algorithm achieves 19,000 QPS on the SIFT1M dataset, outperforming other methods by more than 40\\%.",
        "entry_id": "http://arxiv.org/abs/2511.16921v1",
        "pub_date": "2025-11-21",
        "translated_summary": "高维空间中的近似最近邻搜索是现代检索与推荐系统的核心组件。当前算法在性能比较时普遍遵循$ε$-召回率约束原则：要求搜索结果达到$1-ε$以上的召回率后比较每秒查询率。但这种方法仅考虑正样本召回率，无法保证错误结果的偏离程度。为突破这一局限，我们提出误差有界的近似最近邻方法，确保返回结果与真实值构成$(1/δ)$-近似。该框架采用图结构实现，我们首次提出具备可证明近似能力的$δ$-误差有界单调图，通过在构图阶段强制施加$δ$-单调几何约束，使得任意查询的贪婪搜索无需回溯即可收敛至$(1/δ)$-近似解。基于该结构，我们设计了误差有界的Top-$k$搜索算法，在查询时自适应控制近似精度。为实现大规模应用，我们进一步提出$δ$-误差有界量化图，这种具备局部性与度平衡特性的变体具有近线性构建复杂度，并通过向量量化加速距离计算且保持理论保证。在ANN-Benchmarks数据集上的实验表明，当召回率要求为0.99时，我们的算法在SIFT1M数据集上实现19,000 QPS，较现有方法提升超40%。"
    },
    {
        "title": "Revisiting Feedback Models for HyDE",
        "summary": "Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.",
        "entry_id": "http://arxiv.org/abs/2511.19349v1",
        "pub_date": "2025-11-24",
        "translated_summary": "当前利用大语言模型进行伪相关反馈的研究，在扩展BM25等稀疏检索器的查询时，通常未采用Rocchio、RM3等成熟反馈模型，而是直接将原始查询与LLM生成的扩展内容进行字符串拼接。这种简单处理是否最优？为解答该问题，我们以HyDE方法为研究对象——该方法通过LLM生成假设性答案文档来丰富查询表征，系统性地重新评估了传统反馈模型的应用价值。实验表明：当采用Rocchio等反馈算法对扩展词项进行提取和加权时，HyDE的检索效果可获得显著提升，这为增强基于LLM的伪相关反馈方法准确率提供了一条简洁有效的改进路径。"
    },
    {
        "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
        "summary": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
        "entry_id": "http://arxiv.org/abs/2511.19325v1",
        "pub_date": "2025-11-24",
        "translated_summary": "查询扩展是通过添加语义相关信息对用户查询进行重构的技术，是单语与跨语言信息检索中确保相关文档不被遗漏的关键环节。随着多语种大语言模型（mLLMs）的发展，查询扩展已从同义词和关联词语义增强转向伪文档生成。伪文档不仅能引入更多相关术语，更能弥合简短查询与长文档之间的鸿沟，这对稠密检索尤为有利。本研究通过多种生成式扩展策略评估了当前主流mLLMs及其微调变体，以探究驱动跨语言检索性能的关键因素。结果显示：查询长度很大程度上决定了提示技术的有效性，而更复杂的提示往往无法带来额外收益；语言差异问题依然显著——基线性能最弱的语言通过跨语言查询扩展可获得最大提升，但不同文字体系语言间的检索效果仍不理想；微调仅当训练与测试数据格式相近时才能提升性能。这些发现凸显了建立更均衡的多语言与跨语言训练及评估资源的迫切性。"
    },
    {
        "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
        "summary": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
        "entry_id": "http://arxiv.org/abs/2511.19324v1",
        "pub_date": "2025-11-24",
        "translated_summary": "跨语言信息检索虽能帮助人们获取多语言知识，但由于资源差异、文字体系不同以及嵌入模型中跨语言语义对齐能力较弱，该技术仍面临挑战。现有流程通常依赖翻译和单语检索启发式方法，这会增加计算开销并引入噪声，导致性能下降。本研究系统评估了四种干预类型——文档翻译、基于预训练编码器的多语言稠密检索、在词汇/短语/查询-文档层面的对比学习，以及交叉编码器重排序——在三个基准数据集上的表现。我们发现：专门针对跨语言检索训练的稠密检索模型持续优于词汇匹配方法，且几乎无法从文档翻译中获益；对比学习能有效缓解语言偏见，对初始对齐能力较弱的编码器带来显著提升；重排序虽具有潜力，但其效果取决于交叉编码器训练数据的质量。尽管高资源语言在整体性能上仍占优势，但针对低资源语言及跨文字体系语言对的检索效果，相比词汇匹配和文档翻译基线方法提升最为显著。这些发现表明，跨语言搜索系统应优先考虑基于语义的多语言嵌入和有针对性的学习对齐方法，而非依赖翻译流程，这对跨文字体系及资源匮乏语言尤为重要。"
    },
    {
        "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation",
        "summary": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.",
        "entry_id": "http://arxiv.org/abs/2511.19176v1",
        "pub_date": "2025-11-24",
        "translated_summary": "食谱推荐已成为在线美食平台的核心任务，其关键挑战在于如何有效利用用户-食谱交互之外的多模态特征。分析表明，即使简单运用多模态信号也能取得可观效果，这预示着系统化增强此类信号具有巨大潜力。我们提出TESMR三阶段推荐框架，通过以下方式将原始多模态特征逐步优化为高效嵌入表示：（1）基于多模态理解基础模型的内容增强；（2）通过用户-食谱交互消息传递的关系增强；（3）结合可学习嵌入对比学习的学习增强。在两个真实数据集上的实验表明，TESMR以7-15%的Recall@10提升率超越现有方法。"
    },
    {
        "title": "Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation",
        "summary": "The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users.",
        "entry_id": "http://arxiv.org/abs/2511.18997v1",
        "pub_date": "2025-11-24",
        "translated_summary": "社交媒体平台上短视频的快速传播为推荐系统带来了独特的挑战与机遇。用户偏好呈现多元化特征，不同策略引发的响应常相互冲突，观看时长与视频点击量等指标间甚至可能呈现负相关性。现有提升模型在处理短视频推荐中的异构多策略场景时存在局限，往往难以同时捕捉不同策略的协同效应与独立因果影响。此外，传统固定权重的响应平衡方法缺乏个性化考量，易导致决策偏差。针对这些问题，我们提出一种新颖的异构多策略提升模型（HMUM）框架，用于短视频推荐中的权衡优化。该框架包含离线混合提升建模（HUM）模块——用于捕捉多策略的协同与独立效应，以及在线动态决策（DDM）模块——通过实时评估不同用户响应的价值权重实现个性化决策。在两组公开数据集、一组工业数据集及快手平台的在线A/B测试中，我们的模型展现出卓越的离线性能及关键指标的显著提升。目前该模型已在平台全面部署，服务数亿用户。"
    },
    {
        "title": "STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models",
        "summary": "Ranking models have become an important part of modern personalized recommendation systems. However, significant challenges persist in handling high-cardinality, heterogeneous, and sparse feature spaces, particularly regarding model scalability and efficiency. We identify two key bottlenecks: (i) Representation Bottleneck: Driven by the high cardinality and dynamic nature of features, model capacity is forced into sparse-activated embedding layers, leading to low-rank representations. This, in turn, triggers phenomena like \"One-Epoch\" and \"Interaction-Collapse,\" ultimately hindering model scalability.(ii) Computational Bottleneck: Integrating all heterogeneous features into a unified model triggers an explosion in the number of feature tokens, rendering traditional attention mechanisms computationally demanding and susceptible to attention dispersion. To dismantle these barriers, we introduce STORE, a unified and scalable token-based ranking framework built upon three core innovations: (1) Semantic Tokenization fundamentally tackles feature heterogeneity and sparsity by decomposing high-cardinality sparse features into a compact set of stable semantic tokens; and (2) Orthogonal Rotation Transformation is employed to rotate the subspace spanned by low-cardinality static features, which facilitates more efficient and effective feature interactions; and (3) Efficient attention that filters low-contributing tokens to improve computional efficiency while preserving model accuracy. Across extensive offline experiments and online A/B tests, our framework consistently improves prediction accuracy(online CTR by 2.71%, AUC by 1.195%) and training effeciency (1.84 throughput).",
        "entry_id": "http://arxiv.org/abs/2511.18805v1",
        "pub_date": "2025-11-24",
        "translated_summary": "排序模型已成为现代个性化推荐系统的重要组成部分。然而在处理高基数、异构且稀疏的特征空间时，尤其在模型可扩展性与效率方面仍存在显著挑战。我们识别出两大核心瓶颈：（i）表征瓶颈：受高基数特征动态特性驱动，模型容量被迫集中于稀疏激活的嵌入层，导致低秩表征，进而引发\"单周期\"与\"交互坍缩\"现象，最终制约模型扩展性；（ii）计算瓶颈：将所有异构特征整合至统一模型会引发特征令牌数量激增，使得传统注意力机制计算成本高昂且易受注意力分散影响。为突破这些障碍，我们提出STORE——基于三大核心创新的统一可扩展令牌排序框架：（1）语义令牌化通过将高基数稀疏特征解构为紧凑的稳定语义令牌集合，从根本上解决特征异构性与稀疏性问题；（2）采用正交旋转变换对低基数静态特征张成的子空间进行旋转，促进更高效的特征交互；（3）通过过滤低贡献令牌的高效注意力机制，在保持模型精度的同时提升计算效率。经过大量离线实验与在线A/B测试，本框架持续提升预测准确率（在线CTR提升2.71%，AUC提升1.195%）与训练效率（吞吐量提升1.84倍）。"
    },
    {
        "title": "Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search",
        "summary": "Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.",
        "entry_id": "http://arxiv.org/abs/2511.18749v1",
        "pub_date": "2025-11-24",
        "translated_summary": "大型语言模型为自动化端到端事实核查带来了希望，但先前研究得出的结果好坏参半。随着主流聊天机器人普遍配备推理能力和网络搜索工具——且已有数百万用户依赖其进行信息验证——开展严谨评估已刻不容缓。我们基于PolitiFact核查的6000余条声明，对OpenAI、谷歌、Meta和DeepSeek的15款最新大模型进行评估，对比了标准模型与具备推理能力及网络搜索功能的变体。结果显示：标准模型表现欠佳，推理能力带来的提升微乎其微，尽管网络中存在事实核查记录，网络搜索仅带来有限改进。相比之下，采用PolitiFact摘要的定向检索增强生成系统，在不同模型变体上平均将宏观F1值提升了233%。这些发现表明，为模型提供经过筛选的高质量上下文，是实现自动化事实核查的有效路径。"
    },
    {
        "title": "Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation",
        "summary": "Recent advances in Large Language Models (LLMs) have opened new avenues for sequential recommendation by enabling natural language reasoning over user behavior sequences. A common approach formulates recommendation as a language modeling task, where interaction histories are transformed into prompts and user preferences are learned via supervised fine-tuning. However, these methods operate solely in the textual modality and often miss users' fine-grained interests, especially when shaped by rich visual signals such as product images or movie posters. Multimodal Large Language Models (MLLMs) offer a promising alternative by aligning text and vision in a shared semantic space. A prevalent training paradigm applies Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) to model user preferences. Yet, two core challenges remain: 1) Imbalanced sample hardness, where random negative sampling causes overfitting on easy examples and under-training on hard ones; 2) Cross-modal semantic bias, where the fixed reference model in DPO prevents the policy model from correcting modality misalignments--especially over long sequences. To address these issues, we propose a Multimodal LLM framework that integrates Hardness-aware and Noise-regularized preference optimization for Recommendation (HaNoRec). Specifically, HaNoRec dynamically adjusts optimization weights based on both the estimated hardness of each training sample and the policy model's real-time responsiveness, prioritizing harder examples. It further introduces Gaussian-perturbed distribution optimization on output logits to enhance cross-modal semantic consistency and reduce modality bias inherited from the reference model.",
        "entry_id": "http://arxiv.org/abs/2511.18740v1",
        "pub_date": "2025-11-24",
        "translated_summary": "大型语言模型（LLM）的最新进展通过实现对用户行为序列的自然语言推理，为序列推荐开辟了新途径。主流方法将推荐任务构建为语言建模问题：将交互历史转换为提示文本，通过监督微调学习用户偏好。然而这些方法仅基于文本模态，往往忽略用户的细粒度兴趣——尤其是当这些兴趣由商品图片、电影海报等丰富视觉信号塑造时。多模态大语言模型（MLLM）通过将文本与视觉对齐到共享语义空间，提供了更有前景的解决方案。当前主流训练范式采用监督微调（SFT）与直接偏好优化（DPO）相结合的方式来建模用户偏好，但仍存在两大核心挑战：1）样本难度失衡，随机负采样会导致模型过拟合简单样本而难以样本训练不足；2）跨模态语义偏差，DPO中固定的参考模型会阻碍策略模型修正模态未对齐问题（在长序列中尤为突出）。为此，我们提出HaNoRec多模态大语言模型框架，通过集成面向推荐的硬度感知与噪声正则化偏好优化方法，动态根据样本预估难度和策略模型实时响应度调整优化权重，优先处理困难样本；同时引入输出逻辑的高斯扰动分布优化，以增强跨模态语义一致性，降低参考模型继承的模态偏差。"
    },
    {
        "title": "When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation",
        "summary": "Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.",
        "entry_id": "http://arxiv.org/abs/2511.18717v1",
        "pub_date": "2025-11-24",
        "translated_summary": "序列推荐通过建模用户偏好来预测下一目标项目。现有研究大多采用被动推荐模式，仅在用户打开应用时进行响应，错失了应用关闭后的推荐机会。本文研究主动推荐范式，通过预测下次交互时间并主动推送项目。该模式面临两大挑战：如何精准预测兴趣时间点，以及如何基于预测时间生成兴趣项目。我们提出PASRec——基于扩散模型的推荐框架，通过联合优化目标实现兴趣时间与兴趣项目的协同对齐。在五个基准数据集上的实验表明，该方法在留一法和时间分割两种评估策略下均优于八种前沿基线模型。"
    },
    {
        "title": "A Recommender System Based on Binary Matrix Representations for Cognitive Disorders",
        "summary": "Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy.",
        "entry_id": "http://arxiv.org/abs/2511.18645v1",
        "pub_date": "2025-11-23",
        "translated_summary": "认知障碍（心理健康）诊断是一项精细而复杂的任务。如何从可能的障碍中筛选出最具信息量的待评估症状，构成了额外挑战。该过程需要掌握诊断标准与症状跨障碍重叠的全面知识，仅凭症状本身难以准确判断。本研究旨在开发一种基于二元矩阵表征的认知障碍诊断推荐系统，其核心算法通过构建障碍与症状组合的二元矩阵，根据患者当前症状对行列进行筛选，从而识别潜在障碍并推荐最具诊断价值的下阶段待查症状。研究采用Python实现了该推荐系统的原型，通过合成测试数据与部分真实数据验证，系统成功从初始症状集中识别出合理障碍，给出优化诊断的后续症状建议，并提供症状-障碍关联的补充信息。尽管目前仅为原型系统，但其已展现出作为临床辅助工具的潜力。该推荐系统的完整应用版本有望帮助心理健康从业者更高效地识别相关障碍，并通过针对性症状追踪调查提升诊断准确性。"
    },
    {
        "title": "General Agentic Memory Via Deep Research",
        "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
        "entry_id": "http://arxiv.org/abs/2511.18423v1",
        "pub_date": "2025-11-23",
        "translated_summary": "记忆对AI智能体至关重要，然而当前广泛采用的静态记忆系统试图预先构建现成可用的记忆，这不可避免地会导致严重的信息丢失。为突破这一局限，我们提出名为“通用智能记忆（GAM）”的创新框架。该框架遵循“即时编译”原则，在离线阶段仅保留简洁有效的记忆，而在运行时专注于为客户端生成优化上下文。为实现这一目标，GAM采用双模块设计：1）记忆模块通过轻量级记忆库提炼关键历史信息，同时在通用页面存储中保存完整历史记录；2）研究模块基于预构建记忆的指引，从页面存储中检索并整合在线请求所需的有效信息。这种设计使GAM能充分发挥前沿大语言模型的智能能力与测试时扩展性，同时通过强化学习实现端到端的性能优化。实验结果表明，在多种需要记忆支撑的任务场景中，GAM相较现有记忆系统均取得显著性能提升。"
    },
    {
        "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations",
        "summary": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.",
        "entry_id": "http://arxiv.org/abs/2511.18413v1",
        "pub_date": "2025-11-23",
        "translated_summary": "代理式推荐将推荐系统视为大型语言模型（LLM）智能体，这些智能体能够在网络应用中规划、推理、使用工具并与不同偏好的用户互动。然而，现有的大多数代理式推荐系统主要关注通用的单智能体规划执行流程或多智能体任务分解流程。由于缺乏面向推荐的设计，这些系统往往未能充分利用用户-物品交互历史中的协同信号，导致推荐结果不尽如人意。为解决这一问题，我们借鉴传统协同过滤算法与基于LLM的多智能体协作之间的相似性，提出了面向代理式推荐的多智能体协同过滤框架（MACF）。具体而言，针对目标用户和查询，我们将相似用户和相关物品实例化为具有独特配置文件的LLM智能体。每个智能体能够调用检索工具、推荐候选物品并与其他智能体交互。与传统协同过滤中静态的偏好聚合不同，MACF通过中央协调智能体，采用动态智能体招募和个性化协作指令的方式，自适应地管理用户与物品智能体之间的协作。在三个不同领域数据集上的实验结果表明，我们的MACF框架相较于强大的代理式推荐基线方法具有显著优势。"
    },
    {
        "title": "A Multimodal Conversational Agent for Tabular Data Analysis",
        "summary": "Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.",
        "entry_id": "http://arxiv.org/abs/2511.18405v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大型语言模型（LLM）能够通过与用户进行包含语音交互的情境感知对话，在保持高性能的同时处理数据分析、可视化和解读，从而重塑信息处理模式。本文提出Talk2Data——一个基于多模态LLM驱动的对话式智能体，用于实现直观的数据探索。该系统允许用户通过语音或文本指令查询数据集，并以图表、表格、统计量或语音解释形式获取答案。该架构以LLM为核心，整合了OpenAI Whisper自动语音识别系统、Qwen-coder代码生成模型、定制化沙箱执行工具以及Coqui文本转语音库，形成智能体协同工作回路。与纯文本分析工具不同，该系统支持跨模态响应适配，并能基于数据集上下文进行多轮对话。在三个数据集48项任务的评估中，原型系统实现95.8%准确率，纯模型生成时间低于1.7秒（不含语音识别与执行时间）。通过对五种参数量（1.5B-32B）LLM的对比实验，揭示了准确率-延迟-成本的平衡关系，其中7B模型在交互场景中表现最佳。通过在与用户对话和代码执行之间建立路由机制（约束于透明沙箱），同时将提示词锚定于模式级上下文，Talk2Data智能体在确保计算可验证的前提下，能可靠地从表格数据中提取可操作的洞察。除系统本身外，本文还探讨了该技术对人机数据交互、LLM驱动分析可信度的影响，以及面向大规模多模态助手的未来拓展方向。"
    },
    {
        "title": "Toward an AI-Native Internet: Rethinking the Web Architecture for Semantic Retrieval",
        "summary": "The rise of Generative AI Search is fundamentally transforming how users and intelligent systems interact with the Internet. LLMs increasingly act as intermediaries between humans and web information. Yet the web remains optimized for human browsing rather than AI-driven semantic retrieval, resulting in wasted network bandwidth, lower information quality, and unnecessary complexity for developers. We introduce the concept of an AI-Native Internet, a web architecture in which servers expose semantically relevant information chunks rather than full documents, supported by a Web-native semantic resolver that allows AI applications to discover relevant information sources before retrieving fine-grained chunks. Through motivational experiments, we quantify the inefficiencies of current HTML-based retrieval, and outline architectural directions and open challenges for evolving today's document-centric web into an AI-oriented substrate that better supports semantic access to web content.",
        "entry_id": "http://arxiv.org/abs/2511.18354v1",
        "pub_date": "2025-11-23",
        "translated_summary": "生成式AI搜索的兴起正在从根本上改变用户及智能系统与互联网的交互方式。大语言模型日益成为人类与网络信息之间的中介桥梁。然而当前网络仍以人类浏览体验为核心进行优化，而非面向AI驱动的语义检索，这导致网络带宽浪费、信息质量下降，并为开发者带来不必要的复杂性。我们提出“AI原生互联网”概念——该架构要求服务器提供语义关联的信息块而非完整文档，并辅以网络原生语义解析器，使AI应用能在检索细粒度信息块前先行发现相关信源。通过动机实验，我们量化了当前基于HTML检索模式的效能损耗，并勾勒出将以文档为核心的现有网络演进为AI导向基础设施的架构路径，以及实现网络内容语义化访问所需的开放挑战。"
    },
    {
        "title": "Time Matters: Enhancing Sequential Recommendations with Time-Guided Graph Neural ODEs",
        "summary": "Sequential recommendation (SR) is widely deployed in e-commerce platforms, streaming services, etc., revealing significant potential to enhance user experience. However, existing methods often overlook two critical factors: irregular user interests between interactions and highly uneven item distributions over time. The former factor implies that actual user preferences are not always continuous, and long-term historical interactions may not be relevant to current purchasing behavior. Therefore, relying only on these historical interactions for recommendations may result in a lack of user interest at the target time. The latter factor, characterized by peaks and valleys in interaction frequency, may result from seasonal trends, special events, or promotions. These externally driven distributions may not align with individual user interests, leading to inaccurate recommendations. To address these deficiencies, we propose TGODE to both enhance and capture the long-term historical interactions. Specifically, we first construct a user time graph and item evolution graph, which utilize user personalized preferences and global item distribution information, respectively. To tackle the temporal sparsity caused by irregular user interactions, we design a time-guided diffusion generator to automatically obtain an augmented time-aware user graph. Additionally, we devise a user interest truncation factor to efficiently identify sparse time intervals and achieve balanced preference inference. After that, the augmented user graph and item graph are fed into a generalized graph neural ordinary differential equation (ODE) to align with the evolution of user preferences and item distributions. This allows two patterns of information evolution to be matched over time. Experimental results demonstrate that TGODE outperforms baseline methods across five datasets, with improvements ranging from 10% to 46%.",
        "entry_id": "http://arxiv.org/abs/2511.18347v1",
        "pub_date": "2025-11-23",
        "translated_summary": "序列推荐系统在电商平台和流媒体服务等领域应用广泛，在提升用户体验方面展现出巨大潜力。然而现有方法往往忽略两个关键因素：交互间不规则的兴趣波动，以及随时间高度不均衡的物品分布。前者意味着用户真实偏好并非持续存在，长期历史交互可能与当前购买行为无关，仅依赖这些历史记录进行推荐可能导致目标时刻缺乏用户兴趣指向；后者表现为交互频率的峰谷波动，可能源于季节趋势、特殊事件或促销活动，这种外部驱动的分布模式若与个体兴趣不匹配将导致推荐失准。为解决这些问题，我们提出TGODE模型，通过增强与捕捉长期历史交互实现精准推荐。具体而言，我们首先构建分别融合用户个性化偏好和全局物品分布信息的用户时间图与物品演化图；针对不规则交互导致的时间稀疏性，设计时间引导的扩散生成器自动获取增强型时间感知用户图；同时开发用户兴趣截断因子，有效识别稀疏时间区间并实现均衡偏好推断。随后将增强的用户图与物品图输入广义图神经常微分方程，使其与用户偏好和物品分布的演化过程对齐，实现双模式信息随时间的动态匹配。实验结果表明，TGODE在五个数据集上均优于基线方法，性能提升幅度达10%至46%。"
    },
    {
        "title": "UFO: Unfair-to-Fair Evolving Mitigates Unfairness in LLM-based Recommender Systems via Self-Play Fine-tuning",
        "summary": "Large language model-based Recommender Systems (LRSs) have demonstrated superior recommendation performance by integrating pre-training with Supervised Fine-Tuning (SFT). However, this approach introduces item-side unfairness. Existing studies primarily attribute this issue to the absence of fairness constraints during SFT and attempt to mitigate unfairness via re-weighting and re-ranking methods. In this paper, we find that unfairness arises not only from SFT but also from pre-training, where inherent biases are further amplified during SFT. This finding underscores the failure of current methods to address the root causes of unfairness. Moreover, current methods struggle to preserve satisfactory recommendation performance. To tackle these issues, we propose an Unfair-to-Fair evOlving (UFO) framework using a self-play mechanism, formulating unfairness mitigation as a two-player game. UFO alternates between two player roles: the \\textit{judger}, which identifies unfairness from both pre-training and SFT, and the \\textit{corrector}, which adjusts the LRS to address identified unfairness while preserving recommendation performance. Iterative optimization between these roles enables UFO to completely resolve unfairness. Extensive experiments demonstrate that UFO effectively mitigates unfairness while improving recommendation performance.",
        "entry_id": "http://arxiv.org/abs/2511.18342v1",
        "pub_date": "2025-11-23",
        "translated_summary": "基于大语言模型的推荐系统（LRSs）通过将预训练与监督微调（SFT）相结合，展现出卓越的推荐性能。然而这种方法会引发项目侧的不公平问题。现有研究主要将该问题归因于SFT阶段缺乏公平性约束，并尝试通过重加权和重排序方法缓解不公平性。本文发现，不公平性不仅源于SFT阶段，预训练阶段固有的偏见也会在SFT过程中被进一步放大。这一发现揭示了现有方法未能解决不公平性根本原因的局限性。此外，现有方法难以保持令人满意的推荐性能。为解决这些问题，我们提出基于自我博弈机制的“不公平-公平演化”（UFO）框架，将不公平缓解问题构建为双玩家博弈。UFO交替执行两种角色：\\textit{评判者}（从预训练和SFT中识别不公平现象）与\\textit{校正者}（在保持推荐性能的同时调整LRS以解决已识别的不公平问题）。通过角色间的迭代优化，UFO能彻底消除不公平性。大量实验表明，UFO在有效缓解不公平性的同时，还能提升推荐性能。"
    },
    {
        "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search",
        "summary": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.",
        "entry_id": "http://arxiv.org/abs/2511.18313v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大型语言模型智能体在从知识库中检索上下文时，常因知识库结构与当前推理状态缺乏一致性而导致推理链条断裂。我们提出路径约束检索方法，通过将图结构约束与语义搜索相结合，确保检索信息在知识图谱中保持逻辑关联。该方法将搜索范围限定在锚点节点可达的节点子集，从根源上避免因检索结构断裂信息而引发的推理不一致问题。我们在PathRAG-6基准上验证该方法，该基准覆盖六大领域、包含180个节点和360条边。实验表明：相较于基线方法24%-32%的结构一致性，PCR实现了完全结构一致性，同时保持优异的相关性评分；在技术领域，PCR在排序10位时实现100%相关性且保持完全结构一致性，显著优于向量搜索与混合检索；与基线相比，PCR使检索上下文的平均图距离降低78%，证明其能获取结构更一致的信息。这些发现表明，路径约束检索能有效提升LLM智能体推理系统的可靠性与连贯性。"
    },
    {
        "title": "Large Language Model Enhanced Graph Invariant Contrastive Learning for Out-of-Distribution Recommendation",
        "summary": "Out-of-distribution (OOD) generalization has emerged as a significant challenge in graph recommender systems. Traditional graph neural network algorithms often fail because they learn spurious environmental correlations instead of stable causal relationships, leading to substantial performance degradation under distribution shifts. While recent advancements in Large Language Models (LLMs) offer a promising avenue due to their vast world knowledge and reasoning capabilities, effectively integrating this knowledge with the fine-grained topology of specific graphs to solve the OOD problem remains a significant challenge. To address these issues, we propose {$\\textbf{Inv}$ariant $\\textbf{G}$raph $\\textbf{C}$ontrastive Learning with $\\textbf{LLM}$s for Out-of-Distribution Recommendation (InvGCLLM)}, an innovative causal learning framework that synergistically integrates the strengths of data-driven models and knowledge-driven LLMs. Our framework first employs a data-driven invariant learning model to generate causal confidence scores for each user-item interaction. These scores then guide an LLM to perform targeted graph refinement, leveraging its world knowledge to prune spurious connections and augment missing causal links. Finally, the structurally purified graphs provide robust supervision for a causality-guided contrastive learning objective, enabling the model to learn representations that are resilient to spurious correlations. Experiments conducted on four public datasets demonstrate that InvGCLLM achieves significant improvements in out-of-distribution recommendation, consistently outperforming state-of-the-art baselines.",
        "entry_id": "http://arxiv.org/abs/2511.18282v1",
        "pub_date": "2025-11-23",
        "translated_summary": "分布外泛化已成为图推荐系统面临的重大挑战。传统图神经网络算法因学习虚假的环境相关性而非稳定的因果关系，在分布变化下会出现显著性能衰退。尽管大语言模型凭借其丰富的世界知识和推理能力为此提供了新思路，但如何有效融合其知识体系与具体图谱的细粒度拓扑结构以解决分布外问题仍具挑战。为此，我们提出基于大语言模型的因果不变图对比学习框架InvGCLLM，该创新性因果学习框架实现了数据驱动模型与知识驱动大语言模型的协同融合。该框架首先通过数据驱动的因果不变学习模型生成用户-物品交互的因果置信度，进而引导大语言模型基于世界知识执行定向图结构优化——剪除虚假连接并补全缺失因果边。最终，经结构纯化的图谱为因果引导的对比学习目标提供鲁棒监督信号，使模型能够学习抵御虚假相关性的表征。在四个公开数据集上的实验表明，InvGCLLM在分布外推荐任务中实现显著提升，持续超越现有最优基线模型。"
    },
    {
        "title": "Democratic Recommendation with User and Item Representatives Produced by Graph Condensation",
        "summary": "The challenges associated with large-scale user-item interaction graphs have attracted increasing attention in graph-based recommendation systems, primarily due to computational inefficiencies and inadequate information propagation. Existing methods provide partial solutions but suffer from notable limitations: model-centric approaches, such as sampling and aggregation, often struggle with generalization, while data-centric techniques, including graph sparsification and coarsening, lead to information loss and ineffective handling of bipartite graph structures. Recent advances in graph condensation offer a promising direction by reducing graph size while preserving essential information, presenting a novel approach to mitigating these challenges. Inspired by the principles of democracy, we propose \\textbf{DemoRec}, a framework that leverages graph condensation to generate user and item representatives for recommendation tasks. By constructing a compact interaction graph and clustering nodes with shared characteristics from the original graph, DemoRec significantly reduces graph size and computational complexity. Furthermore, it mitigates the over-reliance on high-order information, a critical challenge in large-scale bipartite graphs. Extensive experiments conducted on four public datasets demonstrate the effectiveness of DemoRec, showcasing substantial improvements in recommendation performance, computational efficiency, and robustness compared to SOTA methods.",
        "entry_id": "http://arxiv.org/abs/2511.18279v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大规模用户-物品交互图带来的挑战在基于图的推荐系统中日益受到关注，主要源于计算效率低下与信息传播不足。现有方法虽提供部分解决方案，但存在明显局限：以模型为中心的方法（如采样与聚合）常面临泛化能力不足，而以数据为中心的技术（如图稀疏化与粗化）则会导致信息丢失及对二分图结构处理失效。图压缩技术的最新进展通过缩减图规模同时保留关键信息，为应对这些挑战提供了新思路。受民主原则启发，我们提出\\textbf{DemoRec}框架，利用图压缩生成用户与物品代表节点以完成推荐任务。通过构建紧凑交互图并聚类原图中具有共同特征的节点，DemoRec显著降低图规模与计算复杂度，同时有效缓解大规模二分图中过度依赖高阶信息的关键问题。在四个公开数据集上的大量实验表明，DemoRec在推荐性能、计算效率和鲁棒性方面均优于现有最优方法，展现出显著优势。"
    },
    {
        "title": "LLM Reasoning for Cold-Start Item Recommendation",
        "summary": "Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.",
        "entry_id": "http://arxiv.org/abs/2511.18261v1",
        "pub_date": "2025-11-23",
        "translated_summary": "大型语言模型凭借其内在的推理能力与海量知识库，在改进推荐系统方面展现出巨大潜力。然而现有研究主要聚焦于用户-物品交互数据充足的暖启动场景，对于交互数据稀疏、传统协同过滤方法难以奏效的冷启动场景探索不足。为突破这一局限，我们针对Netflix领域的冷启动物品推荐提出创新推理策略。该方法利用大型语言模型的先进推理能力，有效推断用户偏好，尤其适用于新上架或交互极少的物品。我们系统评估了监督微调、基于强化学习的微调以及融合两种方法的混合方案，以优化推荐性能。基于真实数据的大规模实验表明，该方案在冷启动推荐场景中实现了方法论效能与实际性能的双重提升。值得注意的是，基于推理的微调模型在特定情况下比Netflix现行排序模型性能提升最高达8%。"
    },
    {
        "title": "HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval",
        "summary": "The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.",
        "entry_id": "http://arxiv.org/abs/2601.16155v1",
        "pub_date": "2026-01-22",
        "translated_summary": "CLIP 的成功推动了文本—视频检索领域的显著进展。然而，现有方法常常陷入“盲目”特征交互：由于查询文本稀疏，模型难以从背景噪声中区分关键视觉信息。为弥合这一鸿沟，我们借鉴人类认知行为，提出 Human Vision-Driven（HVD）模型。该框架建立了一种由粗到精的对齐机制，包含两个核心组件：帧特征选择模块（FFSM）与片段特征压缩模块（PFCM）。FFSM 模拟人类的宏观感知，通过选取关键帧消除时间冗余；PFCM 则模仿微观感知，利用先进的注意力机制将片段特征聚合为显著视觉实体，实现精准的实体级匹配。在五个基准数据集上的大量实验表明，HVD 不仅能象人类一样聚焦关键视觉线索，还取得了当前最优性能。"
    },
    {
        "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
        "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
        "entry_id": "http://arxiv.org/abs/2601.16125v1",
        "pub_date": "2026-01-22",
        "translated_summary": "组合式图像检索（CIR）是多模态理解中一项关键且复杂的任务。现有的 CIR 评测基准普遍存在查询类别单一、难以反映真实场景多样需求的问题。为弥补这一评估差距，本文利用图像编辑对修改类型和内容进行精准控制，设计了一条可在极广类别范围内合成查询流的流水线。借助该流水线，我们构建了细粒度的 CIR 评测基准——EDIR。  \nEDIR 包含 5,000 条高质量查询，按五大主类、十五个子类精细组织。我们对 13 种最先进的多模态嵌入模型进行了全面评测，发现显著的性能缺口：即使在如 RzenEmbed 和 GME 等顶级模型上，各子类间仍存在显著差异，凸显了本基准的严苛性。通过对比分析，我们还揭示了现有基准在模态偏差和类别覆盖不足等方面的内在缺陷。  \n进一步地，我们开展了领域内训练实验，验证了该基准的可操作性，并通过区分“可通过针对性数据解决的类别”与“揭示现有架构固有局限的类别”，明确了任务面临的根本挑战。"
    },
    {
        "title": "Unveiling and Simulating Short-Video Addiction Behaviors via Economic Addiction Theory",
        "summary": "Short-video applications have attracted substantial user traffic. However, these platforms also foster problematic usage patterns, commonly referred to as short-video addiction, which pose risks to both user health and the sustainable development of platforms. Prior studies on this issue have primarily relied on questionnaires or volunteer-based data collection, which are often limited by small sample sizes and population biases. In contrast, short-video platforms have large-scale behavioral data, offering a valuable foundation for analyzing addictive behaviors. To examine addiction-aware behavior patterns, we combine economic addiction theory with users' implicit behavior captured by recommendation systems. Our analysis shows that short-video addiction follows functional patterns similar to traditional forms of addictive behavior (e.g., substance abuse) and that its intensity is consistent with findings from previous social science studies. To develop a simulator that can learn and model these patterns, we introduce a novel training framework, AddictSim. To consider the personalized addiction patterns, AddictSim uses a mean-to-adapted strategy with group relative policy optimization training. Experiments on two large-scale datasets show that AddictSim consistently outperforms existing training strategies. Our simulation results show that integrating diversity-aware algorithms can mitigate addictive behaviors well.",
        "entry_id": "http://arxiv.org/abs/2601.15975v1",
        "pub_date": "2026-01-22",
        "translated_summary": "短视频应用吸引了庞大用户流量，但这些平台也催生了被称为“短视频成瘾”的问题性使用模式，既损害用户健康，也威胁平台的可持续发展。现有研究多依赖问卷调查或小规模自愿样本，存在样本量小、人群偏差等局限。相较之下，短视频平台积累了海量行为数据，为探讨成瘾行为提供了宝贵基石。我们将经济学成瘾理论与推荐系统所捕获的隐性用户行为相结合，揭示短视频成瘾在机制上与传统成瘾（如物质滥用）具有相似的功能模式，其强度也与既往社会科学研究的发现一致。为学习并建模这些模式，我们提出新型训练框架 AddictSim；该框架采用“均值–自适应”策略，结合“群体相对策略优化”训练，兼顾个性化成瘾差异。在两大规模数据集的实验表明，AddictSim 显著优于现有训练策略。模拟结果进一步显示，融入多样性感知算法可有效缓解成瘾行为。"
    },
    {
        "title": "MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging",
        "summary": "Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.",
        "entry_id": "http://arxiv.org/abs/2601.15930v1",
        "pub_date": "2026-01-22",
        "translated_summary": "模型融合（MM）为在不接触原始训练数据且无需昂贵重训的情况下整合多个专业化模型提供了一种高效机制。尽管MM已在计算机视觉等领域取得成功，但其在推荐系统（RS）中的作用仍甚少研究。最近，生成式推荐（GR）作为RS的新范式出现，其特点是模型规模迅速扩大、计算成本高昂，因而在成本敏感场景中MM尤为诱人。本文首次从情境化视角对GR中的MM进行系统研究。我们关注一个在实际应用中既根本又未被充分探讨的挑战：如何合并分别针对多种真实情境训练而成的生成式推荐模型，这些情境源于用户行为的时序演进和异构应用场景。\n\n为此，我们提出统一框架MMGRid——一种结构化的GR检查点情境网格。该网格按由时序演化与领域多样性共同诱导的多种情境组织模型，所有检查点均共享同一基础大语言模型（LLM），但在情境特化数据上微调，从而构建出一个既真实又可控制的模型空间，用于在不同GR范式与融合算法间系统分析MM。我们的研究揭示了若干关键洞察：第一，从LLM训练GR模型时，因词元分布迁移和目标函数差异会导致融合时的参数冲突；通过用基模型替换来解耦任务感知与情境特定的参数变化可有效缓解此冲突。第二，在跨情境的增量训练会引入“近期偏好”偏差，可通过加权情境融合有效平衡。值得注意的是，我们观察到最优融合权重与情境依赖的交互特征显著相关，为真实部署中的权重选择提供了可操作指导。"
    },
    {
        "title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion",
        "summary": "Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.",
        "entry_id": "http://arxiv.org/abs/2601.15860v1",
        "pub_date": "2026-01-22",
        "translated_summary": "表格检索任务旨在从大规模语料中根据自然语言查询找到最相关的表格。然而，非结构化文本与结构化表格之间在结构和语义上的差异，使得嵌入对齐尤为困难。近期方法如 QGpT 尝试通过生成合成查询来丰富表格语义，但仍依赖粗略的局部表采样和简单的融合策略，限制了语义多样性，并阻碍了有效的查询-表格对齐。\n\n为此，我们提出轻量级框架 STAR（Semantic Table Representation），通过语义聚类与加权融合提升表格的语义表示。STAR 首先基于表头感知的 K-means 聚类，将语义相似的行分组；随后选取代表每个聚类中心的实例，组成丰富多样的局部表。接着，STAR 针对不同聚类生成专门的合成查询，以全面覆盖表格语义空间。最后，STAR 采用加权融合策略整合表格与查询嵌入，实现细粒度的语义对齐。该设计使 STAR 能够从结构化和文本来源中捕获互补信息，显著提升语义表示的表达能力。\n\n在五个基准数据集上的实验表明，STAR 在所有数据集的 Recall 指标上均显著优于 QGpT，验证了语义聚类与自适应加权融合在提升表格表示稳健性方面的有效性。代码开源地址：https://github.com/adsl135789/STAR"
    },
    {
        "title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval",
        "summary": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.",
        "entry_id": "http://arxiv.org/abs/2601.15849v1",
        "pub_date": "2026-01-22",
        "translated_summary": "尽管通用型文本嵌入模型在文本检索中表现优异，但在表格检索场景下却难以达到最佳状态：高度结构化的表格内容导致语义压缩，并引发查询与表格之间的语义错配。近期通过大语言模型（LLM）进行检索增强的方法，采用合成查询来缓解这一问题，然而它们通常依赖启发式的局部表选择，极少把生成的合成查询用作监督信号来进一步优化嵌入模型。我们提出 CGPT——一种利用 LLM 监督信号来增强表格检索的训练框架。CGPT 首先对表格实例进行 K-means 聚类，跨簇采样以构建语义更加丰富的局部表；再由 LLM 为这些局部表生成合成查询，并以此进行硬负例对比微调，从而精炼嵌入模型。在 MimoTable、OTTQA、FetaQA 与 E2E-WTQ 四个公开基准上的实验表明，CGPT 全面且显著地超越了包括 QGpT 在内的检索基线，R@1 平均提升 16.54%。在统一的多领域语料场景下，CGPT 进一步展现了强劲的跨领域泛化能力，即使使用更小的 LLM 生成合成查询仍保持有效性。这些结果说明：将面向语义的局部表构建与对比微调相结合、并以 LLM 生成的监督信号为驱动，是大规模表格检索的一种高效且可扩展的新范式。代码开源： https://github.com/yumeow0122/CGPT"
    },
    {
        "title": "CoNRec: Context-Discerning Negative Recommendation with LLMs",
        "summary": "Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.",
        "entry_id": "http://arxiv.org/abs/2601.15721v1",
        "pub_date": "2026-01-22",
        "translated_summary": "理解用户喜欢什么是相对容易的；然而，理解用户讨厌什么仍是一项充满挑战且研究不足的课题。在当代推荐系统中，对用户负面偏好的研究正日益凸显其重要性。许多平台已引入显式负反馈机制，并借此信号来进一步优化推荐模型。除传统业务指标外，用户体验驱动的指标——尤其是负反馈率——已成为评估系统性能的关键标尺。然而，现有方法大多仅将负反馈作为辅助信号来提升正向推荐，很少有工作直接建模用户的负向兴趣，而这在线下场景中可能极具价值。此外，缘于负反馈数据固有的稀疏性，模型往往由于正向反馈的主导而陷入上下文理解偏差。\n\n为应对上述挑战，我们首次提出一个面向负反馈建模的大语言模型框架，并辅以专门设计的“上下文辨析”模块。框架用语义ID表示取代文本化的物品描述，并通过引入物品级对齐任务来增强大模型对负反馈背后语义语境的把握。进一步，我们设计了渐进式GRPO训练范式，使模型能够动态调谐正向与负向行为上下文的利用力度。此外，我们的研究还发现，传统的“下一负项预测”目标与用户的真实负面偏好存在根本错配问题，这种错配深受系统推荐顺序的影响。为此，我们提出一种新型奖励函数及评估指标，基于多日未来负反馈及其协同信号，以缓解这一错配。"
    },
    {
        "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems",
        "summary": "Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.",
        "entry_id": "http://arxiv.org/abs/2601.15678v1",
        "pub_date": "2026-01-22",
        "translated_summary": "检索增强生成（RAG）系统通过将文档检索与大型语言模型相结合，已被广泛部署。但在涉及隐私的场景中，RAG引入了一种新的隐私风险：攻击者可构造精心设计的查询，逐步从底层语料库中窃取敏感内容。现有研究虽已提出多轮窃取攻击，却依赖启发式策略，缺乏长期提取规划。为此，我们将 RAG 窃取攻击形式化为自适应随机覆盖问题（ASCP）——每轮查询视为旨在最大化条件边际增益（CMG）的概率动作，在不确定性支持下进行有原则的远期规划。\n\n然而，实际 RAG 攻击与 ASCP 融合面临三大挑战：CMG 不可观测、动作空间难以处理以及可行性约束。为解决这些问题，我们在全局攻击者侧维护一个持久状态，以引导攻击。基于此，我们提出 RAGCRAWLER：构建知识图谱表明显露信息，利用该全局状态估计 CMG，并在语义空间规划指向尚未检索区域的查询。\n\n在覆盖多种 RAG 架构及数据集的全面实验中，RAGCRAWLER 始终优于全部基线方法：在限定查询预算内可实现最高 84.4% 的语料覆盖率，比表现最佳基线提升 20.7%；同时保持高语义保真度与强内容重建精度，攻击成本低廉。尤其值得注意的是，即使在采用查询重写与多查询检索策略的最新 RAG 系统面前，RAGCRAWLER 仍保持有效性，展现了其鲁棒性。我们的研究揭示了 RAG 的显著安全缺口，呼吁对 RAG 提供更强有力的防护。"
    },
    {
        "title": "Enhancing guidance for missing data in diffusion-based sequential recommendation",
        "summary": "Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.",
        "entry_id": "http://arxiv.org/abs/2601.15673v1",
        "pub_date": "2026-01-22",
        "translated_summary": "当代序列推荐方法日益复杂，正由传统的分类范式转向以扩散模型为核心的生成范式。然而，指导信号的来源——用户历史序列信息——常常因缺失数据而导致质量下降，进而影响生成效果。现有研究通过剔除局部相似项来缓解这一问题，却忽视了用户兴趣中的“关键转折点”。这些转折点对准确预测用户后续意图至关重要，却未被利用。\n\n为解决此缺陷，我们提出了一种新型反事实注意力调控扩散模型（Counterfactual Attention Regulation Diffusion, CARD）。其核心思路是：放大序列中“关键兴趣转折点”所产生的信号，同时识别并抑制其他噪声。CARD 包含两大组件：\n\n1. 双边 Thompson 采样（Dual-side Thompson Sampling）：在训练过程中动态识别经历了显著兴趣漂移的序列；\n2. 反事实注意力机制：为这些序列中的每一件物品计算其重要性得分，从而完成动态重加权。\n\n借助经过重加权的交互向量，CARD 向扩散模型提供高可信度的指导信号，显著提高生成质量。实验表明，CARD 在真实数据集上表现优越，且计算成本可控。项目代码开源： https://github.com/yanqilong3321/CARD"
    },
    {
        "title": "Blockchain-Based Spectrum Resource Securitization via Semi-Fungible Token-Lock",
        "summary": "As 6G networks evolve, spectrum assets require flexible, dynamic, and efficient utilization, motivating blockchain based spectrum securitization. Existing approaches based on ERC404 style hybrid token models rely on frequent minting and burning during asset transfers, which disrupt token identity continuity and increase on chain overhead. This paper proposes the Semi Fungible Token Lock (SFT Lock) method, a lock/unlock based mechanism that preserves NFT identity and historical traceability while enabling fractional ownership and transferability. By replacing mint/burn operations with deterministic state transitions, SFT Lock ensures consistent lifecycle representation of spectrum assets and significantly reduces on chain operations. Based on this mechanism, a modular smart contract architecture is designed to support spectrum authorization, securitization, and sharing, and a staking mechanism is introduced to enhance asset liquidity. Experimental results on a private Ethereum network demonstrate that, compared with ERC404 style hybrid token models, the proposed method achieves substantial gas savings while maintaining functional correctness and traceability.",
        "entry_id": "http://arxiv.org/abs/2601.15594v1",
        "pub_date": "2026-01-22",
        "translated_summary": "随着 6G 网络的发展，频谱资源需要灵活、动态且高效的利用，这推动了基于区块链的频谱证券化需求。现有的 ERC404 式混合代币模型在资产转移时依赖频繁的铸造与销毁，既打断了代币身份的连续性，又增加了链上开销。本文提出“半同质代币锁定（SFT Lock）”方法，一种基于锁定/解锁的机制：在保留 NFT 身份和历史可追溯性的同时，实现了所有权分割和可转让性。该机制用确定性状态转换取代铸造/销毁操作，使频谱资产的全生命周期在链上保持一致表达，并显著减少链上操作。依托这一机制，论文进一步设计了模块化智能合约架构，支持频谱授权、证券化与共享，并引入质押机制以提升资产流动性。在私有以太坊网络上的实验结果表明，与 ERC404 式混合代币模型相比，SFT Lock 在保证功能正确性和可追溯性的前提下，可显著降低 Gas 开销。"
    },
    {
        "title": "PromptHelper: A Prompt Recommender System for Encouraging Creativity in AI Chatbot Interactions",
        "summary": "Prompting is central to interaction with AI systems, yet many users struggle to explore alternative directions, articulate creative intent, or understand how variations in prompts shape model outputs. We introduce prompt recommender systems (PRS) as an interaction approach that supports exploration, suggesting contextually relevant follow-up prompts. We present PromptHelper, a PRS prototype integrated into an AI chatbot that surfaces semantically diverse prompt suggestions while users work on real writing tasks. We evaluate PromptHelper in a 2x2 fully within-subjects study (N=32) across creative and academic writing tasks. Results show that PromptHelper significantly increases users' perceived exploration and expressiveness without increasing cognitive workload. Qualitative findings illustrate how prompt recommendations help users branch into new directions, overcome uncertainty about what to ask next, and better articulate their intent. We discuss implications for designing AI interfaces that scaffold exploratory interaction while preserving user agency, and release open-source resources to support research on prompt recommendation.",
        "entry_id": "http://arxiv.org/abs/2601.15575v1",
        "pub_date": "2026-01-22",
        "translated_summary": "虽然提示词是与 AI 系统交互的核心，但许多用户在实际写作过程中难以探索新的创作方向、表达创意意图，或理解提示词变化如何影响模型输出。我们提出“提示词推荐系统”（Prompt Recommender Systems，PRS），作为一种支持探索性的交互方式，能为用户提供情境相关的后续提示词建议。我们开发了 PromptHelper——嵌入聊天机器人的 PRS 原型；在用户的真实写作任务中，它会智能地呈现语义多样的提示词建议。\n\n通过一项 2×2 的完全被试内实验（N = 32），我们比较了 PromptHelper 在创意写作与学术写作两种任务中的表现。结果显示，PromptHelper 显著提升了用户对探索空间和表达能力的感知，同时并未增加认知负荷。质性研究进一步表明，提示词推荐能帮助用户分叉出全新的思路、克服“接下来问什么”的不确定感，并更清晰地把创意目标语言化。\n\n我们讨论了如何设计能够支持探索式交互、同时保留用户自主权的 AI 界面，并开源了相关资源，以促进提示词推荐研究。"
    },
    {
        "title": "DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking",
        "summary": "We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.",
        "entry_id": "http://arxiv.org/abs/2601.15518v1",
        "pub_date": "2026-01-21",
        "translated_summary": "我们提出了一种两阶段检索系统，融合多种互补检索方法与可学习的重排器及基于大语言模型（LLM）的再排序，以应对 TREC “话到嘴边”(ToT) 任务。第一阶段采用混合检索，将 LLM 检索、稀疏检索（BM25）与稠密检索（BGE-M3）相结合，并引入“主题感知”多索引稠密检索，把 Wikipedia 语料切分成 24 个主题域。第二阶段分别评测训练后的 LambdaMART 重排模型与 LLM 再排序。为了训练上述模型，我们使用 LLM 生成了 5000 条人工合成的 ToT 查询。最终，通过将混合检索与 Gemini-2.5-flash 再排序结合，系统在测试集上实现召回率 0.66、NDCG@1000 0.41，验证了融合检索的有效性。"
    },
    {
        "title": "Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia and Grokipedia on Controversial Topics",
        "summary": "Online encyclopedias are central to contemporary information infrastructures and have become focal points of debates over ideological bias. Wikipedia, in particular, has long been accused of left-leaning bias, while Grokipedia, an AI-generated encyclopedia launched by xAI, has been framed as a right-leaning alternative. This paper presents a comparative analysis of Wikipedia and Grokipedia on well-established politically contested topics. Specifically, we examine differences in semantic framing, political orientation, and content prioritization. We find that semantic similarity between the two platforms decays across article sections and diverges more strongly on controversial topics than on randomly sampled ones. Additionally, we show that both encyclopedias predominantly exhibit left-leaning framings, although Grokipedia exhibits a more bimodal distribution with increased prominence of right-leaning content. The experimental code is publicly available.",
        "entry_id": "http://arxiv.org/abs/2601.15484v1",
        "pub_date": "2026-01-21",
        "translated_summary": "在线百科全书已成为当代信息基础设施的核心，并逐渐成为意识形态偏见争论的焦点。其中，维基百科长期以来被指责具有左倾偏向；而由 xAI 推出的 AI 生成百科全书 Grokipedia，则被定位为右倾替代方案。本文对两平台在若干已有定论、却具有政治争议的话题上进行了比较分析，重点关注语义框架、政治取向和内容侧重点的差异。研究发现，两平台文章各版块之间的语义相似度随段落深入而迅速下降，在争议话题上的分歧显著高于随机抽样主题。此外，尽管两部百科全书均主要呈现左倾框架，Grokipedia 的分布更趋双峰，右倾内容的占比明显提升。实验代码已公开。"
    },
    {
        "title": "Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering",
        "summary": "The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.",
        "entry_id": "http://arxiv.org/abs/2601.15457v1",
        "pub_date": "2026-01-21",
        "translated_summary": "将大型语言模型（LLMs）引入公共卫生政策领域，可为高效梳理疾控中心（CDC）等机构的庞大监管指导文件库提供变革性手段。然而，LLM 倾向于“幻觉”——即生成看似合理却事实错误的陈述——这在信息完整性不容妥协的高风险场景中，成为技术落地的一大障碍。本实证研究探索检索增强生成（RAG）架构能否通过将生成结果锚定于权威文档来缓解上述风险。具体而言，本文对比了三种设置：纯生成（Vanilla LLM）、基础 RAG，以及采用交叉编码器重排的高级 RAG。实验基于 Mistral-7B-Instruct-v0.2 及 all-MiniLM-L6-v2 嵌入模型，处理 CDC 官方政策分析框架与指导文件的语料，并在精心设计的复杂政策问题基准上评估系统准确性。研究还考察了两种分块策略——递归字符切分与基于 token 的语义切分——对结果的差异。\n\n定量结果显示：与 Vanilla LLM（忠实度 0.347）相比，基础 RAG 显著提升忠实度至 0.621；而高级 RAG 进一步达到 0.797，表明二级检索机制是满足政策问答所需精度的关键。然而，文档分割结构限制仍是多步推理任务的瓶颈。"
    },
    {
        "title": "MEDFORD in a Box: Improvements and Future Directions for a Metadata Description Language",
        "summary": "Scientific research metadata is vital to ensure the validity, reusability, and cost-effectiveness of research efforts. The MEDFORD metadata language was previously introduced to simplify the process of writing and maintaining metadata for non-programmers. However, barriers to entry and usability remain, including limited automatic validation, difficulty of data transport, and user unfamiliarity with text file editing. To address these issues, we introduce MEDFORD-in-a-Box (MIAB), a documentation ecosystem to facilitate researcher adoption and earlier metadata capture. MIAB contains many improvements, including an updated MEDFORD parser with expanded validation routines and BagIt export capability. MIAB also includes an improved VS Code extension that supports these changes through a visual IDE. By simplifying metadata generation, this new tool supports the creation of correct, consistent, and reusable metadata, ultimately improving research reproducibility.",
        "entry_id": "http://arxiv.org/abs/2601.15432v1",
        "pub_date": "2026-01-21",
        "translated_summary": "科学研究元数据对于确保研究的真实性、可重用性与成本效益至关重要。此前提出的 MEDFORD 元数据语言旨在让非程序员能够轻松地编写和维护元数据，但依然存在入门门槛高、可用性不足的问题，包括自动验证有限、数据迁移困难，以及用户不熟悉文本文件编辑。为了解决这些问题，本文推出 MEDFORD-in-a-Box（MIAB）。这是一个综合性文档工具生态，旨在促进研究者快速采纳并尽早记录元数据。MIAB 带来了多项改进：MEDFORD 解析器已更新，支持更完善的验证流程，并可导出 BagIt 格式；同时，配套的 VS Code 扩展也全面升级，以可视化 IDE 的形式支撑上述功能。借助更简洁的元数据生成方式，新工具能够帮助研究人员创建正确、一致且可重用的元数据，最终提升研究的可再现性。"
    },
    {
        "title": "Beyond the Geometric Curse: High-Dimensional N-Gram Hashing for Dense Retrieval",
        "summary": "Why do even the most powerful 7B-parameter embedding models struggle with simple retrieval tasks that the decades old BM25 handles with ease? Recent theory suggests that this happens because of a dimensionality bottleneck. This occurs when we force infinite linguistic nuances into small, fixed-length learned vectors. We developed NUMEN to break this bottleneck by removing the learning process entirely. Instead of training heavy layers to map text to a constrained space, NUMEN uses deterministic character hashing to project language directly onto high-dimensional vectors. This approach requires no training, supports an unlimited vocabulary, and allows the geometric capacity scale as needed. On the LIMIT benchmark, NUMEN achieves 93.90 % Recall@100 at 32,768 dimensions. This makes it the first dense retrieval model to officially surpass the sparse BM25 baseline 93.6 %. Our findings show that the real problem in dense retrieval isn't the architecture, but the embedding layer itself. The solution isn't necessarily smarter training, but simply providing more room to breathe.",
        "entry_id": "http://arxiv.org/abs/2601.15205v1",
        "pub_date": "2026-01-21",
        "translated_summary": "即便是最强的 7B 参数级嵌入模型，为何也会在 BM25 这类已问世数十年的稀疏方法面前败下阵来？最新理论指出，症结在于“维度瓶颈”：必须把无限的语言细节塞进一个又小又固定的可学习向量之中。为此，我们提出 NUMEN——一种彻底抛弃学习方式、以打破瓶颈的检索模型。NUMEN 不再用重参数网络把文本约束到低维空间，而是利用确定性字符级哈希，直接将语言映射到极高维向量；无须训练、词表无限、几何容量按需扩展。在 LIMIT 基准上，NUMEN 在 32,768 维便获得 93.90 % 的 Recall@100，首次让稠密模型正式超越 BM25 的 93.6 % 基线。实验表明，稠密检索的真正瓶颈不在网络结构，而在那一层嵌入本身；解决之道并非更聪明的训练，而只是“给它足够的空间呼吸”。"
    },
    {
        "title": "Supporting Humans in Evaluating AI Summaries of Legal Depositions",
        "summary": "While large language models (LLMs) are increasingly used to summarize long documents, this trend poses significant challenges in the legal domain, where the factual accuracy of deposition summaries is crucial. Nugget-based methods have been shown to be extremely helpful for the automated evaluation of summarization approaches. In this work, we translate these methods to the user side and explore how nuggets could directly assist end users. Although prior systems have demonstrated the promise of nugget-based evaluation, its potential to support end users remains underexplored. Focusing on the legal domain, we present a prototype that leverages a factual nugget-based approach to support legal professionals in two concrete scenarios: (1) determining which of two summaries is better, and (2) manually improving an automatically generated summary.",
        "entry_id": "http://arxiv.org/abs/2601.15182v1",
        "pub_date": "2026-01-21",
        "translated_summary": "尽管大型语言模型（LLM）越来越多地被用于长文档摘要，这一趋势在法律领域却带来严峻挑战——证人陈述摘要的事实准确性至关重要。基于信息“金块”（nugget）的方法已被证明对自动评估摘要质量极有助益。本研究将这一方法迁移到用户侧，探索如何让金块直接赋能终端用户。以往系统虽展示了金块式评估的潜力，但其支撑终端使用者的潜能仍缺乏深入研究。聚焦于法律场景，我们开发了一款原型工具：它以事实级金块为核心，在两种具体情境中为法律工作者提供助力：（1）在两篇摘要中判断哪篇更优；（2）就自动生成的摘要进行手动修正与提升。"
    },
    {
        "title": "From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems",
        "summary": "Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mitigate popularity bias in recommendation models. The SAE is trained to replicate a trained model's behavior while enabling neuron-level interpretability. By introducing synthetic users with strong preferences for either popular or unpopular items, we identify neurons encoding popularity signals through their activation patterns. We then steer recommendations by adjusting the activations of the most biased neurons. Experiments on three public datasets with a sequential recommendation model demonstrate that PopSteer significantly enhances fairness with minimal impact on accuracy, while providing interpretable insights and fine-grained control over the fairness-accuracy trade-off.",
        "entry_id": "http://arxiv.org/abs/2601.15122v1",
        "pub_date": "2026-01-21",
        "translated_summary": "流行度偏差是推荐系统中普遍存在的难题：少数热门商品占据绝大部分曝光机会，而多数长尾商品则长期被忽视。这种失衡不仅降低推荐质量，还可能造成商品曝光的不公平。现有缓解方法虽在一定程度上有所改善，但其内部机制缺乏透明度。本文提出一种事后矫正框架 PopSteer：借助稀疏自编码器（SAE）实现对推荐模型中流行度偏差的可解释分析与干预。SAE 被训练用于复现已训练模型的推荐行为，同时支持神经元层面的可视化与解读。通过构造对热门或非热门商品具有强烈偏好的合成用户，我们从激活模式中定位出编码流行度信号的神经元。随后，通过微调这些最偏置神经元的激活值，实现对推荐结果的导向式修正。在三个公开数据集及序列化推荐模型上的实验表明，PopSteer 在几乎不影响推荐准确率的前提下，显著提升公平性；同时提供可解释洞察，并以细粒度方式平衡公平性与准确性间的取舍。"
    },
    {
        "title": "Incentive-Tuning: Understanding and Designing Incentives for Empirical Human-AI Decision-Making Studies",
        "summary": "AI has revolutionised decision-making across various fields. Yet human judgement remains paramount for high-stakes decision-making. This has fueled explorations of collaborative decision-making between humans and AI systems, aiming to leverage the strengths of both. To explore this dynamic, researchers conduct empirical studies, investigating how humans use AI assistance for decision-making and how this collaboration impacts results. A critical aspect of conducting these studies is the role of participants, often recruited through crowdsourcing platforms. The validity of these studies hinges on the behaviours of the participants, hence effective incentives that can potentially affect these behaviours are a key part of designing and executing these studies. In this work, we aim to address the critical role of incentive design for conducting empirical human-AI decision-making studies, focusing on understanding, designing, and documenting incentive schemes. Through a thematic review of existing research, we explored the current practices, challenges, and opportunities associated with incentive design for human-AI decision-making empirical studies. We identified recurring patterns, or themes, such as what comprises the components of an incentive scheme, how incentive schemes are manipulated by researchers, and the impact they can have on research outcomes. Leveraging the acquired understanding, we curated a set of guidelines to aid researchers in designing effective incentive schemes for their studies, called the Incentive-Tuning Framework, outlining how researchers can undertake, reflect on, and document the incentive design process. By advocating for a standardised yet flexible approach to incentive design and contributing valuable insights along with practical tools, we hope to pave the way for more reliable and generalizable knowledge in the field of human-AI decision-making.",
        "entry_id": "http://arxiv.org/abs/2601.15064v1",
        "pub_date": "2026-01-21",
        "translated_summary": "人工智能已在众多领域彻底变革了决策方式，然而在高风险决策中，人类判断仍不可或缺。这一现状推动了人机协同决策的探索，旨在充分融合双方优势。为了深入理解这一协作机制，研究者开展实证研究，探究人类如何借助 AI 协助进行决策，以及这种协作对最终结果的影响。此类研究的关键在于参与者——他们通常通过众包平台招募。研究的效度依赖参与者的真实行为，因此，可能左右这些行为的激励机制便成为研究设计与实施的核心环节。\n\n本文聚焦于人机决策实证研究中激励机制设计的关键价值，围绕“理解—设计—记录”三大维度展开。通过对现有文献的主题式系统回顾，我们梳理了激励设计的当前做法、主要挑战与未来机遇，并对其中反复出现的规律或主题进行归纳：（1）激励方案的构成要素；（2）研究者如何操纵激励变量；（3）激励设置对研究结果的潜在影响。基于所得洞见，我们提出一套可供研究者在研究中设计高效激励的指南——“激励调适框架”（Incentive-Tuning Framework）。该框架明确了如何启动、反思与记录激励设计流程，倡导一种既标准化又具灵活性的激励设计方法。\n\n借助标准化而灵活的方法论，以及随框架附带的实用工具，我们期望为人机协同决策领域奠定更可靠、更可推广的知识基础。"
    },
    {
        "title": "What Should I Cite? A RAG Benchmark for Academic Citation Prediction",
        "summary": "With the rapid growth of Web-based academic publications, more and more papers are being published annually, making it increasingly difficult to find relevant prior work. Citation prediction aims to automatically suggest appropriate references, helping scholars navigate the expanding scientific literature. Here we present \\textbf{CiteRAG}, the first comprehensive retrieval-augmented generation (RAG)-integrated benchmark for evaluating large language models on academic citation prediction, featuring a multi-level retrieval strategy, specialized retrievers, and generators. Our benchmark makes four core contributions: (1) We establish two instances of the citation prediction task with different granularity. Task 1 focuses on coarse-grained list-specific citation prediction, while Task 2 targets fine-grained position-specific citation prediction. To enhance these two tasks, we build a dataset containing 7,267 instances for Task 1 and 8,541 instances for Task 2, enabling comprehensive evaluation of both retrieval and generation. (2) We construct a three-level large-scale corpus with 554k papers spanning many major subfields, using an incremental pipeline. (3) We propose a multi-level hybrid RAG approach for citation prediction, fine-tuning embedding models with contrastive learning to capture complex citation relationships, paired with specialized generation models. (4) We conduct extensive experiments across state-of-the-art language models, including closed-source APIs, open-source models, and our fine-tuned generators, demonstrating the effectiveness of our framework. Our open-source toolkit enables reproducible evaluation and focuses on academic literature, providing the first comprehensive evaluation framework for citation prediction and serving as a methodological template for other scientific domains. Our source code and data are released at https://github.com/LQgdwind/CiteRAG.",
        "entry_id": "http://arxiv.org/abs/2601.14949v1",
        "pub_date": "2026-01-21",
        "translated_summary": "随着网络学术出版物的迅猛增长，每年发布的论文数量不断上升，寻找相关前期工作变得愈发困难。引文预测旨在自动推荐合适参考文献，帮助学者在膨胀的科学文献中导航。在此，我们发布 **CiteRAG**，这是首个全面集成检索增强生成（RAG）的基准，用于评估大语言模型在学术引文预测中的表现。它采用了多层次检索策略、专用检索器与专用生成器。我们的基准在四个核心方面做出贡献：\n\n(1) 设定两个粒度不同的引文预测任务。任务1面向粗粒度的“列表级”引文预测，任务2聚焦细粒度的“位置级”引文预测。为了支撑这两项任务，我们构建了一个包含7,267个实例的数据集用于任务1，以及8,541个实例的数据集用于任务2，从而同时全面评估检索与生成性能。  \n(2) 采用增量式流水线，构建了一个涵盖海量主要子领域、包含55.4万篇论文的三级大规模语料库。  \n(3) 提出面向引文预测的多级混合RAG方法：通过对比学习微调嵌入模型，以捕捉复杂的引用关系，并配备专用生成模型。  \n(4) 在最新大语言模型上进行了大量实验，涵盖闭源API、开源模型和我们微调后的生成器，证明了框架的有效性。  \n\n我们的开源工具包确保了可复现的评估，专注于学术文献，为引文预测提供了首个全面评估框架，并可作为其他科学领域的方法学模板。源码与数据已发布：https://github.com/LQgdwind/CiteRAG。"
    },
    {
        "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians",
        "summary": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.",
        "entry_id": "http://arxiv.org/abs/2601.16967v1",
        "pub_date": "2026-01-23",
        "translated_summary": "在低收入和中等收入国家（LMICs），由于缺乏及时维护、获取专业技术支持有限，以及制造商——尤其是通过第三方供应商或捐赠渠道获得的设备——几乎不提供支援，大量医疗诊断设备长期处于闲置或故障状态。这一问题加剧了设备停机时间、诊断延误和患者护理质量下降。本研究提出并验证了一种面向生物医学技术人员的AI实时支持平台，用以协助诊断与修复医疗设备。该平台将大型语言模型（LLM）与用户友好的 Web 界面相结合，使影像技师/放射师及生物医学技术人员可输入错误代码或设备异常症状，并获得准确的分步故障排除指南。平台还内置全球同行互助论坛，促进稀有或未记录问题的知识共享与背景补充。以飞利浦 HDI 5000 超声机为概念验证对象，系统对错误代码识别的精准度达 100%，纠正方案推荐的准确率达 80%。研究结果表明，在资源受限环境中，AI 驱动的医疗设备维护支持系统具有可行性和广阔前景，有望显著减少设备停机时间，进而提升医疗服务质量。"
    },
    {
        "title": "Explaining Group Recommendations via Counterfactuals",
        "summary": "Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past interactions would change a group recommendation. We formalize this concept, introduce utility and fairness measures tailored to groups, and design heuristic algorithms, such as Pareto-based filtering and grow-and-prune strategies, for efficient explanation discovery. Experiments on MovieLens and Amazon datasets show clear trade-offs: low-cost methods produce larger, less fair explanations, while other approaches yield concise and balanced results at higher cost. Furthermore, the Pareto-filtering heuristic demonstrates significant efficiency improvements in sparse settings.",
        "entry_id": "http://arxiv.org/abs/2601.16882v1",
        "pub_date": "2026-01-23",
        "translated_summary": "群体推荐系统帮助用户作出集体决策，但往往缺乏透明度，使得成员难以知晓为何某项内容被推荐。现有的解释方法主要针对个人，在面对多重偏好交织的群体场景时显得力不从心。本文提出了一套面向群体的反事实解释框架：通过展示若移除某些具体历史交互会使群体推荐结果发生怎样的变化来解释推荐原因。我们首先对该概念进行形式化定义，随后引入专为群体设计的效用与公平性度量，并基于此设计了启发式算法，包括基于帕累托的前置过滤与“增长-修剪”策略，以实现高效的解释发现。在 MovieLens 和 Amazon 数据集上的实验揭示了清晰的权衡：低成本方法生成的解释规模更大且更不公平；而其他方法虽然成本较高，却能产出简洁且均衡的解释。此外，帕累托过滤启发算法在稀疏场景下显著提升了效率。"
    },
    {
        "title": "From Atom to Community: Structured and Evolving Agent Memory for User Behavior Modeling",
        "summary": "User behavior modeling lies at the heart of personalized applications like recommender systems. With LLM-based agents, user preference representation has evolved from latent embeddings to semantic memory. While existing memory mechanisms show promise in textual dialogues, modeling non-textual behaviors remains challenging, as preferences must be inferred from implicit signals like clicks without ground truth supervision. Current approaches rely on a single unstructured summary, updated through simple overwriting. However, this is suboptimal: users exhibit multi-faceted interests that get conflated, preferences evolve yet naive overwriting causes forgetting, and sparse individual interactions necessitate collaborative signals. We present STEAM (\\textit{\\textbf{ST}ructured and \\textbf{E}volving \\textbf{A}gent \\textbf{M}emory}), a novel framework that reimagines how agent memory is organized and updated. STEAM decomposes preferences into atomic memory units, each capturing a distinct interest dimension with explicit links to observed behaviors. To exploit collaborative patterns, STEAM organizes similar memories across users into communities and generates prototype memories for signal propagation. The framework further incorporates adaptive evolution mechanisms, including consolidation for refining memories and formation for capturing emerging interests. Experiments on three real-world datasets demonstrate that STEAM substantially outperforms state-of-the-art baselines in recommendation accuracy, simulation fidelity, and diversity.",
        "entry_id": "http://arxiv.org/abs/2601.16872v1",
        "pub_date": "2026-01-23",
        "translated_summary": "用户行为建模是个性化应用（如推荐系统）的核心所在。借助基于大语言模型的智能体，用户对偏好的表示已从潜在嵌入演进为语义化记忆。现有记忆机制在文本对话中颇具成效，但在建模非文本行为时仍面临挑战：此时必须从点击这类隐式信号中推断偏好，而缺乏真实标签作为监督。现有方法多依赖单一非结构化摘要，并以简单覆盖的方式进行更新。然而，这种做法并不理想：用户的兴趣是多方面的，易在单一摘要中混淆；偏好会持续变化，而粗暴覆盖会导致遗忘；个体交互稀疏，需要借助协同信号。我们提出了STEAM（结构化且演化的智能体记忆）框架，从组织与更新两个维度重塑智能体记忆。STEAM将偏好拆分为原子式记忆单元，每个单元聚焦单一兴趣维度，并通过显式链接对应到可观察行为。借助协同模式，STEAM将用户间的相似记忆聚为社区，生成原型记忆以实现信号传播。此外，框架还引入自适应演化机制，通过巩固（refining memories）精炼既有记忆，通过生成（capturing emerging interests）捕捉新兴兴趣。在三个真实数据集的实验表明，STEAM在推荐精度、仿真逼真度和多样性方面均显著优于最新基线。"
    },
    {
        "title": "Navigating the Shift: A Comparative Analysis of Web Search and Generative AI Response Generation",
        "summary": "The rise of generative AI as a primary information source presents a paradigm shift from traditional web search. This paper presents a large-scale empirical study quantifying the fundamental differences between the results returned by Google Search and leading generative AI services. We analyze multiple dimensions, demonstrating that AI-generated answers and web search results diverge significantly in their consulted source domains, the typology of these domains (e.g., earned media vs. owned, social), query intent and the freshness of the information provided. We then investigate the role of LLM pre-training as a key factor shaping these differences, analyzing how this intrinsic knowledge base interacts with and influences real-time web search when enabled. Our findings reveal the distinct mechanics of these two information ecosystems, leading to critical observations on the emergent field of Answer Engine Optimization (AEO) and its contrast with traditional Search Engine Optimization (SEO).",
        "entry_id": "http://arxiv.org/abs/2601.16858v1",
        "pub_date": "2026-01-23",
        "translated_summary": "生成式人工智能作为首要信息来源的崛起，标志着信息获取方式从传统网页搜索的范式转变。本文通过大规模实证研究，量化分析了谷歌搜索与主流生成式人工智能服务返回结果的本质差异。我们从多个维度展开分析，发现人工智能生成答案与网页搜索结果在以下方面存在显著差异：引用的源网站域名、域名类型（例如付费媒体、社交媒体、自有媒体）、查询意图以及所提供信息的时效性。在此基础上，我们探讨了大语言模型预训练作为形成这些差异的关键因素，分析了这种内在知识库在与实时网页检索协同工作时如何产生互动与影响。我们的发现揭示了两个截然不同信息生态系统的运作机制，从而对新兴的回答引擎优化（AEO）领域提出重要观察，并指出其与传统搜索引擎优化（SEO）的本质区别。"
    },
    {
        "title": "PI2I: A Personalized Item-Based Collaborative Filtering Retrieval Framework",
        "summary": "Efficiently selecting relevant content from vast candidate pools is a critical challenge in modern recommender systems. Traditional methods, such as item-to-item collaborative filtering (CF) and two-tower models, often fall short in capturing the complex user-item interactions due to uniform truncation strategies and overdue user-item crossing. To address these limitations, we propose Personalized Item-to-Item (PI2I), a novel two-stage retrieval framework that enhances the personalization capabilities of CF. In the first Indexer Building Stage (IBS), we optimize the retrieval pool by relaxing truncation thresholds to maximize Hit Rate, thereby temporarily retaining more items users might be interested in. In the second Personalized Retrieval Stage (PRS), we introduce an interactive scoring model to overcome the limitations of inner product calculations, allowing for richer modeling of intricate user-item interactions. Additionally, we construct negative samples based on the trigger-target (item-to-item) relationship, ensuring consistency between offline training and online inference. Offline experiments on large-scale real-world datasets demonstrate that PI2I outperforms traditional CF methods and rivals Two-Tower models. Deployed in the \"Guess You Like\" section on Taobao, PI2I achieved a 1.05% increase in online transaction rates. In addition, we have released a large-scale recommendation dataset collected from Taobao, containing 130 million real-world user interactions used in the experiments of this paper. The dataset is publicly available at https://huggingface.co/datasets/PI2I/PI2I, which could serve as a valuable benchmark for the research community.",
        "entry_id": "http://arxiv.org/abs/2601.16815v1",
        "pub_date": "2026-01-23",
        "translated_summary": "从海量候选项中高效地遴选相关内容，是现代推荐系统面临的关键挑战。传统的 item-to-item 协同过滤（CF）与双塔模型，由于采用统一的截断策略及滞后的 user–item 交互建模，往往难以刻画复杂的用户-物品关系。为此，我们提出 Personalized Item-to-Item（PI2I），一种两阶段检索框架，显著提升了 CF 在个性化方面的能力。在第一阶段「索引构建阶段」（IBS）中，我们通过放宽截断阈值来优化检索池，以最大化命中率（Hit Rate），进而暂时保留更多潜在兴趣物品。第二阶段「个性化检索阶段」（PRS）引入交互式打分模型，克服了内积计算的限制，能够更丰富地建模精细的用户-物品交互。此外，我们以触发-目标（item-to-item）关系构建负样本，使离线训练与在线推断保持一致。大规模真实数据集的离线实验表明，PI2I 的表现优于传统 CF 方法，并与双塔模型势均力敌。在淘宝「猜你喜欢」场景上线后，PI2I 为在线交易转化率带来了 1.05% 的提升。我们还公开发布了源自淘宝的大规模推荐数据集，包含 1.3 亿条真实用户交互，用于本研究的实验验证，数据下载地址：https://huggingface.co/datasets/PI2I/PI2I，可为学界提供宝贵基准。"
    },
    {
        "title": "LLM-powered Real-time Patent Citation Recommendation for Financial Technologies",
        "summary": "Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval and citation recommendation methods typically rely on static indexes or periodic retraining, which limits their ability to operate effectively in such dynamic settings. In this study, we propose a real-time patent citation recommendation framework designed for large and fast-changing financial patent corpora. Using a dataset of 428,843 financial patents granted by the China National Intellectual Property Administration (CNIPA) between 2000 and 2024, we build a three-stage recommendation pipeline. The pipeline uses large language model (LLM) embeddings to represent the semantic content of patent abstracts, applies efficient approximate nearest-neighbor search to construct a manageable candidate set, and ranks candidates by semantic similarity to produce top-k citation recommendations. In addition to improving recommendation accuracy, the proposed framework directly addresses the dynamic nature of patent systems. By using an incremental indexing strategy based on hierarchical navigable small-world (HNSW) graphs, newly issued patents can be added without rebuilding the entire index. A rolling day-by-day update experiment shows that incremental updating improves recall while substantially reducing computational cost compared with rebuild-based indexing. The proposed method also consistently outperforms traditional text-based baselines and alternative nearest-neighbor retrieval approaches.",
        "entry_id": "http://arxiv.org/abs/2601.16775v1",
        "pub_date": "2026-01-23",
        "translated_summary": "金融创新的加速伴随着专利申请的激增，使得及时而全面的在先技术检索愈发困难。这一问题在金融科技领域尤为突出：技术迭代迅速，专利库持续膨胀，且每当出现新申请时，已有引用推荐系统就必须随之更新。现有专利检索与引用推荐方法多依赖静态索引或周期性重训，难以胜任如此动态的场景。针对这一挑战，本文提出一套面向大规模、高速变化金融专利库的实时引用推荐框架。基于中国国家知识产权局（CNIPA）2000–2024 年间授权的全部 428,843 件金融专利，我们构建了三阶段推荐流水线：首先利用大语言模型（LLM）嵌入专利摘要的语义表示；随后通过高效的近似最近邻搜索生成可控候选集；最后按语义相似度排序，输出 top-k 引用结果。除提升推荐精度外，该框架直接解决了专利系统的动态性。借助基于分层可导航小世界（HNSW）图的增量索引策略，新授权专利可直接插入，无需重建整个索引。逐日滚动更新实验表明，增量索引不仅提升了召回率，还显著减少了计算开销。与传统文本检索基线和其他近邻检索方法相比，本文方法始终保持优势。"
    },
    {
        "title": "Better Generalizing to Unseen Concepts: An Evaluation Framework and An LLM-Based Auto-Labeled Pipeline for Biomedical Concept Recognition",
        "summary": "Generalization to unseen concepts is a central challenge due to the scarcity of human annotations in Mention-agnostic Biomedical Concept Recognition (MA-BCR). This work makes two key contributions to systematically address this issue. First, we propose an evaluation framework built on hierarchical concept indices and novel metrics to measure generalization. Second, we explore LLM-based Auto-Labeled Data (ALD) as a scalable resource, creating a task-specific pipeline for its generation. Our research unequivocally shows that while LLM-generated ALD cannot fully substitute for manual annotations, it is a valuable resource for improving generalization, successfully providing models with the broader coverage and structural knowledge needed to approach recognizing unseen concepts. Code and datasets are available at https://github.com/bio-ie-tool/hi-ald.",
        "entry_id": "http://arxiv.org/abs/2601.16711v1",
        "pub_date": "2026-01-23",
        "translated_summary": "鉴于人工标注在提及无关型生物医学概念识别（MA-BCR）中的稀缺，如何泛化到未见概念成为核心挑战。本研究通过两大贡献系统性地应对这一问题。首先，我们提出一套评估框架，包括层级概念索引与全新指标，以量化模型泛化能力。其次，我们探索利用大语言模型自动生成标注数据（ALD），并构建面向任务的生成管线，实现可扩展的知识资源补充。实验充分表明，尽管LLM生成的ALD无法完全替代人工注释，但其显著提升模型泛化性能，可提供更广泛的覆盖及结构化知识，使模型更趋近识别未见概念。代码与数据集开源地址：https://github.com/bio-ie-tool/hi-ald。"
    },
    {
        "title": "PRISM: Purified Representation and Integrated Semantic Modeling for Generative Sequential Recommendation",
        "summary": "Generative Sequential Recommendation (GSR) has emerged as a promising paradigm, reframing recommendation as an autoregressive sequence generation task over discrete Semantic IDs (SIDs), typically derived via codebook-based quantization. Despite its great potential in unifying retrieval and ranking, existing GSR frameworks still face two critical limitations: (1) impure and unstable semantic tokenization, where quantization methods struggle with interaction noise and codebook collapse, resulting in SIDs with ambiguous discrimination; and (2) lossy and weakly structured generation, where reliance solely on coarse-grained discrete tokens inevitably introduces information loss and neglects items' hierarchical logic. To address these issues, we propose a novel generative recommendation framework, PRISM, with Purified Representation and Integrated Semantic Modeling. Specifically, to ensure high-quality tokenization, we design a Purified Semantic Quantizer that constructs a robust codebook via adaptive collaborative denoising and hierarchical semantic anchoring mechanisms. To compensate for information loss during quantization, we further propose an Integrated Semantic Recommender, which incorporates a dynamic semantic integration mechanism to integrate fine-grained semantics and enforces logical validity through a semantic structure alignment objective. PRISM consistently outperforms state-of-the-art baselines across four real-world datasets, demonstrating substantial performance gains, particularly in high-sparsity scenarios.",
        "entry_id": "http://arxiv.org/abs/2601.16556v1",
        "pub_date": "2026-01-23",
        "translated_summary": "生成式序列推荐（GSR）作为一种新兴范式，将推荐任务重塑为基于离散语义 ID（SID）的自回归序列生成，这些 SID 一般通过基于码本的量化获得。尽管这一范式在统一检索与排序方面显示出巨大潜力，现有 GSR 框架仍面临两大关键局限：（1）失真且不稳定的高维语义标记化——量化方法难以应对交互噪声与码本崩溃，导致 SID 判别性模糊；（2）信息缺失且结构薄弱的生成过程——仅依赖粗粒度离散标记会不可避免地损失信息，并忽视物品的层次结构逻辑。\n\n针对上述问题，本文提出 PRISM 框架，通过“提纯表征”和“整合语义建模”实现更高质量的生成式推荐。首先，为保证高保真标记化，我们设计了“提纯语义量化器”，利用自适应协同去噪与层次语义锚定机制构建鲁棒码本。其次，为弥补量化阶段的信息损失，我们进一步提出“整合语义推荐器”，引入动态语义整合机制融合细粒度语义，并通过语义结构对齐目标确保逻辑一致性。在四个真实数据集上的广泛实验表明，PRISM 不仅在整体性能上全面超越现有最强基线，尤其在极端稀疏场景下展现了显著增益。"
    },
    {
        "title": "LLM-based Semantic Search for Conversational Queries in E-commerce",
        "summary": "Conversational user queries are increasingly challenging traditional e-commerce platforms, whose search systems are typically optimized for keyword-based queries. We present an LLM-based semantic search framework that effectively captures user intent from conversational queries by combining domain-specific embeddings with structured filters. To address the challenge of limited labeled data, we generate synthetic data using LLMs to guide the fine-tuning of two models: an embedding model that positions semantically similar products close together in the representation space, and a generative model for converting natural language queries into structured constraints. By combining similarity-based retrieval with constraint-based filtering, our framework achieves strong precision and recall across various settings compared to baseline approaches on a real-world dataset.",
        "entry_id": "http://arxiv.org/abs/2601.16492v1",
        "pub_date": "2026-01-23",
        "translated_summary": "传统电子商务平台的搜索系统通常为关键词优化，面对日益复杂的对话式用户查询已力不从心。本文提出一种基于大型语言模型的语义搜索框架，利用领域特定的嵌入表示与结构化过滤器相结合，从对话中精准捕捉用户真实意图。为缓解标注数据不足的挑战，我们借助大型语言模型生成合成数据，并据此微调两个模型：其一为嵌入模型，将语义相似的商品在表示空间中拉近距离；其二为生成模型，负责将自然语言查询转换为结构化约束条件。通过把基于相似度的召回与基于约束条件的重排序相结合，该框架在真实数据集上的多个设定中均显著优于基线方法，取得更高的准确率和召回率。"
    },
    {
        "title": "Segregation Before Polarization: How Recommendation Strategies Shape Echo Chamber Pathways",
        "summary": "Social media platforms facilitate echo chambers through feedback loops between user preferences and recommendation algorithms. While algorithmic homogeneity is well-documented, the distinct evolutionary pathways driven by content-based versus link-based recommendations remain unclear. Using an extended dynamic Bounded Confidence Model (BCM), we show that content-based algorithms--unlike their link-based counterparts--steer social networks toward a segregation-before-polarization (SbP) pathway. Along this trajectory, structural segregation precedes opinion divergence, accelerating individual isolation while delaying but ultimately intensifying collective polarization. Furthermore, we reveal a paradox in information sharing: Reposting increases the number of connections in the network, yet it simultaneously reinforces echo chambers because it amplifies small, latent opinion differences that would otherwise remain inconsequential. These findings suggest that mitigating polarization requires stage-dependent algorithmic interventions, shifting from content-centric to structure-centric strategies as networks evolve.",
        "entry_id": "http://arxiv.org/abs/2601.16457v1",
        "pub_date": "2026-01-23",
        "translated_summary": "社交媒体平台通过用户偏好与推荐算法之间的反馈回路形成了信息回音室。虽然算法同质化现象已被充分记录，但由基于内容的推荐与基于链接的推荐所驱动的差异化演化路径仍不清晰。本研究借助扩展的动态 Bounded Confidence 模型（BCM）发现：与基于链接的推荐相比，基于内容的算法会将社交网络推向“先隔离后极化”（segregation-before-polarization, SbP）的路径。在此路径上，结构隔离先于观点分歧出现，它不仅加速了个体的孤立，而且虽将集体极化的发生延后，却最终使其更加剧烈。此外，我们还发现信息分享中的一个悖论：转发行为虽然增加了网络中的连接数量，却通过放大原本微不足道且潜在的微小意见差异而强化了回音室效应。上述结果表明，要缓解极化，需要在网络演化的不同阶段采用差异化算法干预：随着网络不断演化，干预策略应由以内容为中心转向以结构为中心。"
    },
    {
        "title": "Self-Manager: Parallel Agent Loop for Long-form Deep Research",
        "summary": "Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.",
        "entry_id": "http://arxiv.org/abs/2601.17879v1",
        "pub_date": "2026-01-25",
        "translated_summary": "长篇深度研究要求在广阔的时间范围内开展多维度调查，以形成综合报告。在处理这类复杂任务时，现有智能体通过在子任务层级进行上下文管理来缓解线性上下文累积和信息丢失问题。然而，它们仍固守在单一线性上下文窗口与顺序执行范式之内，导致任务间相互干扰和阻塞行为，严重限制了规模化能力与适应性。\n\n针对这一瓶颈，本文提出 Self-Manager，一种可并行执行的智能体循环机制，支持异步与并发运算。主线程能够创建带有独立上下文的多个子线程，并可通过「线程控制块」（Thread Control Blocks）进行迭代式管理，从而实现更加聚焦、灵活的并行代理执行。\n\n为验证其有效性，我们在 DeepResearch Benchmark 上对 Self-Manager 进行基准测试。结果表明，Self-Manager 在所有评测维度均一致超越现有的单智能体循环基线。此外，我们通过大量分析实验，系统验证了 Self-Manager 设计选择的必要性，并展示了其在上下文容量、计算效率及泛化能力方面的显著优势。"
    },
    {
        "title": "Unleashing the Potential of Sparse Attention on Long-term Behaviors for CTR Prediction",
        "summary": "In recent years, the success of large language models (LLMs) has driven the exploration of scaling laws in recommender systems. However, models that demonstrate scaling laws are actually challenging to deploy in industrial settings for modeling long sequences of user behaviors, due to the high computational complexity of the standard self-attention mechanism. Despite various sparse self-attention mechanisms proposed in other fields, they are not fully suited for recommendation scenarios. This is because user behaviors exhibit personalization and temporal characteristics: different users have distinct behavior patterns, and these patterns change over time, with data from these users differing significantly from data in other fields in terms of distribution. To address these challenges, we propose SparseCTR, an efficient and effective model specifically designed for long-term behaviors of users. To be precise, we first segment behavior sequences into chunks in a personalized manner to avoid separating continuous behaviors and enable parallel processing of sequences. Based on these chunks, we propose a three-branch sparse self-attention mechanism to jointly identify users' global interests, interest transitions, and short-term interests. Furthermore, we design a composite relative temporal encoding via learnable, head-specific bias coefficients, better capturing sequential and periodic relationships among user behaviors. Extensive experimental results show that SparseCTR not only improves efficiency but also outperforms state-of-the-art methods. More importantly, it exhibits an obvious scaling law phenomenon, maintaining performance improvements across three orders of magnitude in FLOPs. In online A/B testing, SparseCTR increased CTR by 1.72\\% and CPM by 1.41\\%. Our source code is available at https://github.com/laiweijiang/SparseCTR.",
        "entry_id": "http://arxiv.org/abs/2601.17836v1",
        "pub_date": "2026-01-25",
        "translated_summary": "近年来，大语言模型（LLMs）的成功促使人们重新审视推荐系统中的规模扩展律。然而，受限于标准自注意力机制高昂的计算复杂度，具备规模扩展性的模型在面向工业场景对长序列用户行为建模时难以实际落地。尽管诸多领域已提出多种稀疏自注意力机制，却未能充分契合推荐任务。根本原因在于用户行为具有个性化与时序特征：不同用户的行为模式差异显著，且随时间动态演化，导致其数据分布在跨领域间存在显著偏移。\n\n为破解上述难题，我们提出 SparseCTR——一个专门面向长周期用户行为的高效、有效模型。具体而言，首先以个性化方式将行为序列切分为块，避免割裂连续行为，同时实现序列的并行处理。基于这些“行为块”，我们设计了一种三分支稀疏自注意力机制，以联合建模用户的全局兴趣、兴趣迁移与短期兴趣。此外，借助可学习的头专属偏置系数，我们构建了复合相对时序编码，更精准地捕捉用户行为中的序列与周期性依赖。\n\n大量实验表明，SparseCTR 不仅显著提升了计算效率，还全面优于当前最优方法；更重要的是，模型呈现出明显的规模扩展律，在三个数量级的 FLOPs 范围内仍持续提升性能。在线 A/B 测试结果显示，SparseCTR 将 CTR 提升 1.72%，CPM 提升 1.41%。源代码已开源：https://github.com/laiweijiang/SparseCTR。"
    },
    {
        "title": "OwlerLite: Scope- and Freshness-Aware Web Retrieval for LLM Assistants",
        "summary": "Browser-based language models often use retrieval-augmented generation (RAG) but typically rely on fixed, outdated indices that give users no control over which sources are consulted. This can lead to answers that mix trusted and untrusted content or draw on stale information. We present OwlerLite, a browser-based RAG system that makes user-defined scopes and data freshness central to retrieval. Users define reusable scopes-sets of web pages or sources-and select them when querying. A freshness-aware crawler monitors live pages, uses a semantic change detector to identify meaningful updates, and selectively re-indexes changed content. OwlerLite integrates text relevance, scope choice, and recency into a unified retrieval model. Implemented as a browser extension, it represents a step toward more controllable and trustworthy web assistants.",
        "entry_id": "http://arxiv.org/abs/2601.17824v1",
        "pub_date": "2026-01-25",
        "translated_summary": "现有的浏览器端语言模型通常使用检索增强生成（RAG），但普遍依赖固定、过期的索引，用户无法决定检索应参考哪些来源。这导致模型给出的答案常常混杂可信与不可信信息，或使用过时的内容。我们提出 OwlerLite——一个以浏览器为中心的 RAG 系统，把用户自定义范围和数据的“新鲜度”置于检索核心。用户可以预先建立可复用的“范围组”（即网页或来源集合），并在提问时直接选用。系统内部的 freshness-aware 爬虫会实时监控页面变动；通过语义变化检测器识别有意义的内容更新，并只对发生实质变动的内容重新索引。OwlerLite 将文本相关性、范围选择以及时效性统一纳入检索评分。作为浏览器扩展实现，OwlerLite 朝着更可控、更可信的网络助理迈进了一步。"
    },
    {
        "title": "Token-Weighted Multi-Target Learning for Generative Recommenders with Curriculum Learning",
        "summary": "Generative recommender systems have recently attracted attention by formulating next-item prediction as an autoregressive sequence generation task. However, most existing methods optimize standard next-token likelihood and implicitly treat all tokens as equally informative, which is misaligned with semantic-ID-based generation. Accordingly, we propose two complementary information-gain-based token-weighting strategies tailored to generative recommendation with semantic IDs. Front-Greater Weighting captures conditional semantic information gain by prioritizing early tokens that most effectively reduce candidate-item uncertainty given their prefixes and encode coarse semantics. Frequency Weighting models marginal information gain under long-tailed item and token distributions, upweighting rare tokens to counteract popularity bias. Beyond individual strategies, we introduce a multi-target learning framework with curriculum learning that jointly optimizes the two token-weighted objectives alongside standard likelihood, enabling stable optimization and adaptive emphasis across training stages. Extensive experiments on benchmark datasets show that our method consistently outperforms strong baselines and existing token-weighting approaches, with improved robustness, strong generalization across different semantic-ID constructions, and substantial gains on both head and tail items. Code is available at https://github.com/CHIUWEINING/Token-Weighted-Multi-Target-Learning-for-Generative-Recommenders-with-Curriculum-Learning.",
        "entry_id": "http://arxiv.org/abs/2601.17787v1",
        "pub_date": "2026-01-25",
        "translated_summary": "生成式推荐系统通过将下一个物品的预测建模为自回归序列生成任务而备受关注。然而，现有方法大多仅优化标准下一个 token 的似然损失，并将所有 token 视作同等信息量，这与基于语义 ID（semantic-ID）的生成任务并不匹配。为解决这一问题，我们提出了两种互补的基于信息增益的 token 加权策略：\n\n1. **Front-Greater 加权（FGW）**：通过优先给予能最大程度减少候选物品不确定度的早期 token 更高权重，从而捕获给定前缀条件下的语义信息增益，并编码粗略的语义信息。\n\n2. **频率加权（FLW）**：针对长尾物品与 token 分布，建模边际信息增益，对罕见但信息量大的 token 赋予更高权重，以缓解流行度偏差。\n\n除单个策略外，我们引入了一个多任务学习框架，采用课程式学习（curriculum learning）将两种 token 加权目标与标准似然损失联合优化，实现训练过程中的稳定优化与自适应重点。在多个公开基准数据集上的大量实验表明，本文方法在鲁棒性、对不同语义 ID 构建方式的泛化能力以及头部/尾部物品性能上均显著优于主流基线和现有 token 加权方案。\n\n代码与模型已开源：https://github.com/CHIUWEINING/TOKEN-Weighted-Multi-Target-Learning-For-Generative-Recommenders-with-Curriculum-Learning"
    },
    {
        "title": "LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval",
        "summary": "Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.",
        "entry_id": "http://arxiv.org/abs/2601.17692v1",
        "pub_date": "2026-01-25",
        "translated_summary": "法规检索对法律辅助和司法决策支持至关重要，然而真实世界的法律查询往往含蓄、多议题且用口语化或不完整的形式表达。这些特性使得传统的检索增强生成（RAG）流程难以召回判决所需的精确法规条文。\n\n稠密（dense）检索器主要关注查询的字面形式，而轻量级重排序器又缺乏判定法规适用性所需的法律推理能力。我们提出 **LegalMALR**，一个将多智能体查询理解系统（MAS）与基于零样本大语言模型的重排序模块（LLM Reranker）相结合的新型检索框架。MAS 为查询生成多样化、具法律依据的改写，并进行多轮稠密检索以扩大候选覆盖面。为稳定大语言模型改写带来的随机行为，我们用广义强化策略优化（GRPO）对统一的 MAS 策略进行优化。随后，LLM 重排序器对累积候选集进行自然语言法律推理，以生成最终排序。\n\n此外，我们构建了 **CSAID** 数据集，其中包含 118 条困难的法规查询及多项法律标签注释，并在 CSAID 与公开基准 STARD 上对 LegalMALR 进行评估。实验表明，无论是在分布内还是分布外场景下，LegalMALR 均显著优于现有 RAG 基线，证明了多视角查询解释、基于强化学习的策略优化以及大模型重排序在法规检索中的有效结合。"
    },
    {
        "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
        "summary": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.",
        "entry_id": "http://arxiv.org/abs/2601.17690v1",
        "pub_date": "2026-01-25",
        "translated_summary": "音频指纹技术为声学信号提供一种可辨识的表征，可用于后续识别与检索系统。为了获得具有辨别力的描述，输入音频通常被划分为更短的时间段，从而提取并分析局部声学特征。现代神经网络方法一般在固定时长的短片段上操作，然而片段长度的选择往往依据经验，缺乏深入探讨。本文研究片段长度对音频指纹性能的影响。我们将现有的神经指纹架构扩展至可适配多种片段长度，并在不同片段长度与不同查询时长下评估检索准确率。实验结果显示，较短片段（0.5 秒）通常表现最佳。此外，我们评估了大语言模型在推荐最优片段长度方面的能力，发现 GPT-5-mini 在三大模型、五种考量下始终能提供最优建议。研究结果为大 规模神经音频检索系统选取片段时长提供了切实可行的指导。"
    },
    {
        "title": "Memento: Towards Proactive Visualization of Everyday Memories with Personal Wearable AR Assistant",
        "summary": "We introduce Memento, a conversational AR assistant that permanently captures and memorizes user's verbal queries alongside their spatiotemporal and activity contexts. By storing these \"memories,\" Memento discovers connections between users' recurring interests and the contexts that trigger them. Upon detection of similar or identical spatiotemporal activity, Memento proactively recalls user interests and delivers up-to-date responses through AR, seamlessly integrating AR experience into their daily routine. Unlike prior work, each interaction in Memento is not a transient event, but a connected series of interactions with coherent long--term perspective, tailored to the user's broader multimodal (visual, spatial, temporal, and embodied) context. We conduct preliminary evaluation through user feedbacks with participants of diverse expertise in immersive apps, and explore the value of proactive context-aware AR assistant in everyday settings. We share our findings and challenges in designing a proactive, context-aware AR system.",
        "entry_id": "http://arxiv.org/abs/2601.17622v1",
        "pub_date": "2026-01-24",
        "translated_summary": "我们推出了一款名为 Memento 的对话式增强现实（AR）助手。它能够永久性地记录用户在提出语音请求时的时空环境与活动情境，并将这些“记忆”一并保存。借助这些记忆，Memento 可以发现用户反复出现的兴趣点与触发此类兴趣的情境之间的关联。一旦检测到相似或一致的时空活动，Memento 便能主动回忆相关兴趣，并以 AR 方式提供最新的响应，无缝融入用户的日常。与既有工作不同，Memento 中的每一次交互都不是转瞬即逝的事件，而是以用户的多模态（视觉、空间、时间及体感）情境为纽带，构建起连续、一致、立足长期视角的互动序列。我们通过邀请了具备不同沉浸式应用经验的参与者进行用户反馈，完成了初步评估，并探讨了日常场景中主动式情境感知 AR 助手的价值。在此基础上，我们分享了构建主动情境感知 AR 系统过程中发现的经验与面临的挑战。"
    },
    {
        "title": "Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests",
        "summary": "LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.",
        "entry_id": "http://arxiv.org/abs/2601.17617v1",
        "pub_date": "2026-01-24",
        "translated_summary": "I'm ready — how can I help you?"
    },
    {
        "title": "Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts",
        "summary": "URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.",
        "entry_id": "http://arxiv.org/abs/2601.17601v1",
        "pub_date": "2026-01-24",
        "translated_summary": "URL 像一座座桥梁，把社交媒体平台和更广泛的网络连接起来，使用户生成内容与外部信息资源相互贯通。在 Twitter（X）上，大约每五条推文就含有一个 URL，凸显了超链接在信息传播中的核心作用。既往研究多聚焦于作者为何发布 URL，但这类以作者为中心的意图在实际中难以观测。为了便于下游大规模应用，本文转向以读者为中心的视角，探讨用户如何理解帖子中所含超链接的意图。我们采用混合方法构建超链接意图分类体系：先行基于大规模众包标注，以自下而上的数据驱动方式聚类；再借助大语言模型提炼类别命名与精确定义。最终分类体系包含 6 个顶层类别与 26 个细粒度意图类，全面覆盖多样的传播目的。在此体系下，我们对 1000 条用户帖子进行标注与分析，发现“广告宣传”“观点论证”与“资源分享”是最常见的三种意图。该分类体系为意图感知的信息检索和自然语言处理应用奠定了理论与数据基础，可提升社交媒体内容的检索、推荐与理解精度。"
    },
    {
        "title": "Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations",
        "summary": "Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.",
        "entry_id": "http://arxiv.org/abs/2601.17569v1",
        "pub_date": "2026-01-24",
        "translated_summary": "个性化对于使大型语言模型（LLM）输出与个体用户的偏好和背景知识保持一致至关重要。目前最先进的方案采用检索增强：先从用户画像中提取相关上下文，再交由 LLM 处理。然而，这类方法不得不在把私密数据暴露给云端模型的风险与依赖能力较弱的本地模型之间权衡。\n\n我们提出 **P³（Private Personalized Prompting）**，一个交互式框架，可在不向云侧 LLM 泄露私密画像的前提下实现高质量个性化。P³ 的工作原理如下：  \n1. 云端大模型仅根据用户原始查询生成 k 个“草稿 token”；  \n2. 端侧小型模型在本地检索并读取用户的私密画像后，对这些草稿进行评估和改写，使其更贴合用户偏好；  \n3. 该步骤往复进行，直到生成结束 token。\n\n在新近发布的个性化问答基准 LaMP-QA（包含 3 个问答数据集）上的实验表明，P³ 在保持云端模型无个人信息暴露的同时，性能稳定优于非个性化云端模型和本地化个性化基线，平均提升 7.4%–9%（统计显著）。更重要的是，P³ 达到了一种“泄密上限”情景（即服务器拥有完整画像）效果的 90.3%–95.7%。\n\n隐私分析（包括可关联性攻击和属性推理攻击）显示，P³ 的隐私泄露水平与完全不暴露个人信息的云侧模型基本相当，仅增加 1.5%–3.5% 的边际泄露。此外，端侧模型仅需生成总 token 的 9.2%，即可部署于边缘设备。综上，P³ 为隐私增强的个性化生成提供了一种实用、高效的解决方案。"
    },
    {
        "title": "Real-Time Trend Prediction via Continually-Aligned LLM Query Generation",
        "summary": "Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.",
        "entry_id": "http://arxiv.org/abs/2601.17567v1",
        "pub_date": "2026-01-24",
        "translated_summary": "在低流量搜索环境中进行热点新闻探测时，会遭遇根深蒂固的冷启动困境：查询量稀少导致系统难以识别新兴或长尾趋势。现有的基于关键词频率或查询峰值的方法在这些稀疏场景下天然迟缓、收效甚微，往往滞后于现实世界的注意力迁移。我们提出 RTTP（Real-Time Trending Prediction），一种新的实时热点预测框架，可直接从新闻内容生成搜索查询，而无需等待用户发起。RTTP 采用持续学习大语言模型（CL-LLM），将新闻帖转化为搜索式查询，并综合互动强度与创作者权威度进行打分，使得在搜索量尚未形成之际就能提前发现趋势。为了在模型升级过程中保持适应性且不损伤推理能力，我们提出 Mix-Policy DPO——一种基于偏好的持续学习方法，融合“在线策略稳定”与“离线策略新颖”，显著缓解灾难性遗忘。RTTP 已在 Facebook 和 Meta AI 产品全面上线：相比业界基线，尾部热点检测 precision@500 提升 91.4%，查询生成准确率提升 19%，并在多周在线训练后仍保持稳定表现。本工作表明，经过持续对齐更新的 LLM 合成搜索信号，可为低流量搜索环境解锁及时的趋势洞察。"
    },
    {
        "title": "Pipeline Inspection, Visualization, and Interoperability in PyTerrier",
        "summary": "PyTerrier provides a declarative framework for building and experimenting with Information Retrieval (IR) pipelines. In this demonstration, we highlight several recent pipeline operations that improve their ability to be programmatically inspected, visualized, and integrated with other tools (via the Model Context Protocol, MCP). These capabilities aim to make it easier for researchers, students, and AI agents to understand and use a wide array of IR pipelines.",
        "entry_id": "http://arxiv.org/abs/2601.17502v1",
        "pub_date": "2026-01-24",
        "translated_summary": "PyTerrier 是一个用于构建与实验信息检索（IR）管道的声明式框架。本次演示重点介绍了近期新增的若干管道操作，这些操作使研究者、学生乃至 AI 智能体能够更容易地程序化检查、可视化 IR 管道，并通过模型上下文协议（MCP）将它们与其他工具无缝集成。"
    },
    {
        "title": "To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval",
        "summary": "Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR",
        "entry_id": "http://arxiv.org/abs/2601.17500v1",
        "pub_date": "2026-01-24",
        "translated_summary": "学会稀疏检索（LSR）方法通过为查询和文档构建可借助倒排索引高效搜索的稀疏词元表示。现有 LSR 研究几乎完全采用不分大小写的骨干模型，其词汇表忽略大小写差异，从而减轻了词汇失配。然而，前沿大模型通常只有大小写敏感版本。尽管领域已发生这一转变，骨干模型的大小写设定对 LSR 的影响尚未被探讨，这给该方法的可持续性带来潜在风险。为此，我们在多个数据集上系统评估同一骨干模型的大小写敏感与大小写不敏感版本组合，比较其 LSR 适用性。实验表明，默认情况下使用大小写敏感骨干的 LSR 性能显著弱于不分大小写版本；但若预先对文本统一小写处理，这一差距可完全消除。进一步词元级分析发现，经过小写后，大小写敏感模型的词汇几乎彻底抑制了带大小写词元，行为趋同于不分大小写模型，从而解释了性能恢复。该结果拓宽了最新大小写敏感模型在 LSR 中的应用范围，并为集成更强大的骨干架构铺平道路。项目完整代码与实现：https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR"
    },
    {
        "title": "PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems",
        "summary": "In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.",
        "entry_id": "http://arxiv.org/abs/2601.17495v1",
        "pub_date": "2026-01-24",
        "translated_summary": "在许多已部署的系统中，新的文本输入通过检索过往的相似案例来处理，例如在数字政务平台中路由并回复市民留言时出现的情形。当这些系统失灵时，问题往往不是语言模型本身，而是嵌入空间中最近的邻域对应了错误的案例。现代机器学习系统越来越依赖于由大型预训练模型和句子编码器生成的固定、高维嵌入。在实际部署场景中，标签稀缺，领域随时间漂移，且重新训练底层编码器既昂贵又不可行。因此，下游性能在很大程度上取决于嵌入几何。然而，未经处理的原始嵌入往往与最近邻检索、相似性搜索，以及直接以嵌入为输入的轻量级分类器所需的局部邻域结构对不齐。\n\n我们提出 PEARL（Prototype-Enhanced Aligned Representation Learning，原型增强的对齐表示学习），一种标签高效的策略，利用有限的监督数据使嵌入向类别原型软对齐。该方法重塑局部邻域几何，同时保持维度不变，避免激进的投影或坍缩。其目标是在纯无监督后处理（提升有限且不一致）和完全监督投影（需要大量标注）之间架起一座桥梁。\n\n我们在受控的标签稀缺到充足区间对 PEARL 进行评估。在标签极端稀缺的条件下，PEARL 显著提升了局部邻域质量，相对于原始嵌入带来 25.7% 的性能增益，较当前最强的无监督后处理也高出 21.1%，而这正是基于相似性的系统最易出错的关键场景。"
    },
    {
        "title": "Towards Fair Large Language Model-based Recommender Systems without Costly Retraining",
        "summary": "Large Language Models (LLMs) have revolutionized Recommender Systems (RS) through advanced generative user modeling. However, LLM-based RS (LLM-RS) often inadvertently perpetuates bias present in the training data, leading to severe fairness issues. Addressing these fairness problems in LLM-RS faces two significant challenges. 1) Existing debiasing methods, designed for specific bias types, lack the generality to handle diverse or emerging biases in real-world applications. 2) Debiasing methods relying on retraining are computationally infeasible given the massive parameter scale of LLMs. To overcome these challenges, we propose FUDLR (Fast Unified Debiasing for LLM-RS). The core idea is to reformulate the debiasing problem as an efficient machine unlearning task with two stages. First, FUDLR identifies bias-inducing samples to unlearn through a novel bias-agnostic mask, optimized to balance fairness improvement with accuracy preservation. Its bias-agnostic design allows adaptability to various or co-existing biases simply by incorporating different fairness metrics. Second, FUDLR performs efficient debiasing by estimating and removing the influence of identified samples on model parameters. Extensive experiments demonstrate that FUDLR effectively and efficiently improves fairness while preserving recommendation accuracy, offering a practical path toward socially responsible LLM-RS. The code and data are available at https://github.com/JinLi-i/FUDLR.",
        "entry_id": "http://arxiv.org/abs/2601.17492v1",
        "pub_date": "2026-01-24",
        "translated_summary": "大语言模型（LLMs）通过先进的生成式用户建模彻底革新了推荐系统（RS）。然而，基于 LLM 的推荐系统（LLM-RS）往往会在不经意间延续训练数据中的固有偏见，引发严重的公平性问题。解决 LLM-RS 中的公平性挑战面临两大关键难题：1）现有的去偏方法往往针对特定类型的偏见设计，缺乏通用性，难以应对实际应用中多样化或新出现的偏见；2）需要重新训练模型的去偏方法，在 LLM 庞大的参数量面前计算代价高得不可承受。\n\n为此，我们提出 FUDLR（Fast Unified Debiasing for LLM-RS），其核心理念是将去偏任务重新形式化为高效的机器“反学习”过程，并分为两阶段实施。首先，FUDLR 通过一个与偏见类型无关的全新掩码，识别需要“反学习”的偏见样本，并在提升公平性的同时尽可能保持准确性。该设计无需针对特定偏见，仅需替换不同的公平性指标即可灵活适配多种或共存的偏见。其次，FUDLR 通过估计并移除已识别样本对模型参数的影响，完成高效的去偏。大量实验表明，FUDLR 能够在显著提升公平性的同时保持推荐准确率，为构建更负责任的社会化 LLM-RS 提供了可行路径。代码与数据已开源：https://github.com/JinLi-i/FUDLR"
    },
    {
        "title": "Adversarial Alignment and Disentanglement for Cross-Domain CTR Prediction with Domain-Encompassing Features",
        "summary": "Cross-domain recommendation (CDR) has been increasingly explored to address data sparsity and cold-start issues. However, recent approaches typically disentangle domain-invariant features shared between source and target domains, as well as domain-specific features for each domain. However, they often rely solely on domain-invariant features combined with target domain-specific features, which can lead to suboptimal performance. To overcome the limitations, this paper presents the Adversarial Alignment and Disentanglement Cross-Domain Recommendation ($A^2DCDR$ ) model, an innovative approach designed to capture a comprehensive range of cross-domain information, including both domain-invariant and valuable non-aligned features. The $A^2DCDR$ model enhances cross-domain recommendation through three key components: refining MMD with adversarial training for better generalization, employing a feature disentangler and reconstruction mechanism for intra-domain disentanglement, and introducing a novel fused representation combining domain-invariant, non-aligned features with original contextual data. Experiments on real-world datasets and online A/B testing show that $A^2DCDR$ outperforms existing methods, confirming its effectiveness and practical applicability. The code is provided at https://github.com/youzi0925/A-2DCDR/tree/main.",
        "entry_id": "http://arxiv.org/abs/2601.17472v1",
        "pub_date": "2026-01-24",
        "translated_summary": "跨域推荐（CDR）被广泛研究以缓解数据稀疏及冷启动困境。近期方法通常将共享的域不变特征与各域特有的域特定特征进行解耦，但仅利用域不变特征与目标域的域特征进行下游推荐，往往导致次优效果。为此，本文提出对抗对齐与解耦跨域推荐模型（A²DCDR）。该模型系统全面地挖掘跨域信息，包括既有的域不变特征，也囊括潜在具有实用价值的非对齐特征。A²DCDR 以三大核心设计强化跨域迁移：1) 通过对抗训练精炼 MMD，实现更佳泛化；2) 部署特征解耦与重构机制，完成域内解耦；3) 创新性地融合域不变、非对齐特征与原始上下文信息，构建综合表征。在真实场景数据集实验及在线 A/B 测试中，A²DCDR 均显著优于现有方法，表明其有效性与落地潜力。代码开源地址：https://github.com/youzi0925/A-2DCDR/tree/main"
    },
    {
        "title": "UniGRec: Unified Generative Recommendation with Soft Identifiers for End-to-End Optimization",
        "summary": "Generative recommendation has recently emerged as a transformative paradigm that directly generates target items, surpassing traditional cascaded approaches. It typically involves two components: a tokenizer that learns item identifiers and a recommender trained on them. Existing methods often decouple tokenization from recommendation or rely on asynchronous alternating optimization, limiting full end-to-end alignment. To address this, we unify the tokenizer and recommender under the ultimate recommendation objective via differentiable soft item identifiers, enabling joint end-to-end training. However, this introduces three challenges: training-inference discrepancy due to soft-to-hard mismatch, item identifier collapse from codeword usage imbalance, and collaborative signal deficiency due to an overemphasis on fine-grained token-level semantics.\n  To tackle these challenges, we propose UniGRec, a unified generative recommendation framework that addresses them from three perspectives. UniGRec employs Annealed Inference Alignment during tokenization to smoothly bridge soft training and hard inference, a Codeword Uniformity Regularization to prevent identifier collapse and encourage codebook diversity, and a Dual Collaborative Distillation mechanism that distills collaborative priors from a lightweight teacher model to jointly guide both the tokenizer and the recommender. Extensive experiments on real-world datasets demonstrate that UniGRec consistently outperforms state-of-the-art baseline methods. Our codes are available at https://github.com/Jialei-03/UniGRec.",
        "entry_id": "http://arxiv.org/abs/2601.17438v1",
        "pub_date": "2026-01-24",
        "translated_summary": "生成式推荐最近已成为一种变革性范式，可直接生成目标物品，从而超越传统的级联方法。它通常包含两个部分：一是学习物品标识符的标记器，二是基于这些标识符训练的推荐器。现有方法通常将标记化与推荐解耦，或依赖异步交替优化，限制了全链路的端到端对齐。为此，我们以最终的推荐目标为核心，将标记器与推荐器统一起来，通过可微分的软物品标识符实现联合端到端训练。然而，这带来了三大挑战：因软、硬标识符不匹配导致的训练-推理差异；由于码字使用失衡引发的物品标识符崩溃；以及对细粒度 token 级语义过度强调而造成的协同信号不足。\n\n为应对上述难题，我们提出 UniGRec，一个统一生成式推荐框架，从三个维度系统解决：通过“退火推理对齐机制”在标记过程中平滑衔接软训练和硬推理；采用“码字均匀正则化”防止标识符崩溃并促进码本多样性；进一步引入“双重协同蒸馏机制”，让轻量级教师模型向标记器与推荐器联合蒸馏协同先验。在多个真实数据集的广泛实验表明，UniGRec 始终优于最新的基线方法。代码开源：https://github.com/Jialei-03/UniGRec"
    },
    {
        "title": "Breaking Flat: A Generalised Query Performance Prediction Evaluation Framework",
        "summary": "The traditional use-case of query performance prediction (QPP) is to identify which queries perform well and which perform poorly for a given ranking model. A more fine-grained and arguably more challenging extension of this task is to determine which ranking models are most effective for a given query. In this work, we generalize the QPP task and its evaluation into three settings: (i) SingleRanker MultiQuery (SRMQ-PP), corresponding to the standard use case; (ii) MultiRanker SingleQuery (MRSQ-PP), which evaluates a QPP model's ability to select the most effective ranker for a query; and (iii) MultiRanker MultiQuery (MRMQ-PP), which considers predictions jointly across all query ranker pairs. Our results show that (a) the relative effectiveness of QPP models varies substantially across tasks (SRMQ-PP vs. MRSQ-PP), and (b) predicting the best ranker for a query is considerably more difficult than predicting the relative difficulty of queries for a given ranker.",
        "entry_id": "http://arxiv.org/abs/2601.17359v1",
        "pub_date": "2026-01-24",
        "translated_summary": "传统查询性能预测（QPP）的常见用途是判定在一个给定排序模型下，哪些查询表现良好、哪些表现不佳。更细粒度也更具挑战性的任务扩展则是为单个查询确定最有效的排序模型。本文将 QPP 任务及其评估推广为三种设定：(i) 单一排序器、多类查询（Single-Ranker Multi-Query，SRMQ-PP），对应于最常见的标准任务；(ii) 多排序器、单类查询（Multi-Ranker Single-Query，MRSQ-PP），用来检验 QPP 模型能否为给定查询选到最合适的排序器；(iii) 多排序器、多类查询（Multi-Ranker Multi-Query，MRMQ-PP），对全部查询-排序器配对同时进行预测。实验结果表明：(a) 不同任务之间（SRMQ-PP 与 MRSQ-PP）QPP 模型的相对表现差异显著；(b) 为单个查询预测最佳排序器的难度，远高于为固定排序器预测查询相对难度。"
    },
    {
        "title": "Beyond Correlations: A Downstream Evaluation Framework for Query Performance Prediction",
        "summary": "The standard practice of query performance prediction (QPP) evaluation is to measure a set-level correlation between the estimated retrieval qualities and the true ones. However, neither this correlation-based evaluation measure quantifies QPP effectiveness at the level of individual queries, nor does this connect to a downstream application, meaning that QPP methods yielding high correlation values may not find a practical application in query-specific decisions in an IR pipeline. In this paper, we propose a downstream-focussed evaluation framework where a distribution of QPP estimates across a list of top-documents retrieved with several rankers is used as priors for IR fusion. While on the one hand, a distribution of these estimates closely matching that of the true retrieval qualities indicates the quality of the predictor, their usage as priors on the other hand indicates a predictor's ability to make informed choices in an IR pipeline. Our experiments firstly establish the importance of QPP estimates in weighted IR fusion, yielding substantial improvements of over 4.5% over unweighted CombSUM and RRF fusion strategies, and secondly, reveal new insights that the downstream effectiveness of QPP does not correlate well with the standard correlation-based QPP evaluation.",
        "entry_id": "http://arxiv.org/abs/2601.17339v1",
        "pub_date": "2026-01-24",
        "translated_summary": "查询性能预测（QPP）的传统评估做法是在集合层面测度估计的检索质量与真实值之间的相关性。然而，这种以相关性为核心的评估方式既无法在单条查询粒度上衡量 QPP 的效果，也无法与其在实际 IR 流程中的应用建立直接联系，因此即便某个 QPP 方法在此指标上表现优异，也可能在面向具体查询的决策环节中毫无实用价值。本文提出一种以“下游任务”为中心的评估框架：将若干排序器返回的 Top 文档所对应的 QPP 估计值构成的分布作为先验，用于 IR 融合。一方面，该分布愈接近真实检索质量分布，说明预测器越可靠；另一方面，其能否在融合时作为先验发挥作用，则直接体现预测器在真实 IR 流程中的决策价值。实验结果首先表明，在加权 IR 融合中引入 QPP 估计可将效果提升 4.5% 以上，显著优于无加权的 CombSUM 及 RRF 融合策略；其次发现，QPP 的下游任务有效性与传统相关性评估指标并不一致，为该领域提供了新的研究视角。"
    },
    {
        "title": "FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search",
        "summary": "Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.",
        "entry_id": "http://arxiv.org/abs/2601.17333v1",
        "pub_date": "2026-01-24",
        "translated_summary": "自然语言查询（NLQ）让用户能以日常语⾔而非结构化查询语句来检索并与信息系统交互。本文就面向金融知识检索的现代 NLQ 系统给出了完备的技术蓝图。与传统方法相比，NLQ 的引入不仅在精度与召回率上显著提升知识检索效果，还能通过高效链接分散的金融对象、事件及其关系，带来更深层的洞察。本系统融合了自然语言处理、检索工程及向量数据模型的核心方法，致力于解决金融数据检索中固有的实体识别、相关性排序、数据时效性及发现性等关键难题。  \n本文首先分析了金融数据集和文档对 NLQ 提出的独特需求，继而阐释离线索引与在线检索的架构组件，并探讨 NLQ 在金融服务中知识检索提升的实际应用场景。我们对支撑该架构的理论基础及实验证据进行深入剖析，最终提供了关于该主题的全景式研究。文中亦详尽说明了实验方法、所用数据、实验结果，以及未来可进一步优化的方向。"
    },
    {
        "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.",
        "entry_id": "http://arxiv.org/abs/2601.18771v1",
        "pub_date": "2026-01-26",
        "translated_summary": "大型语言模型（LLM）在复杂推理任务中展现出非凡的能力，尤其是在通过搜索机制系统性调用外部知识库时表现尤甚。该领域已从传统的检索增强生成（RAG）框架，演进为通过显式搜索策略编排多步推理的精细搜索体系。然而，现有搜索框架仍严重依赖于隐含的自然语言推理来决定搜索策略以及如何在推理步骤间利用检索到的信息。这种对隐含推理的依赖带来根本性挑战：难以管理子问题间的依赖关系、难以高效复用已检索到的知识，也难以通过强化学习学得最优搜索策略。\n\n针对上述局限，我们提出 Dep-Search——一种具备依赖感知的搜索框架，借助 GRPO 实现结构化推理、检索与持久记忆的三重融合，从而突破现有搜索框架的上限。Dep-Search 引入了显式控制机制，使模型能够显式分解具有依赖关系的复合问题、按需检索信息，从预先存储的记忆中调取有用知识，并将冗长推理上下文凝练为可复用的记忆条目。\n\n在 7 个涵盖广泛主题的问答数据集上的大量实验表明，Dep-Search 显著提升了大模型处理复杂多跳推理任务的能力，并超越多个强基线，在不同模型规模上均实现实质性能跃升。"
    },
    {
        "title": "Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval",
        "summary": "Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.\n  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\\mathbf{P}$. We introduce \\texttt{ComputePN}, a novel evaluation algorithm that makes $\\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \\texttt{ComputePN} ensures the efficient evaluation of any query in $\\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.",
        "entry_id": "http://arxiv.org/abs/2601.18747v1",
        "pub_date": "2026-01-26",
        "translated_summary": "现代信息检索正从简单的文档过滤转向复杂的神经符号推理工作流。然而，当前检索架构在应对这一新模式对严密逻辑和算术约束的需求时，面临着根本性的效率困境：标准基于迭代器的引擎（Document-at-a-Time）天生不支持复杂嵌套逻辑图，强迫执行此类查询通常会导致难以处理的运行时性能；反之，朴素的递归方法（Term-at-a-Time）虽然可以支撑这些结构，但在实施广泛逻辑排除时会带来极高的内存开销。本文断言，一个检索引擎必须具备“捕捉 **P**”的能力——以计算高效的方式直接在索引上评估任意多项式时间属性。我们定义了一种形式化的检索语言 L_R（基于有向无环图 DAG），并证明其恰恰刻画了复杂度类 **P**。我们提出 ComputePN，一种新颖的评估算法，通过将原生 DAG 遍历与内存高效的“正-负”响应机制相结合，确保 L_R 内任何查询的高效评估。此工作建立了将搜索索引转化为通用计算引擎的理论基础。"
    },
    {
        "title": "S$^2$GR: Stepwise Semantic-Guided Reasoning in Latent Space for Generative Recommendation",
        "summary": "Generative Recommendation (GR) has emerged as a transformative paradigm with its end-to-end generation advantages. However, existing GR methods primarily focus on direct Semantic ID (SID) generation from interaction sequences, failing to activate deeper reasoning capabilities analogous to those in large language models and thus limiting performance potential. We identify two critical limitations in current reasoning-enhanced GR approaches: (1) Strict sequential separation between reasoning and generation steps creates imbalanced computational focus across hierarchical SID codes, degrading quality for SID codes; (2) Generated reasoning vectors lack interpretable semantics, while reasoning paths suffer from unverifiable supervision. In this paper, we propose stepwise semantic-guided reasoning in latent space (S$^2$GR), a novel reasoning enhanced GR framework. First, we establish a robust semantic foundation via codebook optimization, integrating item co-occurrence relationship to capture behavioral patterns, and load balancing and uniformity objectives that maximize codebook utilization while reinforcing coarse-to-fine semantic hierarchies. Our core innovation introduces the stepwise reasoning mechanism inserting thinking tokens before each SID generation step, where each token explicitly represents coarse-grained semantics supervised via contrastive learning against ground-truth codebook cluster distributions ensuring physically grounded reasoning paths and balanced computational focus across all SID codes. Extensive experiments demonstrate the superiority of S$^2$GR, and online A/B test confirms efficacy on large-scale industrial short video platform.",
        "entry_id": "http://arxiv.org/abs/2601.18664v1",
        "pub_date": "2026-01-26",
        "translated_summary": "生成式推荐（GR）借助端到端生成的优势，已成为一种变革性范式。然而，现有 GR 方法直接从交互序列生成语义 ID（SID），缺乏类似大模型的深层推理能力，限制了性能上限。我们研究发现，现有推理增强型 GR 存在两大关键缺陷：  \n(1) 推理与生成步骤被强行序列分割，导致分层 SID 代码的计算资源分配失衡，进而降低代码质量；  \n(2) 产生的推理向量无显式语义，且整条推理路径缺乏可验证的监督信号。  \n\n为此，本文提出逐步语义引导的隐空间推理（S²GR）。  \n首先，通过码本优化建立坚实的语义地基：引入物品共现关系以捕捉行为模式，并设计负载均衡与一致性目标，在最大化码本利用率的同时强化粗→细层次语义。  \n核心创新在于逐步推理机制：在每个 SID 生成步骤前插入“思维”令牌，令牌显式表示粗粒度语义，并以对比学习方式与真实码本聚类分布对齐；这既确保推理路径物理可锚定，又令计算资源被均衡分配到所有 SID 代码。  \n\n大量实验验证 S²GR 的显著优势；在大型工业短视频平台的在线 A/B 测试中亦得到成效验证。"
    },
    {
        "title": "FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG",
        "summary": "Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.",
        "entry_id": "http://arxiv.org/abs/2601.18579v1",
        "pub_date": "2026-01-26",
        "translated_summary": "现有的图RAG 方法为了在语料图谱上获取更具洞察力的检索结果，往往交替调用大型语言模型进行推理，导致时间开销巨大。为实现时间高效的洞察式检索，我们提出 FastInsight。首先，我们构建一个图检索分类法，将现有方法抽象为三类基本操作：向量搜索、图搜索和基于模型的搜索。借助该分类法，我们发现现有方法存在两大关键短板：基于模型的搜索忽视拓扑结构，而图搜索则缺乏语义理解。\n\nFastInsight 通过融合两项新颖操作来克服上述局限：\n1. Graph-based Reranker（GRanker）：充当图模型式搜索模块，兼顾语义与拓扑。\n2. Semantic-Topological eXpansion（STeX）：融合向量搜索与图搜索，持续扩展路径。\n\n在涵盖检索与生成的多组数据集上的大量实验表明，FastInsight 在检索精度和生成质量方面均优于当前最佳基线，并在效能与时间效率的权衡中实现了显著的帕累托改进。"
    },
    {
        "title": "Feature-Indexed Federated Recommendation with Residual-Quantized Codebooks",
        "summary": "Federated recommendation provides a privacy-preserving solution for training recommender systems without centralizing user interactions. However, existing methods follow an ID-indexed communication paradigm that transmit whole item embeddings between clients and the server, which has three major limitations: 1) consumes uncontrollable communication resources, 2) the uploaded item information cannot generalize to related non-interacted items, and 3) is sensitive to client noisy feedback. To solve these problems, it is necessary to fundamentally change the existing ID-indexed communication paradigm. Therefore, we propose a feature-indexed communication paradigm that transmits feature code embeddings as codebooks rather than raw item embeddings. Building on this paradigm, we present RQFedRec, which assigns each item a list of discrete code IDs via Residual Quantization (RQ)-Kmeans. Each client generates and trains code embeddings as codebooks based on discrete code IDs provided by the server, and the server collects and aggregates these codebooks rather than item embeddings. This design makes communication controllable since the codebooks could cover all items, enabling updates to propagate across related items in same code ID. In addition, since code embedding represents many items, which is more robust to a single noisy item. To jointly capture semantic and collaborative information, RQFedRec further adopts a collaborative-semantic dual-channel aggregation with a curriculum strategy that emphasizes semantic codes early and gradually increases the contribution of collaborative codes over training. Extensive experiments on real-world datasets demonstrate that RQFedRec consistently outperforms state-of-the-art federated recommendation baselines while significantly reducing communication overhead.",
        "entry_id": "http://arxiv.org/abs/2601.18570v1",
        "pub_date": "2026-01-26",
        "translated_summary": "联邦推荐为在不集中用户交互数据的情况下训练推荐系统提供了隐私保护的解决方案。然而，现有方法沿用基于 ID 的通信范式，在客户端与服务器之间完整传输项目嵌入向量，存在三大局限：1) 通信资源开销不可控；2) 上传的项目信息难以泛化到未曾交互的相关项目；3) 对客户端的含噪反馈敏感。为从根本上克服这些问题，我们提出以特征索引为核心的通信范式，用码本中的特征码嵌入取代原始项目嵌入进行传输。\n\n依托这一范式，我们设计了 RQFedRec，具体过程如下：首先通过残差量化（RQ）-Kmeans 为每个项目分配一组离散码 ID；服务器将这些码 ID 下发后，客户端利用这些离散码训练码本（并非项目嵌入），然后上传更新后的码本，由服务器聚合。该设计带来三点优势：其一，因码本可覆盖全部项目，通信量固定且可控；其二，共享同一代码 ID 的多个项目可同时受益，实现跨项目信息泛化；其三，单一项目的噪声对多个项目共享的码嵌入影响有限，鲁棒性更强。\n\n为融合语义与协同信息，RQFedRec 引入“协同-语义”双通道聚合，并以课程式训练策略逐步加大协同通道权重：训练早期侧重语义通道，后期逐步引入协同信息。大规模真实数据实验表明，RQFedRec 在显著降低通信开销的同时，准确率始终优于现有联邦推荐基线。"
    },
    {
        "title": "Token-level Collaborative Alignment for LLM-based Generative Recommendation",
        "summary": "Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.",
        "entry_id": "http://arxiv.org/abs/2601.18457v1",
        "pub_date": "2026-01-26",
        "translated_summary": "大型语言模型（LLM）通过利用丰富的语义知识，在生成式推荐中展现了巨大潜力。然而，现有的基于 LLM 的推荐系统难以有效纳入协同过滤（CF）信号，这是由于 CF 所建模的“物品级用户偏好”与 LLM 训练的“令牌级下一个令牌预测（NTP）”之间存在根本的不匹配。早期方法通常把 CF 结果仅当作上下文提示或表征偏差，并依赖多阶段训练来缩小行为-语义空间差异，导致 CF 无法直接干预 LLM 的生成过程。\n\n在该工作中，我们提出 Token-level Collaborative Alignment for Recommendation（TCA4Rec），一种与模型无关、即插即用的框架，可在 CF 监督与 LLM 生成之间构建明确的优化级接口。TCA4Rec 包括：\n(i) Collaborative Tokenizer，将原始物品级 CF logits 映射为与 LLM 令牌空间对齐的令牌级分布；\n(ii) Soft Label Alignment，将 CF 提供的分布与原始 one-hot 监督融合，以优化一个软 NTP 目标。\n\n该设计在完全不破坏 LLM 生成式训练特性的前提下，实现了与用户偏好的协同对齐。TCA4Rec 可与任意传统 CF 模型兼容，并泛化到多种解码器式 LLM 推荐架构；同时提供显式机制来调节“行为对齐”和“语义流畅性”，使生成的推荐既准确又可控。\n\n大量实验表明，TCA4Rec 在多种 CF 模型与 LLM 推荐系统上均持续提升推荐性能。"
    },
    {
        "title": "TopKGAT: A Top-K Objective-Driven Architecture for Recommendation",
        "summary": "Recommendation systems (RS) aim to retrieve the top-K items most relevant to users, with metrics such as Precision@K and Recall@K commonly used to assess effectiveness. The architecture of an RS model acts as an inductive bias, shaping the patterns the model is inclined to learn. In recent years, numerous recommendation architectures have emerged, spanning traditional matrix factorization, deep neural networks, and graph neural networks. However, their designs are often not explicitly aligned with the top-K objective, thereby limiting their effectiveness.\n  To address this limitation, we propose TopKGAT, a novel recommendation architecture directly derived from a differentiable approximation of top-K metrics. The forward computation of a single TopKGAT layer is intrinsically aligned with the gradient ascent dynamics of the Precision@K metric, enabling the model to naturally improve top-K recommendation accuracy. Structurally, TopKGAT resembles a graph attention network and can be implemented efficiently. Extensive experiments on four benchmark datasets demonstrate that TopKGAT consistently outperforms state-of-the-art baselines. The code is available at https://github.com/StupidThree/TopKGAT.",
        "entry_id": "http://arxiv.org/abs/2601.18432v1",
        "pub_date": "2026-01-26",
        "translated_summary": "推荐系统（RS）旨在为用户返回最相关的 top-K 个物品，常用 Precision@K 和 Recall@K 等指标进行评估。RS 模型的架构本身即是一种归纳偏置，决定了模型倾向于学习何种模式。近年来，众多推荐架构层出不穷，从传统的矩阵分解、深度神经网络到图神经网络，层出不穷。然而，这些架构的设计往往与 top-K 目标并未显式对齐，从而限制了它们的有效性。\n\n为克服该局限，我们提出一种全新的推荐架构——TopKGAT，它以 top-K 指标的端到端可微近似为直接出发点。单个 TopKGAT 层的前向计算在本质上对齐了 Precision@K 的梯度上升动态，使模型自然地优化 top-K 推荐精度。结构上，TopKGAT 类似图注意力网络，且可高效实现。在四个基准数据集的广泛实验表明，TopKGAT 一致地优于目前的最先进基线。代码已开源：https://github.com/StupidThree/TopKGAT。"
    },
    {
        "title": "Beyond the Checkbox: Strengthening DSA Compliance Through Social Media Algorithmic Auditing",
        "summary": "Algorithms of online platforms are required under the Digital Services Act (DSA) to comply with specific obligations concerning algorithmic transparency, user protection and privacy. To verify compliance with these requirements, DSA mandates platforms to undergo independent audits. Little is known about current auditing practices and their effectiveness in ensuring such compliance. To this end, we bridge regulatory and technical perspectives by critically examining selected audit reports across three critical algorithmic-related provisions: restrictions on profiling minors, transparency in recommender systems, and limitations on targeted advertising using sensitive data. Our analysis shows significant inconsistencies in methodologies and lack of technical depth when evaluating AI-powered systems. To enhance the depth, scale, and independence of compliance assessments, we propose to employ algorithmic auditing -- a process of behavioural assessment of AI algorithms by means of simulating user behaviour, observing algorithm responses and analysing them for audited phenomena.",
        "entry_id": "http://arxiv.org/abs/2601.18405v1",
        "pub_date": "2026-01-26",
        "translated_summary": "根据《数字服务法案》（DSA），在线平台的算法必须遵守关于算法透明度、用户保护与隐私的特定义务。为确保这些合规要求得以落实，DSA 强制平台接受独立审计。然而，目前尚不清楚现存的审计做法能否有效检验合规情况。\n\n为此，我们从监管与技术双重视角出发，对若干审计报告进行批判性分析，聚焦与之密切相关的三项条款：对未成年人画像的限制、推荐系统透明度，以及对敏感数据定向广告的限定。分析结果显示，现有方法在评估由 AI 驱动的系统时，方法论差异显著，技术深度不足。\n\n为了提升合规评估的深度、规模与独立性，我们提议采用“算法审计”——一种通过模拟用户行为、观察算法反应并对其审计现象进行分析，从而对 AI 算法开展行为评估的审核流程。"
    },
    {
        "title": "Corpus-Based Approaches to Igbo Diacritic Restoration",
        "summary": "With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.\n  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.",
        "entry_id": "http://arxiv.org/abs/2601.18380v1",
        "pub_date": "2026-01-26",
        "translated_summary": "借助自然语言处理（NLP），研究者们力图让计算机识别并理解人类语言中的模式。然而，这一任务并不容易：语言的句法、语用与音系中嵌入了许多动态而各异的特性，这些都必须被有效捕获并处理。得益于NLP研究者不断拓展其边界，计算机处理自然语言的能力正持续提升。但目前的研究集中于英语、日语、德语、法语、俄语、普通话等资源丰富、被广泛使用的语言。全球约7000种语言中，还有95%以上在NLP领域属于“低资源语言”，缺乏可供研究的数据、工具与技术。\n\n本文首先概述了音调符号（diacritic）歧义问题，并回顾了其他语言的相关去歧义方法。聚焦于伊博语（Igbo），本研究记录了开发灵活数据集生成框架以进行音调符号复原的全过程。具体而言，我们提出了三种主要方案：传统 n-gram 模型、分类模型，以及词嵌入模型。n-gram 模型利用目标去掉音调符号的单词之前出现的一串词作为关键预测特征。分类模型则采用目标单词左右两侧的局部词窗。嵌入模型通过比较组合上下文向量与每个候选变体向量的相似度分数来进行决策。"
    },
    {
        "title": "Orchestrating Specialized Agents for Trustworthy Enterprise RAG",
        "summary": "Retrieval-Augmented Generation (RAG) shows promise for enterprise knowledge work, yet it often underperforms in high-stakes decision settings that require deep synthesis, strict traceability, and recovery from underspecified prompts. One-pass retrieval-and-write pipelines frequently yield shallow summaries, inconsistent grounding, and weak mechanisms for completeness verification. We introduce ADORE (Adaptive Deep Orchestration for Research in Enterprise), an agentic framework that replaces linear retrieval with iterative, user-steered investigation coordinated by a central orchestrator and a set of specialized agents. ADORE's key insight is that a structured Memory Bank (a curated evidence store with explicit claim-evidence linkage and section-level admissible evidence) enables traceable report generation and systematic checks for evidence completeness. Our contributions are threefold: (1) Memory-locked synthesis - report generation is constrained to a structured Memory Bank (Claim-Evidence Graph) with section-level admissible evidence, enabling traceable claims and grounded citations; (2) Evidence-coverage-guided execution - a retrieval-reflection loop audits section-level evidence coverage to trigger targeted follow-up retrieval and terminates via an evidence-driven stopping criterion; (3) Section-packed long-context grounding - section-level packing, pruning, and citation-preserving compression make long-form synthesis feasible under context limits. Across our evaluation suite, ADORE ranks first on DeepResearch Bench (52.65) and achieves the highest head-to-head preference win rate on DeepConsult (77.2%) against commercial systems.",
        "entry_id": "http://arxiv.org/abs/2601.18267v1",
        "pub_date": "2026-01-26",
        "translated_summary": "虽然检索增强生成（RAG）在企业级知识工作中展现出潜力，但在高风险决策场景——要求进行深度综合、严格可追溯并从不充分的提示中恢复——却常常表现不佳。单次“检索-生成”流水线往往只能产生浅层总结、证据引用不一致，且缺乏核查完整性的有效机制。我们提出 ADORE（企业研究自适应深度编排框架），它用迭代、用户引导的调查流程取代线性检索，通过一位中央协调器和若干专业智能体协同完成。ADORE的核心洞见在于：结构化记忆库（一个带有声明-证据显式映射并以“章节-级可采纳证据”为粒度管理的策划化证据仓库）可实现可追溯报告，并能系统性地验证证据完整性。我们的贡献有三：(1) 记忆锁定式综合——报告生成被约束到结构化记忆库（声明-证据图）且每章仅使用可采纳证据，实现声明可追溯、引文有根；(2) 证据覆盖引导的执行——检索-反思闭环先审章节级证据覆盖率，触发针对性补充检索，并以证据驱动的结束判据达成停；(3) 章节化长上下文接地——按章节打包、修剪并进行保留引文的压缩，使长篇综合在上下文长度限制内可行。在全面评估中，ADORE 在 DeepResearch Bench 上排名第一（52.65 分），并在 DeepConsult 上与商业系统对比时取得 77.2% 的头部偏好胜率。"
    },
    {
        "title": "GenCI: Generative Modeling of User Interest Shift via Cohort-based Intent Learning for CTR Prediction",
        "summary": "Click-through rate (CTR) prediction plays a pivotal role in online advertising and recommender systems. Despite notable progress in modeling user preferences from historical behaviors, two key challenges persist. First, exsiting discriminative paradigms focus on matching candidates to user history, often overfitting to historically dominant features and failing to adapt to rapid interest shifts. Second, a critical information chasm emerges from the point-wise ranking paradigm. By scoring each candidate in isolation, CTR models discard the rich contextual signal implied by the recalled set as a whole, leading to a misalignment where long-term preferences often override the user's immediate, evolving intent. To address these issues, we propose GenCI, a generative user intent framework that leverages semantic interest cohorts to model dynamic user preferences for CTR prediction. The framework first employs a generative model, trained with a next-item prediction (NTP) objective, to proactively produce candidate interest cohorts. These cohorts serve as explicit, candidate-agnostic representations of a user's immediate intent. A hierarchical candidate-aware network then injects this rich contextual signal into the ranking stage, refining them with cross-attention to align with both user history and the target item. The entire model is trained end-to-end, creating a more aligned and effective CTR prediction pipeline. Extensive experiments on three widely used datasets demonstrate the effectiveness of our approach.",
        "entry_id": "http://arxiv.org/abs/2601.18251v1",
        "pub_date": "2026-01-26",
        "translated_summary": "点击率（CTR）预测在网络广告和推荐系统中至关重要。尽管基于历史行为建模用户偏好的技术取得了显著进展，但仍面临两大核心难题。其一，现有的判别式范式侧重于将候选项目与用户历史匹配，极易过拟合于历史上占主导地位的特征，无法及时捕捉和适应兴趣的快速漂移。其二，逐点排序范式带来严峻的信息断崖：模型孤立地为每个候选打分，全然忽略召回结果整体蕴含的丰富上下文信号，从而造成长期偏好过度压制用户当下、瞬时的演化意图。\n\n为解决上述问题，我们提出 GenCI——一个利用语义兴趣同组（semantic interest cohorts）来建模动态用户意图的生成式 CTR 预测框架。该框架首先通过以“下一项目预测（Next-Item Prediction）”为目标的生成模型，主动产出候选兴趣同组。这些同组作为与用户当下意图相对应、且与任何具体候选项目无关的显式表征。随后，一个分层的候选感知网络在打分阶段注入该上下文信号，并通过交叉注意力将其与用户历史及目标项目共同对齐。整个模型端到端训练，构建起更高对齐度、更强效果的 CTR 预测流程。在三个公开基准数据集上的大量实验验证了该方法的有效性。"
    },
    {
        "title": "Generative Chain of Behavior for User Trajectory Prediction",
        "summary": "Modeling long-term user behavior trajectories is essential for understanding evolving preferences and enabling proactive recommendations. However, most sequential recommenders focus on next-item prediction, overlooking dependencies across multiple future actions. We propose Generative Chain of Behavior (GCB), a generative framework that models user interactions as an autoregressive chain of semantic behaviors over multiple future steps. GCB first encodes items into semantic IDs via RQ-VAE with k-means refinement, forming a discrete latent space that preserves semantic proximity. On top of this space, a transformer-based autoregressive generator predicts multi-step future behaviors conditioned on user history, capturing long-horizon intent transitions and generating coherent trajectories. Experiments on benchmark datasets show that GCB consistently outperforms state-of-the-art sequential recommenders in multi-step accuracy and trajectory consistency. Beyond these gains, GCB offers a unified generative formulation for capturing user preference evolution.",
        "entry_id": "http://arxiv.org/abs/2601.18213v1",
        "pub_date": "2026-01-26",
        "translated_summary": "对长期用户行为轨迹建模是理解用户偏好演化并实现主动推荐的关键。然而，现有序列推荐系统大多只关注“下一项”预测，忽视了用户在未来多个动作间的依赖关系。我们提出生成式行为链（Generative Chain of Behavior, GCB），一个将用户交互建模为多步未来语义行为自回归链的生成框架。GCB 首先利用配备 k-means 细化的 RQ-VAE 将物品编码成语义 ID，构建保留语义邻近性的离散潜空间。在此空间之上，基于 Transformer 的自回归生成器以用户历史为条件，预测多步未来行为，从而捕捉用户长期意图的转换并生成连贯的行为轨迹。在公开基准数据集上的实验表明，GCB 在多步预测准确率和轨迹一致性方面均显著优于现有序列推荐模型。除此之外，GCB 还提供了一种统一的生成式框架，可用于刻画用户偏好的动态演化。"
    },
    {
        "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
        "summary": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.",
        "entry_id": "http://arxiv.org/abs/2601.18207v1",
        "pub_date": "2026-01-26",
        "translated_summary": "搜索智能体是一类基于语言模型（LM）的系统，它们通过推理并检索知识库（或网络）来回答问题；近期方法仅使用“可验证奖励的强化学习”（RLVR）针对最终答案的正确性进行监督。目前多数 RLVR 搜索智能体都面向通用领域问答，其相关性有限，难以覆盖科学、工程与医学等专业 AI 系统所需的场景。  \n本文提出训练智能体直接在科研文献中检索与推理——该任务不仅检验技术型问答能力，也与科研人员的真实需求紧密契合，对未来“AI 科学家”系统的能力构建至关重要。  \n\n具体而言，我们发布了一个包含 1600 万条生物医学论文摘要的检索语料库，并构建了一个高难度的事实验问答（factoid QA）数据集 PaperSearchQA，共 6 万条可完全在该语料内获得答案的样本及配套基准。我们在该环境中训练的搜索智能体显著优于非强化学习的检索基线；进一步定量分析表明，智能体展现了规划、推理与自验证等有趣行为。  \n\n我们的语料、数据集和基准均可与流行的 Search-R1 RLVR 代码库无缝配合使用，并在 https://huggingface.co/collections/jmhb/papersearchqa 全面开源。最后，所提出的数据构建流程高度可扩展，可轻松迁移到其他科学领域。"
    },
    {
        "title": "DMAP: Human-Aligned Structural Document Map for Multimodal Document Understanding",
        "summary": "Existing multimodal document question-answering (QA) systems predominantly rely on flat semantic retrieval, representing documents as a set of disconnected text chunks and largely neglecting their intrinsic hierarchical and relational structures. Such flattening disrupts logical and spatial dependencies - such as section organization, figure-text correspondence, and cross-reference relations, that humans naturally exploit for comprehension. To address this limitation, we introduce a document-level structural Document MAP (DMAP), which explicitly encodes both hierarchical organization and inter-element relationships within multimodal documents. Specifically, we design a Structured-Semantic Understanding Agent to construct DMAP by organizing textual content together with figures, tables, charts, etc. into a human-aligned hierarchical schema that captures both semantic and layout dependencies. Building upon this representation, a Reflective Reasoning Agent performs structure-aware and evidence-driven reasoning, dynamically assessing the sufficiency of retrieved context and iteratively refining answers through targeted interactions with DMAP. Extensive experiments on MMDocQA benchmarks demonstrate that DMAP yields document-specific structural representations aligned with human interpretive patterns, substantially enhancing retrieval precision, reasoning consistency, and multimodal comprehension over conventional RAG-based approaches. Code is available at https://github.com/Forlorin/DMAP",
        "entry_id": "http://arxiv.org/abs/2601.18203v1",
        "pub_date": "2026-01-26",
        "translated_summary": "现有的多模态文档问答（QA）系统主要依靠扁平化的语义检索，将文档视为一组互不关联的文本块，而忽略了其固有的层级和关系结构。这种扁平化破坏了人类在理解过程中自然依赖的逻辑与空间关联，如章节组织、图文对应及跨引用关系。为弥补这一缺陷，我们提出文档级结构表示——Document MAP（DMAP），显式编码多模态文档内部的层级结构与元素间关系。为此，我们设计了一种“结构-语义理解 Agent”，按照人类可理解的层级模式，将文本内容、图表、图示等有机组织起来，捕获其语义与版面依赖。在此基础上，再引入“反思推理 Agent”，在结构感知和证据驱动下进行推理，动态评估检索到的上下文是否充分，并通过对 DMAP 的定向交互不断精炼答案。我们在 MMDocQA 系列基准上进行的广泛实验表明，DMAP 生成的文档特定结构表示与人类理解模式高度一致，在检索精度、推理连贯性和多模态理解方面显著优于传统RAG方法。代码开源地址：https://github.com/Forlorin/DMAP"
    },
    {
        "title": "Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking",
        "summary": "Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\\% NDCG@10 with -49.5\\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.",
        "entry_id": "http://arxiv.org/abs/2601.18146v1",
        "pub_date": "2026-01-26",
        "translated_summary": "大型语言模型（LLM）正日益被用于检索与推荐中的排序任务。尽管推理式提示能够提升排序效用，我们初探发现其增益并不稳定，且伴随高昂的计算开销，从而表明“何时推理”与“如何推理”同等关键。针对该问题，本文提出一种推理路由框架：以轻量级、即插即用的路由头在生成前为每个实例决策采用直接推理（Non-Think）还是链式推理（Think）。路由头仅依赖两类生成前信号：1) 紧凑的排序感知特征（如候选项分散度）；2) 模型感知的难度信号——由反映模型对推理需求估计的诊断核查表导出。借助这些特征，路由头在生成前输出一个可控标记，指引是否进入Think模式。此外，路由头可在验证 Pareto 前沿上自适应选择运行策略，在系统约束变化时动态将计算资源分配给最有可能因推理而受益的实例。在三个公开排序数据集上，使用多种开源 LLM 进行实验，均证实本框架在提升排序效果的同时显著降低 token 消耗（如在 MovieLens 上使用 Qwen3-4B，NDCG@10 提升 6.3%，token 减少 49.5%），展示了推理路由在精度-效率权衡中的实用价值。"
    },
    {
        "title": "Enhancing LLM-based Recommendation with Preference Hint Discovery from Knowledge Graph",
        "summary": "LLMs have garnered substantial attention in recommendation systems. Yet they fall short of traditional recommenders when capturing complex preference patterns. Recent works have tried integrating traditional recommendation embeddings into LLMs to resolve this issue, yet a core gap persists between their continuous embedding and discrete semantic spaces. Intuitively, textual attributes derived from interactions can serve as critical preference rationales for LLMs' recommendation logic. However, directly inputting such attribute knowledge presents two core challenges: (1) Deficiency of sparse interactions in reflecting preference hints for unseen items; (2) Substantial noise introduction from treating all attributes as hints. To this end, we propose a preference hint discovery model based on the interaction-integrated knowledge graph, enhancing LLM-based recommendation. It utilizes traditional recommendation principles to selectively extract crucial attributes as hints. Specifically, we design a collaborative preference hint extraction schema, which utilizes semantic knowledge from similar users' explicit interactions as hints for unseen items. Furthermore, we develop an instance-wise dual-attention mechanism to quantify the preference credibility of candidate attributes, identifying hints specific to each unseen item. Using these item- and user-based hints, we adopt a flattened hint organization method to shorten input length and feed the textual hint information to the LLM for commonsense reasoning. Extensive experiments on both pair-wise and list-wise recommendation tasks verify the effectiveness of our proposed framework, indicating an average relative improvement of over 3.02% against baselines.",
        "entry_id": "http://arxiv.org/abs/2601.18096v1",
        "pub_date": "2026-01-26",
        "translated_summary": "大语言模型（LLM）在推荐系统领域虽备受瞩目，但其在捕捉复杂用户偏好模式方面仍不及传统推荐器。近期研究尝试将传统推荐的连续嵌入直接注入LLM，却难以弥补连续语义向量与离散文本符号之间的本质鸿沟。直观上，由交互行为衍生的文本属性可成为LLM理解用户偏好的关键线索，然而将这类属性直接注入模型会引发两大挑战：（1）稀疏交互难以在未见新商品上提供充分的偏好信号；（2）若把所有属性一概视作提示，会引入显著噪声。\n\n为此，本文提出一种基于交互式知识图谱的“偏好提示发现”模型，用以增强LLM 的推荐能力。该模型继承传统协同过滤思想，有选择性地提取出最具影响力的属性作为提示信息。具体而言，我们设计了“协同式偏好提示提取”范式：用相似用户在与新商品相似或关联商品上的显式交互，来生成面向未见商品的语义提示。此外，我们提出实例化的“双重注意力”机制，为每个候选属性计算其在此特定未观测商品上的“偏好可信度”，从而过滤出真正有价值的提示。\n\n随后，我们采用扁平化的提示组织方式压缩输入长度，将精炼后的文本提示注入LLM开展常识推理。我们在成对排序与列表排序两类推荐任务上的大量实验表明，本框架相较基线平均相对提升达 3.02% 以上，验证了其有效性。"
    },
    {
        "title": "Post-Training Denoising of User Profiles with LLMs in Collaborative Filtering Recommendation",
        "summary": "Implicit feedback -- the main data source for training Recommender Systems (RSs) -- is inherently noisy and has been shown to negatively affect recommendation effectiveness. Denoising has been proposed as a method for removing noisy implicit feedback and improving recommendations. Prior work has focused on in-training denoising, however this requires additional data, changes to the model architecture and training procedure or fine-tuning, all of which can be costly and data hungry. In this work, we focus on post-training denoising. Different from in-training denoising, post-training denoising does not involve changing the architecture of the model nor its training procedure, and does not require additional data. Specifically, we present a method for post-training denoising user profiles using Large Language Models (LLMs) for Collaborative Filtering (CF) recommendations. Our approach prompts LLMs with (i) a user profile (user interactions), (ii) a candidate item, and (iii) its rank as given by the CF recommender, and asks the LLM to remove items from the user profile to improve the rank of the candidate item. Experiments with a state-of-the-art CF recommender and 4 open and closed source LLMs in 3 datasets show that our denoising yields improvements up to 13% in effectiveness over the original user profiles. Our code is available at https://github.com/edervishaj/denoising-user-profiles-LLM.",
        "entry_id": "http://arxiv.org/abs/2601.18009v1",
        "pub_date": "2026-01-25",
        "translated_summary": "隐式反馈——训练推荐系统（RSs）时的最主要数据来源——与生俱来地带噪声，且已被证明会降低推荐的精度。为剔除这些噪声并提升效果，去噪技术应运而生。已有研究普遍采用“训练中”去噪策略，但这需要额外数据、修改模型架构与训练流程或在训练后进行微调，代价高昂且对数据量敏感。\n\n本文提出“训练后”去噪，与“训练中”的策略截然不同：既不改动模型架构与训练流程，也不依赖额外数据。具体而言，我们利用大语言模型（LLM），在协同过滤（CF）推荐场景中实现训练后的用户档案去噪。方法很简单：把（i）用户档案（即交互记录）、（ii）待推荐的候选物品，以及（iii）CF模型给出的该物品排名一起提示给LLM，并要求LLM从用户档案中删除某些物品，以便提升该候选物品的排名。\n\n我们在三种数据集上，用一款当前最优的CF推荐器与4个开源及闭源LLM进行了实验。结果显示，经过去噪后的用户档案，其推荐效果相比原始档案提升最高达13%。开源代码已发布：https://github.com/edervishaj/denoising-user-profiles-LLM"
    },
    {
        "title": "MedViz: An Agent-based, Visual-guided Research Assistant for Navigating Biomedical Literature",
        "summary": "Biomedical researchers face increasing challenges in navigating millions of publications in diverse domains. Traditional search engines typically return articles as ranked text lists, offering little support for global exploration or in-depth analysis. Although recent advances in generative AI and large language models have shown promise in tasks such as summarization, extraction, and question answering, their dialog-based implementations are poorly integrated with literature search workflows. To address this gap, we introduce MedViz, a visual analytics system that integrates multiple AI agents with interactive visualization to support the exploration of the large-scale biomedical literature. MedViz combines a semantic map of millions of articles with agent-driven functions for querying, summarizing, and hypothesis generation, allowing researchers to iteratively refine questions, identify trends, and uncover hidden connections. By bridging intelligent agents with interactive visualization, MedViz transforms biomedical literature search into a dynamic, exploratory process that accelerates knowledge discovery.",
        "entry_id": "http://arxiv.org/abs/2601.20709v1",
        "pub_date": "2026-01-28",
        "translated_summary": "生物医学研究人员在跨领域浏览数以百万计的文献时面临着日益增长的挑战。传统的搜索引擎通常只会以文本列表的形式返回按排名排序的文章，既无法支持全局探索，也难以为深度分析提供帮助。虽然在总结、信息抽取和问答等任务上，最新的生成式 AI 和大语言模型已显示出巨大潜力，但其基于对话的实现方式尚未很好地融入文献检索流程。为了解决这一缺口，我们提出了 MedViz——一套结合多智能体 AI 与交互可视化的视觉分析系统，用于大规模生物医学文献的探索。MedViz 将数百万篇文章构建成语义地图，并集成由智能体驱动的查询、总结与假设生成功能，使研究人员能够迭代地细化问题、识别趋势并挖掘隐性关联。通过把智能体与交互可视化紧密结合，MedViz 将传统的生物医学文献检索转变为一个动态而可探索的过程，从而加速知识的发现。"
    },
    {
        "title": "Overview of the TREC 2025 Tip-of-the-Tongue track",
        "summary": "Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2025 ToT track focused on a single ad-hoc retrieval task. This year, we extended the track to general domain and incorporated different sets of test queries from diverse sources, namely from the MS-ToT dataset, manual topic development, and LLM-based synthetic query generation. This year, 9 groups (including the track coordinators) submitted 32 runs.",
        "entry_id": "http://arxiv.org/abs/2601.20671v1",
        "pub_date": "2026-01-28",
        "translated_summary": "舌尖现象（Tip-of-the-tongue，ToT）的已知项检索指的是：用户需要重新找到某个对象，却无法可靠回忆其标识符。此类 ToT 信息需求（查询）表述冗长，常涵盖多种复杂现象，致使传统信息检索系统难以处理。TREC 2025 ToT 测试任务专设一项 ad-hoc 检索任务，今年我们将其扩展至通用领域，并从多元来源纳入测试查询集，包括 MS-ToT 数据集、人工开发的题目以及基于大语言模型的合成查询生成。今年共有 9 组单位（含任务协调方）提交了共 32 组结果。"
    },
    {
        "title": "TGSBM: Transformer-Guided Stochastic Block Model for Link Prediction",
        "summary": "Link prediction is a cornerstone of the Web ecosystem, powering applications from recommendation and search to knowledge graph completion and collaboration forecasting. However, large-scale networks present unique challenges: they contain hundreds of thousands of nodes and edges with heterogeneous and overlapping community structures that evolve over time. Existing approaches face notable limitations: traditional graph neural networks struggle to capture global structural dependencies, while recent graph transformers achieve strong performance but incur quadratic complexity and lack interpretable latent structure. We propose \\textbf{TGSBM} (Transformer-Guided Stochastic Block Model), a framework that integrates the principled generative structure of Overlapping Stochastic Block Models with the representational power of sparse Graph Transformers. TGSBM comprises three main components: (i) \\emph{expander-augmented sparse attention} that enables near-linear complexity and efficient global mixing, (ii) a \\emph{neural variational encoder} that infers structured posteriors over community memberships and strengths, and (iii) a \\emph{neural edge decoder} that reconstructs links via OSBM's generative process, preserving interpretability. Experiments across diverse benchmarks demonstrate competitive performance (mean rank 1.6 under HeaRT protocol), superior scalability (up to $6\\times$ faster training), and interpretable community structures. These results position TGSBM as a practical approach that strikes a balance between accuracy, efficiency, and transparency for large-scale link prediction.",
        "entry_id": "http://arxiv.org/abs/2601.20646v1",
        "pub_date": "2026-01-28",
        "translated_summary": "链路预测是 Web 生态系统的基石，支撑着从推荐与搜索，到知识图谱补全与合作预测等众多应用。然而，大规模网络带来了独特挑战：它们包含数十万量级的节点与边，其社区结构多元且重叠，并随时间演化。现有方法受两大制约：传统图神经网络难以捕捉全局结构性依赖；新兴图变换器虽性能强劲，却带来二次复杂度，且潜藏结构缺乏可解释性。\n\n我们提出 **TGSBM**（Transformer 引导的随机块模型）框架，将可解释的**重叠随机块模型（OSBM）**的生成式结构与**稀疏图变换器**的表征能力有机结合。TGSBM 由三大模块构成：\n1) **扩展图增强的稀疏注意力**，将近线性复杂度的全局混合能力付诸实践；\n2) **神经变分编码器**，推断节点所属社区及其强度的结构化后验；\n3) **神经边解码器**，通过 OSBM 的生成流程重构链接，从而保留可解释性。\n\n在多种基准数据集上的实验表明，TGSBM 在 HeaRT 协议下平均排名第 1.6，训练速度提升最高达 6 倍，并能显式地揭示社区结构。这些结果使 TGSBM 成为兼顾精度、效率和可解释性的大规模链路预测实用方案。"
    },
    {
        "title": "When Vision Meets Texts in Listwise Reranking",
        "summary": "Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets. Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities. In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts. To bridge the modality gap, we introduce a progressive cross-modal training strategy. We first train modalities separately: leveraging abundant text reranking data, we distill knowledge into the text branch. For images, where data is scarce, we construct distilled pairs from multimodal large language model (MLLM) captions on image retrieval benchmarks. Subsequently, we distill a joint image-text reranking dataset. Rank-Nexus achieves outstanding performance on text reranking benchmarks (TREC, BEIR) and the challenging image reranking benchmark (INQUIRE, MMDocIR), using only a lightweight 2B pretrained visual-language model. This efficient design ensures strong generalization across diverse multimodal scenarios without excessive parameters or reasoning overhead.",
        "entry_id": "http://arxiv.org/abs/2601.20623v1",
        "pub_date": "2026-01-28",
        "translated_summary": "信息检索的最新进展表明，融合视觉与文本信息具有巨大潜力。然而，由于模态差异以及对齐数据稀缺，针对图文混合文档的重排仍面临挑战。现有方法通常依赖 7B–32B 的超大模型，并采用基于推理的知识蒸馏，既带来冗余算力开销，又主要聚焦于纯文本模态。\n\n本文提出 Rank-Nexus，一种轻量级 2B 参数的图文文档重排器，能够对包含图像与文本的候选列表进行列表级质量重排。为弥合模态鸿沟，我们设计了渐进式跨模态训练策略：首先分别训练两种模态——利用丰富的文本重排数据，将知识蒸馏至文本分支；针对数据稀缺但重要的图像分支，则在图像检索基准上，借助多模态大语言模型 (MLLM) 对图像生成描述，从而构造蒸馏样本对。随后，两路并进，提炼出联合图文重排训练集。\n\n实验结果显示，Rank-Nexus 不仅在文本重排基准（TREC、BEIR）上表现优异，在极具挑战性的图像重排基准（INQUIRE、MMDocIR）上亦同样出色。无需庞大参数量或昂贵推理开销，Rank-Nexus 即可在多模态场景中实现强泛化。"
    },
    {
        "title": "On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style",
        "summary": "Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation.",
        "entry_id": "http://arxiv.org/abs/2601.20478v1",
        "pub_date": "2026-01-28",
        "translated_summary": "通奏低音是一种巴洛克即兴伴奏方式，演奏者在管风琴或羽管键琴上，依据乐谱给出的固定低音线条，即兴叠加多声部。通奏低音并非仅止于历史，而是一种仍在持续的“活传统”。对齐通奏低音数据集（ACoRD）首次在符号领域记录了当代通奏低音的实际演奏，包含 5 首通奏低音乐谱的 175 个 MIDI 演奏录音，由 7 位演奏者完成，为我们观察并量化即兴带来的多样性提供了样本。\n\n新近提出的“通奏低音演奏—乐谱对齐系统”可将即兴音符映射到对应的乐谱音符。为了研究对齐后的通奏低音演奏，我们需要一种合适的特征表示。本文提出“griff”：一种受历史通奏低音理论启发的符号化表示，能够以移调不变的方式编码通奏低音即兴的音高内容与结构。Griff 直接从对齐后的演奏中提取：将与同一乐谱音符对应的演奏音符按时值排序并组合成符号块，从而获得有意义的令牌，由此构成一个可用于分析通奏低音演奏风格的特征空间。\n\n我们统计分析 ACoRD 录音提取的 griff，并通过两项实验展示其如何区分不同演奏者的个人风格。最后论证：若要进行更精细的个人风格分析，就必须保留通奏低音即兴的内部结构；而 griff 则提供了一条兼具历史依据且值得进一步实证检验的特征化路径。"
    },
    {
        "title": "Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval",
        "summary": "Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views'' of the user's intent. However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text. Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance. To address this, we propose Diffusion-aware Multi-view Contrastive Learning (DMCL), a hallucination-robust training framework that casts DAI-TIR as joint optimization over representations of query intent and the target image. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to align textual and diffusion-generated query views while suppressing hallucinated query signals. This yields an encoder that acts as a semantic filter, effectively mapping hallucinated cues into a null space, improving robustness to spurious cues and better representing the user's intent. Attention visualization and geometric embedding-space analyses corroborate this filtering behavior. Across five standard benchmarks, DMCL delivers consistent improvements in multi-round Hits@10, reaching as high as 7.37\\% over prior fine-tuned and zero-shot baselines, which indicates it is a general and robust training framework for DAI-TIR.",
        "entry_id": "http://arxiv.org/abs/2601.20391v1",
        "pub_date": "2026-01-28",
        "translated_summary": "扩散增强交互式文本到图像检索（DAI-TIR）是一种前景广阔的范式：它通过扩散模型生成查询图像、并将这些图像作为用户意图的额外“视图”，从而提升检索性能。然而，这类生成视图可能并不正确——扩散模型生成的视觉线索会出现幻觉，与原始查询文本产生冲突。我们在实验中证实的幻觉线索会显著拉低 DAI-TIR 性能。为此，我们提出“扩散感知的多视角对比学习”（DMCL），一种对幻觉具备鲁棒性的训练框架，将 DAI-TIR 视为对“查询意图”表示与目标图像表示的联合优化。DMCL 引入语义一致性目标和扩散感知对比目标，使文本与扩散生成的查询视图对齐，同时抑制幻觉查询信号。由此得到的编码器充当语义过滤器，将幻觉线索有效映射到“空”空间，从而提高对虚假线索的鲁棒性，并更准确地传达用户意图。注意力可视化和嵌入空间几何分析都验证了该过滤机制。在五个标准基准数据集上，DMCL 在多轮检索 Hits@10 指标上带来持续提升，相比此前的微调与零样本基线最高可提高 7.37%，证明 DMCL 是一种通用且稳健的 DAI-TIR 训练框架。"
    },
    {
        "title": "Less is More: Benchmarking LLM Based Recommendation Agents",
        "summary": "Large Language Models (LLMs) are increasingly deployed for personalized product recommendations, with practitioners commonly assuming that longer user purchase histories lead to better predictions. We challenge this assumption through a systematic benchmark of four state of the art LLMs GPT-4o-mini, DeepSeek-V3, Qwen2.5-72B, and Gemini 2.5 Flash across context lengths ranging from 5 to 50 items using the REGEN dataset.\n  Surprisingly, our experiments with 50 users in a within subject design reveal no significant quality improvement with increased context length. Quality scores remain flat across all conditions (0.17--0.23). Our findings have significant practical implications: practitioners can reduce inference costs by approximately 88\\% by using context (5--10 items) instead of longer histories (50 items), without sacrificing recommendation quality. We also analyze latency patterns across providers and find model specific behaviors that inform deployment decisions. This work challenges the existing ``more context is better'' paradigm and provides actionable guidelines for cost effective LLM based recommendation systems.",
        "entry_id": "http://arxiv.org/abs/2601.20316v1",
        "pub_date": "2026-01-28",
        "translated_summary": "大型语言模型（LLM）越来越多地被用于个性化商品推荐，业界普遍假设用户购买历史越长，预测效果越好。我们在 REGEN 数据集上对四种最先进的 LLM（GPT-4o-mini、DeepSeek-V3、Qwen2.5-72B 与 Gemini 2.5 Flash）进行系统基准测试，考察的上下文长度从 5 到 50 条商品不等，直接质疑了这一假设。\n\n出人意料的是，在采用 50 名用户的被试内设计方案中，我们发现随着上下文长度增加，推荐质量并未显著提升：各条件下的质量得分始终维持在 0.17–0.23 之间。该结果具有重要现实意义：从业者只需使用 5–10 条商品的短上下文，就能在不牺牲推荐质量的前提下把推理成本降低约 88%。我们进一步分析了不同提供商的延迟模式，揭示了因模型而异的行为特征，为部署决策提供依据。\n\n本研究打破了“上下文越长越好”的传统信条，为打造经济高效的 LLM 推荐系统提供了可操作的指导。"
    },
    {
        "title": "One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking",
        "summary": "Neural ranking models (NRMs) achieve strong retrieval effectiveness, yet prior work has shown they are vulnerable to adversarial perturbations. We revisit this robustness question with a minimal, query-aware attack that promotes a target document by inserting or substituting a single, semantically aligned word - the query center. We study heuristic and gradient-guided variants, including a white-box method that identifies influential insertion points. On TREC-DL 2019/2020 with BERT and monoT5 re-rankers, our single-word attacks achieve up to 91% success while modifying fewer than two tokens per document on average, achieving competitive rank and score boosts with far fewer edits under a comparable white-box setup to ensure fair evaluation against PRADA. We also introduce new diagnostic metrics to analyze attack sensitivity beyond aggregate success rates. Our analysis reveals a Goldilocks zone in which mid-ranked documents are most vulnerable. These findings demonstrate practical risks and motivate future defenses for robust neural ranking.",
        "entry_id": "http://arxiv.org/abs/2601.20283v1",
        "pub_date": "2026-01-28",
        "translated_summary": "神经排序模型（NRMs）虽然取得了优异的检索效果，但已有研究表明它们容易受到对抗扰动的影响。本文用一种极简且依赖查询的攻击重新审视其鲁棒性问题：通过插入或替换一个与查询高度语义对齐的单词——查询中心词——来提升目标文档的排名。我们研究了启发式与基于梯度的多种变体，包括可访问模型的白盒方法，可精确定位最具影响的插入位置。在 TREC-DL 2019/2020 数据集上使用 BERT 和 monoT5 重排器的实验表明，单次单词攻击成功率最高可达 91%，平均每篇文档修改不到两个 token；在与先前 PRADA 一致的公平白盒设置下，我们的方法以远少于对方的编辑次数就达到了相当的排名与得分提升。此外，我们还提出新的诊断指标，突破整体成功率的局限，深入分析模型对攻击的敏感度。分析结果显示存在一个“金发女孩区”（Goldilocks zone）：中等排名的文档最易受到攻击。这些发现揭示了神经排序模型在实际中的风险，为推进鲁棒神经排序的防御机制提供了动机。"
    },
    {
        "title": "MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation",
        "summary": "The scaling law, which indicates that model performance improves with increasing dataset and model capacity, has fueled a growing trend in expanding recommendation models in both industry and academia. However, the advent of large-scale recommenders also brings significantly higher computational costs, particularly under the long-sequence dependencies inherent in the user intent of recommendation systems. Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost for the following requests. Despite their effectiveness, these methods often treat memory merely as a medium for acceleration, without adequately considering the space overhead it introduces. This presents a critical challenge in real-world recommendation systems with billions of users, each of whom might initiate thousands of interactions and require massive memory for state storage. Fortunately, there have been several memory management strategies examined for compression in LLM, while most have not been evaluated on the recommendation task. To mitigate this gap, we introduce MALLOC, a comprehensive benchmark for memory-aware long sequence compression. MALLOC presents a comprehensive investigation and systematic classification of memory management techniques applicable to large sequential recommendations. These techniques are integrated into state-of-the-art recommenders, enabling a reproducible and accessible evaluation platform. Through extensive experiments across accuracy, efficiency, and complexity, we demonstrate the holistic reliability of MALLOC in advancing large-scale recommendation. Code is available at https://anonymous.4open.science/r/MALLOC.",
        "entry_id": "http://arxiv.org/abs/2601.20234v1",
        "pub_date": "2026-01-28",
        "translated_summary": "随着数据集与模型规模的扩大，推荐系统在学术与工业界均兴起了一股“扩张”热潮。然而，超大规模的推荐模型也带来了前所未有的计算开销，尤其是对必须处理长序列用户意图的场景更为突出。为缓解这一问题，主流做法预先将每个用户的历史行为中间状态缓存下来，从而将后续请求的二次重复计算成本降至线性。这些方法虽有效，却普遍把记忆体仅视为加速工具，忽视其带来的庞大空间开销——在真实系统中，面对数十亿用户、每人可能产生数以千计交互的情境，其存储需求将呈指数级膨胀。值得庆幸的是，关于大语言模型（LLM）的压缩式记忆管理策略近年已有诸多研究，但其在推荐任务上的效果仍缺乏系统验证。\n\n为填补这一空白，我们提出 MALLOC：一个面向记忆高效的长序列压缩的综合基准。MALLOC 首次对可应用于大规模序列推荐系统的各类记忆管理技术进行了全面梳理、系统分类，并将其无缝嵌入当前最先进的推荐模型，构建出可复现且易于使用的评估平台。通过对准确性、效率和复杂度三大维度的详尽实验，我们验证了 MALLOC 在推进超大规模推荐系统稳健发展方面的整体可靠性。\n\n代码开源：https://anonymous.4open.science/r/MALLOC"
    },
    {
        "title": "Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation",
        "summary": "Short-video recommender systems typically optimize ranking models using dense user behavioral signals, such as clicks and watch time. However, these signals are only indirect proxies of user satisfaction and often suffer from noise and bias. Recently, explicit satisfaction feedback collected through questionnaires has emerged as a high-quality direct alignment supervision, but is extremely sparse and easily overwhelmed by abundant behavioral data, making it difficult to incorporate into online recommendation models. To address these challenges, we propose a novel framework which is towards End-to-End Alignment of user Satisfaction via Questionaire, named EASQ, to enable real-time alignment of ranking models with true user satisfaction. Specifically, we first construct an independent parameter pathway for sparse questionnaire signals by combining a multi-task architecture and a lightweight LoRA module. The multi-task design separates sparse satisfaction supervision from dense behavioral signals, preventing the former from being overwhelmed. The LoRA module pre-inject these preferences in a parameter-isolated manner, ensuring stability in the backbone while optimizing user satisfaction. Furthermore, we employ a DPO-based optimization objective tailored for online learning, which aligns the main model outputs with sparse satisfaction signals in real time. This design enables end-to-end online learning, allowing the model to continuously adapt to new questionnaire feedback while maintaining the stability and effectiveness of the backbone. Extensive offline experiments and large-scale online A/B tests demonstrate that EASQ consistently improves user satisfaction metrics across multiple scenarios. EASQ has been successfully deployed in a production short-video recommendation system, delivering significant and stable business gains.",
        "entry_id": "http://arxiv.org/abs/2601.20215v1",
        "pub_date": "2026-01-28",
        "translated_summary": "短视频推荐系统通常以用户的密集行为信号（如点击与观看时长）来训练排序模型。但这些信号只是用户满意度的间接代理，常常伴随噪声与偏差。近年，通过问卷收集的显式满意度反馈被视为高质量直接对齐监督，却极度稀疏，易被大量行为数据淹没，难以融入在线推荐模型。为应对此挑战，本文提出“端到端问卷满意度对齐”框架——EASQ，使排序模型实时逼近用户真实满意度。具体地，我们通过多任务架构结合轻量级 LoRA 模块，为稀疏问卷信号建立独立参数通道。多任务设计把稀少满意度监督与密集行为信号隔离，防止前者被淹没；LoRA 模块以参数隔离方式预先“注入”问卷偏好，既保障主干网络稳定，又提升满意度。进一步，我们设计面向在线学习的 DPO 优化目标，实时令主模型输出对齐稀疏满意度信号，实现端到端的在线更新，在新问卷回流时持续进化而不失稳定性。大量离线实验和大规模在线 A/B 验证表明，EASQ能在多场景中显著提高满意度指标。EASQ已在生产级短视频推荐系统落地，带来了显著且稳定的业务增益。"
    },
    {
        "title": "High-Resolution Mapping of Port Dynamics from Open-Access AIS Data in Tokyo Bay",
        "summary": "Knowledge about vessel activity in port areas and around major industrial zones provides insights into economic trends, supports decision-making for shipping and port operators, and contributes to maritime safety. Vessel data from terrestrial receivers of the Automatic Identification System (AIS) have become increasingly openly available, and we demonstrate that such data can be used to infer port activities at high resolution and with precision comparable to official statistics. We analyze open-access AIS data from a three-month period in 2024 for Tokyo Bay, located in Japan's most densely populated urban region. Accounting for uneven data coverage, we reconstruct vessel activity in Tokyo Bay at $\\sim\\,$30~m resolution and identify 161 active berths across seven major port areas in the bay. During the analysis period, we find an average of $35\\pm17_{\\text{stat}}$ vessels moving within the bay at any given time, and $293\\pm22_{\\text{stat}}+65_{\\text{syst}}-10_{\\text{syst}}$ vessels entering or leaving the bay daily, with an average gross tonnage of $11{,}860^{+280}_{-\\;\\,50}$. These figures indicate an accelerating long-term trend toward fewer but larger vessels in Tokyo Bay's commercial traffic. Furthermore, we find that in dense urban environments, radio shadows in vessel AIS data can reveal the precise locations of inherently passive receiver stations.",
        "entry_id": "http://arxiv.org/abs/2601.20211v1",
        "pub_date": "2026-01-28",
        "translated_summary": "了解港口区域和主要工业地带周边船舶的动态，能够洞察经济趋势，为航运与港口运营商的决策提供支持，并对保障海上安全具有重要意义。来自自动识别系统（AIS）地面接收机的船舶数据日趋开放可用；我们证明，依据这类开放数据可以高分辨率地推断港口活动，其结果精度能与官方统计相媲美。我们以日本人口最稠密的都市区域——东京湾2024年历时三个月的公开AIS数据为例展开分析。针对数据覆盖不均的问题，我们重建了东京湾内约30米分辨率的船舶活动图景，并在港湾七大主要港区识别出161个活跃泊位。\n\n在分析时段内，湾内平均每次同时有 35±17（统计误差）艘船舶移动；每天进出湾区的船舶数量为 293±22（统计误差）+65（系统涨差）-10（系统跌差）艘，其平均总吨位为 11,860（上限误差+280，下限误差-50）。上述数字显示，东京湾商业运输的长期趋势正在加速向“船数减少、吨位增大”演变。此外，在人口稠密的都市环境中，AIS无线电阴影区的出现可反推出原本被动的接收站精准位置。"
    },
    {
        "title": "MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation",
        "summary": "Item indexing, which maps a large corpus of items into compact discrete representations, is critical for both discriminative and generative recommender systems, yet existing Vector Quantization (VQ)-based approaches struggle with the highly skewed and non-stationary item distributions common in streaming industry recommenders, leading to poor assignment accuracy, imbalanced cluster occupancy, and insufficient cluster separation. To address these challenges, we propose MERGE, a next-generation item indexing paradigm that adaptively constructs clusters from scratch, dynamically monitors cluster occupancy, and forms hierarchical index structures via fine-to-coarse merging. Extensive experiments demonstrate that MERGE significantly improves assignment accuracy, cluster uniformity, and cluster separation compared with existing indexing methods, while online A/B tests show substantial gains in key business metrics, highlighting its potential as a foundational indexing approach for large-scale recommendation.",
        "entry_id": "http://arxiv.org/abs/2601.20199v1",
        "pub_date": "2026-01-28",
        "translated_summary": "项目索引（Item indexing）通过将庞大语料中的项目映射为紧致的离散表示，对判别式和生成式推荐系统都至关重要。然而，现有的基于矢量量化（Vector Quantization, VQ）的方法在面对流媒体行业推荐器中高倾斜、非平稳的项目分布时，往往存在指派不准、簇占用失衡及簇间区分不足等缺陷。为解决这些难题，我们提出下一代项目索引范式 MERGE：它能够从零开始自适应地构建簇，实时监测簇的占用情况，并通过从细到粗的顺序合并形成层次化索引结构。大量实验表明，与现有索引方法相比，MERGE 在指派精度、簇均匀度和簇分离度上均有显著提升；在线 A/B 测试更在关键业务指标上取得大幅增长，彰显了其作为大规模推荐基础索引方案的潜力。"
    },
    {
        "title": "Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms",
        "summary": "Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.",
        "entry_id": "http://arxiv.org/abs/2601.20131v1",
        "pub_date": "2026-01-27",
        "translated_summary": "设计嵌入检索系统需要在效率与效果这对矛盾之间寻找平衡，设计空间纷繁复杂。本文将相关决策按系统栈自顶向下的顺序进行结构化梳理。首先，在表示层中，我们通过讨论损失函数与网络架构（特别是 Bi-encoder 和 Cross-encoder）如何定义语义相关性的同时，也塑造了其在向量空间的投影形式。其次，在粒度层探讨分段策略——原子级切分（Atomic）与层次化切分（Hierarchical）——如何缓解长文档中的信息瓶颈。随后，在编排层，我们超越了单向量范式，讨论层次化检索、受代理分解、多阶段重排流水线等方法，以突破容量限制。最后，在鲁棒性层，我们指出并给出架构层面的缓解方案，以应对领域泛化失败、词汇盲区以及因时间漂移而导致的检索质量静默退化。通过对这些限制因素与设计选择进行归类，本文为从业者提供了一个整体框架，以在现代神经搜索系统优化“效率–效果”边界。"
    },
    {
        "title": "Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing",
        "summary": "Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.",
        "entry_id": "http://arxiv.org/abs/2601.20107v1",
        "pub_date": "2026-01-27",
        "translated_summary": "近期的视觉-语言模型（如 ColPali）虽已实现细粒度视觉文档检索（VDR），却带来难以承受的索引向量开销。无需训练的剪枝策略（如基于 EOS-注意力的方法）可将索引向量减少约 60%，且无需模型参数调整；然而在压缩率高于 80% 的极限场景下，其性能往往劣于随机选择。先有研究（如 Light-ColPali）将此归因于“视觉 token 的重要性本质上依赖查询”，从而质疑无训练剪枝的可行性。\n\n本工作提出结构锚点剪枝（SAP）——一种无需训练的剪枝方法，通过定位中间层的关键视觉块，在极高压缩率下仍保持优异检索质量。我们还引入 Oracle Score Retention（OSR）协议，以评估各层信息如何影响压缩效率。在 ViDoRe 基准上的实验表明，SAP 可将索引向量压缩超过 90%，同时保持检索精度，为视觉 RAG 提供极具扩展性的解决方案。此外，基于 OSR 的分析揭示：语义结构锚点视觉块持续存在于中间层，与传统剪枝方法聚焦的结构信号已退化的末层截然不同。"
    },
    {
        "title": "IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation",
        "summary": "Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.",
        "entry_id": "http://arxiv.org/abs/2601.20084v1",
        "pub_date": "2026-01-27",
        "translated_summary": "可解释的黑盒稠密检索器仍是检索增强生成（RAG）中的核心难题。要诊断检索行为并改进模型设计，就必须理解查询与文档之间的语义交互方式。然而，现有稠密检索器依赖静态嵌入分别表示查询和文档，掩盖了二者之间的双向关系。诸如重排序器这类事后解释方法则计算代价高昂，延迟推理时间，却仍无法揭示底层的语义对齐。\n\n为克服上述局限，本文提出可解释模块式检索神经网络（IMRNNs），一种轻量级框架，可在推理阶段为任意稠密检索器加入动态、双向的调制机制。IMRNNs 采用两个独立适配器：其一以当前查询为条件调整文档嵌入；其二利用初步检索到的文档在语料层面的反馈精炼查询嵌入。通过这一迭代调制过程，模型可动态调整表示并显式地呈现查询与文档之间的可解释语义依赖。\n\n实验表明，IMRNNs 不仅提升了解释性，也增强了检索效果。在七个基准数据集上，将本方法应用于标准稠密检索器，可在 nDCG、召回率和 MRR 上较最优基线分别平均提升 6.35%、7.14% 和 7.04%。这些结果说明，受可解释性驱动的调制机制能够同时解释并提升 RAG 系统中的检索性能。"
    },
    {
        "title": "LLaTTE: Scaling Laws for Multi-Stage Sequence Modeling in Large-Scale Ads Recommendation",
        "summary": "We present LLaTTE (LLM-Style Latent Transformers for Temporal Events), a scalable transformer architecture for production ads recommendation. Through systematic experiments, we demonstrate that sequence modeling in recommendation systems follows predictable power-law scaling similar to LLMs. Crucially, we find that semantic features bend the scaling curve: they are a prerequisite for scaling, enabling the model to effectively utilize the capacity of deeper and longer architectures. To realize the benefits of continued scaling under strict latency constraints, we introduce a two-stage architecture that offloads the heavy computation of large, long-context models to an asynchronous upstream user model. We demonstrate that upstream improvements transfer predictably to downstream ranking tasks. Deployed as the largest user model at Meta, this multi-stage framework drives a 4.3\\% conversion uplift on Facebook Feed and Reels with minimal serving overhead, establishing a practical blueprint for harnessing scaling laws in industrial recommender systems.",
        "entry_id": "http://arxiv.org/abs/2601.20083v1",
        "pub_date": "2026-01-27",
        "translated_summary": "我们提出 LLaTTE（用于时序事件的 LLM 风格隐层 Transformer），这是一种可扩展的 Transformer 架构，专为生产环境广告推荐而设计。通过系统实验，我们证明推荐系统中的序列建模遵循与 LLM 相似的幂律缩放规律。关键发现是，**语义特征能够“弯曲”缩放曲线**：它们不仅是规模化的前置条件，更使模型可以有效地利用更深、更长的架构所带来的容量优势。为了在严格延迟约束下持续享受规模化带来的益处，我们设计了两阶段架构，将大型长上下文本模型的重计算部分**异步卸载**到上游的用户模型。实验显示，上游改进可以可预测地迁移到下游排序任务。该多级框架已作为 Meta 最大的用户模型上线，在 Facebook Feed 与 Reels 上带来 **4.3% 的转化提升**，且几乎不增加在线服务开销，为工业级推荐系统如何实践利用扩展规律树立了可行范本。"
    },
    {
        "title": "When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering",
        "summary": "Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.",
        "entry_id": "http://arxiv.org/abs/2601.19827v1",
        "pub_date": "2026-01-27",
        "translated_summary": "检索增强生成（RAG）使大语言模型（LLM）突破了参数知识的局限，但在多跳推理、领域知识稀疏且证据异质的科学领域中，何时使用迭代检索-推理循环才能明显优于静态 RAG，仍缺乏明确结论。我们通过首次受控的、机制层面对比实验，检验同步迭代检索与推理能否超越“理想化静态上限”（Gold Context）RAG。我们在三种设置下对 11 个前沿 LLM 进行基准测试：（i）无上下文，仅依赖参数记忆；（ii）Gold Context，一次性提供全部 oracle 证据；（iii）迭代 RAG，采用无需训练的控制器，按检索 ↔ 假设精炼 ↔ 证据感知式停止的顺序交替进行。利用聚焦化学领域的 ChemKGMultiHopQA 数据集，我们抽取真正需要外部检索的问题，并通过覆盖缺口、锚点传递失效（anchor-carry drop）、查询质量、组合保真度和控制校准等多项诊断工具分析模型行为。\n\n跨模型结果表明，迭代 RAG 普遍超越 Gold Context，最高带来 25.6 个百分点的提升，尤其是在未经推理专门微调的模型中更明显。分阶段式检索减轻了后期跳数失败，缓解了上下文超载，并能动态修正早期假设漂移。然而，残留失败模式包括跳数覆盖不完整、干扰锚轨迹（distractor latch）卡住、提前停止失准，以及即使检索全部正确时的组合失败率仍较高。总体而言，分阶段检索往往比“理想证据一次性提供”本身更具决定性。我们针对专业科学场景，给出了部署与诊断 RAG 系统的实操指南，并为构建更可靠、可控的迭代检索-推理框架奠定了方法论基础。"
    },
    {
        "title": "An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care",
        "summary": "There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.",
        "entry_id": "http://arxiv.org/abs/2601.19824v1",
        "pub_date": "2026-01-27",
        "translated_summary": "要在医疗场景中有效应用推荐系统，必须克服一系列挑战：临床数据缺乏公开获取途径，用户难以理解推荐依据，执行推荐可能带来的风险，以及对其疗效的不确定性。为此，本文提出的推荐模型利用心理测评数据的内在结构，生成忠实于模型且易于医护人员解读的可视化解释。我们以极为细分的老年基层医疗领域为切口，验证了该模型如何辅助临床医生制定个性化照护计划。\n\n我们使用由巴西合作研究团队收集的医疗数据集，对新模型进行了离线性能对比评估；同时通过用户实验检验其可视化解释的可读性。结果表明，该模型能够有效推动推荐系统在老年基层医疗——这一因人口结构快速演变而在需求、机遇和信息技术需求方面持续增长的专业领域——的实际落地应用。"
    },
    {
        "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
        "summary": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users' immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.",
        "entry_id": "http://arxiv.org/abs/2601.19761v1",
        "pub_date": "2026-01-27",
        "translated_summary": "社交机器人的个性化，指的是机器人满足单个用户需求与/或偏好的能力。现有方案通常依赖大语言模型（LLM）根据用户元数据与历史交互生成情境感知回复，或借助强化学习（RL）等自适应方法，基于用户的实时反馈在线学习。然而，这些方法难以全面捕捉用户偏好（包括长期、短期及细粒度层面），也难以据此对行为进行排序和选择，也无法主动实现交互的个性化与确保符合伦理的自适应。为此，本文借鉴专注于建模用户偏好并提供个性化推荐的推荐系统（RS）。为确保 RS 技术在整个社交机器人流程中切实落地、无缝集成，我们（i）对齐社会机器人与推荐系统的范式，（ii）锁定可强化机器人个性化的关键技术，并以模块化、即插即用的形式进行设计。这不仅为社会机器人引入推荐系统建立了框架，也开启了 RS 与 HRI（人机交互）领域的深度合作之路，从而加速两大领域的创新。"
    },
    {
        "title": "Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues",
        "summary": "Missing-modality information on e-commerce platforms, such as absent product images or textual descriptions, often arises from annotation errors or incomplete metadata, impairing both product presentation and downstream applications such as recommendation systems. Motivated by the multimodal generative capabilities of recent Multimodal Large Language Models (MLLMs), this work investigates a fundamental yet underexplored question: can MLLMs generate missing modalities for products in e-commerce scenarios? We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.\n  We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks. Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment. In addition, performance varies substantially across product categories and model scales, and we observe no trivial correlation between model size and performance, in contrast to trends commonly reported in mainstream benchmarks. We also explore Group Relative Policy Optimization (GRPO) to better align MLLMs with this task. GRPO improves image-to-text completion but does not yield gains for text-to-image completion. Overall, these findings expose the limitations of current MLLMs in real-world cross-modal generation and represent an early step toward more effective missing-modality product completion.",
        "entry_id": "http://arxiv.org/abs/2601.19750v2",
        "pub_date": "2026-01-27",
        "translated_summary": "电商场景中，诸如商品图片或文字描述缺失的模态缺失问题，往往源于标注错误或元数据不全，既影响商品呈现，也损害推荐系统等下游应用。受近期多模态大语言模型（MLLM）生成能力的启发，本研究探讨了一个基础却鲜有人触及的问题：MLLM能否在电商情境下为商品补全缺失的模态？我们提出缺失模态商品补全基准（MMPCBench），下设两个子基准：内容质量补全基准与推荐效果基准。\n\n我们在九个真实商品类别上，针对图文和文图双向补全任务，对来自 Qwen2.5-VL 与 Gemma-3 家族的六种前沿 MLLM 进行了系统评测。实验表明，这些模型虽能捕捉高层语义，却在细粒度的词级对齐以及像素/子块级对齐上力不从心。此外，性能在不同商品类别与模型规模间差异显著，且与模型大小并无单调正相关，这与主流基准普遍观察到的趋势相左。我们还探索了 Group Relative Policy Optimization（GRPO）以提升任务适配度：GRPO 在文生图补全有所提升，却对图生文补全无增益。整体而言，这些发现揭示了当前 MLLM 在真实跨模态补全中的局限，并为更有效的商品缺失模态补全迈出了早期一步。"
    },
    {
        "title": "Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation",
        "summary": "Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.",
        "entry_id": "http://arxiv.org/abs/2601.20848v1",
        "pub_date": "2026-01-28",
        "translated_summary": "尽管近年来在缓解推荐系统不公平性方面的努力持续增长，现有具备公平感知能力的方法通常只能将公平性约束固定在训练阶段，并且在训练结束后缺乏灵活调整的空间。然而在真实场景中，不同利益相关方可能在不同时间提出各异的公平性需求，若针对每一种需求都重新训练模型将代价高昂。为此，我们提出 Cofair，一种单次训练即可实现训练后公平性控制的推荐框架。具体而言，Cofair 引入一个共享表示层，配合以公平性为条件的适配器模块，能够为不同公平性级别生成相应的用户嵌入；同时设计用户级正则化项，保证在不同级别间用户的公平性得以单调提升。理论分析表明，Cofair 的对抗目标提供了人口统计平等性的上界，而正则化项则强制在每个用户维度上实现渐进式公平。我们在多种数据集和骨干模型上的全面实验表明，该框架无需针对每一种新需求重新训练即可在不同公平性水平上实现动态控制，并获得与当前最佳基线相当或更优的公平-精度权衡曲线。代码已开源：https://github.com/weixinchen98/Cofair。"
    },
    {
        "title": "$\\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval",
        "summary": "This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of \"distances\" or \"similarities,\" including the $\\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.",
        "entry_id": "http://arxiv.org/abs/2601.20844v1",
        "pub_date": "2026-01-28",
        "translated_summary": "本文研究了将子集隶属关系（m 个元素以及至多为 k 个元素的 ${m\\choose k}$ 个子集）嵌入到向量空间所需的最低维度，记为最小可嵌入维度（MED）。我们理论上推导了多种“距离”或“相似度”定义——包括 $\\ell_2$ 度量、内积以及余弦相似度——下 MED 的紧致界限，并通过实验验证了这些界。进一步地，我们在一个更可行的设定中开展数值模拟：将 ${m\\choose k}$ 个子集表示为其所含元素嵌入所成之质心。模拟结果表明 MED 与待嵌入元素数目之间呈现出对数级别的依赖关系。这些发现暗示，基于嵌入的检索局限主要源于可学习性挑战，而非几何约束，为未来算法设计指明了方向。"
    },
    {
        "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications",
        "summary": "Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.\n  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.\n  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.\n  All test suites, harnesses, and results are included for reproducibility.",
        "entry_id": "http://arxiv.org/abs/2601.22025v1",
        "pub_date": "2026-01-29",
        "translated_summary": "大语言模型（LLM）应用的评测与传统软件测试不同：输出呈随机性、高维，并且对提示和模型的变化极其敏感。我们提出一种“定义—测试—诊断—修复”的评测驱动式工作流，把上述挑战转化为可重复迭代的工程闭环。\n\n为此，我们设计了**最小可行评测套件（MVES)**：一个分层的标准化评测方案，分别面向（i）通用 LLM 应用、（ii）检索增强生成（RAG）以及（iii）带工具的代理（agentic）工作流。文中进一步综合常见的评测手段——自动校验、人工评分表、以及 LLM-as-Judge——并总结了 judge 模式的失效案例和规避建议。\n\n在可复现的本地实验（Ollama + Llama 3 8B Instruct / Qwen 2.5 7B Instruct）中，我们发现：将任务专用提示替换为“更通用”的提示模板，会在性能之间产生权衡。在小型结构化测试中，Llama 3 的实体抽取通过率从 100% 降至 90%，RAG 合规性由 93.3% 降至 80%，而指令遵循能力却得到提升。这充分说明：提示优化必须以数据驱动的迭代取代“万能模板”，并对每一次改进的效果做出审慎量化。\n\n所有测试套件、实验脚本与完整结果均已开源，确保完全可复现。"
    },
    {
        "title": "LANCER: LLM Reranking for Nugget Coverage",
        "summary": "Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.",
        "entry_id": "http://arxiv.org/abs/2601.22008v1",
        "pub_date": "2026-01-29",
        "translated_summary": "与问答等短文本检索增强生成（RAG）不同，长文本 RAG 需要在检索阶段获得涵盖广泛相关信息的文档。自动报告生成便是典型场景：它不仅要求信息相关，更要求输出能够全面详尽。然而，现有检索方法主要针对“相关性排序”而非“信息覆盖”进行优化。为此，我们提出 LANCER，一种面向“信息覆盖”的基于大模型的重排序方法。LANCER 首先预测若要满足信息需求需回答哪些子问题；其次判断哪些文档可回答上述子问题；最后重排文档，使最终排序尽可能覆盖更多的信息“金片”。实验表明，以 nugget 覆盖指标衡量，LANCER 显著提升了检索质量，其 α-nDCG 和信息覆盖率均优于其他基于大模型的重排序方法。 oracle 分析进一步证实，子问题生成在其中发挥关键作用。"
    },
    {
        "title": "SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation",
        "summary": "Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.\n  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.",
        "entry_id": "http://arxiv.org/abs/2601.21986v1",
        "pub_date": "2026-01-29",
        "translated_summary": "传统序列推荐（SR）模型通常仅从用户–物品交互中学习到维数很低的物品 ID 表示，往往会忽略物品标题或描述等文本信息。近来，大规模语言模型（LLM）的进展催生了一系列新研究：先将物品文本编码为高维语义向量，再设计各种变换策略将其注入 SR 模型。这些嵌入变换策略可归纳为两类，但均存在明显缺陷：1）基于 adapter 的方法易发生严重的维度坍缩，信息过度集中于少数主导维度；2）基于 SVD 的方法过于僵化且依赖手工设定，仅保留极少的主成分，而将剩余谱信息全部丢弃。\n\n为了克服上述缺陷，我们提出 SpecTran——一种 Transformer 式的谱感知 adapter，直接在频谱域操作，兼顾整个频谱，动态选取并聚合信息丰富的分量。通过学习型谱–位置编码将奇异值线索作为归纳偏差注入，引导注意力聚焦于显著谱成分，并在嵌入维度间强化多样性。在四个真实数据集及三种主流 SR 骨干模型上的实验表明，SpecTran 持续优于已有强基线，平均提升达到 9.17%。"
    },
    {
        "title": "JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG",
        "summary": "The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \\textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \\textbf{JADE} (\\textbf{J}oint \\textbf{A}gentic \\textbf{D}ynamic \\textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \\textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.",
        "entry_id": "http://arxiv.org/abs/2601.21916v1",
        "pub_date": "2026-01-29",
        "translated_summary": "检索增强生成（RAG）的演进已推动系统从静态的检索流水线转向由中央规划器主导的、更具动态性的多轮推理代理式工作流程。然而，现有范式面临一个关键二分：它们要么在刻板的固定图架构中联合优化各模块，要么虽赋予动态规划能力，却把执行器当作冻结的“黑箱”工具。我们发现，这种 \\textit{解耦优化} 造成了“战略–操作不匹配”——即使规划策略再精妙，也会因为本地执行器未及时适应而无法兑现，导致系统复杂度增加却最终性能下降。本文提出 \\textbf{JADE}（\\textbf{联合代理式动态执行}），一种面向动态、多轮工作流程的规划与执行联合优化框架。该方法将系统视为在同一共享主网络下的协作式多智能体团队，支持以最终结果奖励为驱动的端到端学习。由此实现 \\textit{共同适应}：规划器学会在现有执行器能力范围内制定策略，而执行器则进化以贴合高层的战略意图。实验表明，JADE 将原本孤立的模块转化为协同系统，通过联合优化显著提升性能，并可依靠动态工作流编排灵活平衡效率与有效性。"
    },
    {
        "title": "ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation",
        "summary": "Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to \"process hallucinations\", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.",
        "entry_id": "http://arxiv.org/abs/2601.21912v1",
        "pub_date": "2026-01-29",
        "translated_summary": "强化学习（RL）已成为在复杂推理任务中优化检索增强型生成（RAG）的有力方法。然而，传统的基于最终结果的 RL 往往受奖励稀疏和信用分配低效的困扰，因为粗糙的标量奖励无法辨识长轨迹中的具体错误步骤。这种模糊性常导致“过程幻象”：模型通过错误逻辑或冗余检索也能抵达正确答案。尽管最新的“过程感知”方法尝试通过静态偏好学习或启发式奖励塑形来缓解，却普遍缺乏用于将步骤级信用与全局结果分离的在线探索能力。\n\n为应对上述挑战，我们提出 ProRAG——一个将学到的步骤级监督注入在线优化循环的过程监督强化学习框架。该框架包含四个阶段：（1）监督策略预热，使模型以结构化推理格式初始化；（2）构建基于蒙特卡洛树搜索（MCTS）的过程奖励模型（PRM），定量评价中间推理质量；（3）PRM 引导的推理精炼，对齐策略与细粒度过程偏好；（4）具备双粒度优势机制的过程监督强化学习。ProRAG 通过融合步骤级过程奖励与全局结果信号，为每个动作提供精确反馈。\n\n在五项多跳推理基准上进行的大量实验表明，相较强大的基于结果与过程感知的 RL 基线，ProRAG 在复杂长时序任务上尤其表现出显著更优的整体性能，验证了细粒度过程监督的有效性。代码和模型已开源：https://github.com/lilinwz/ProRAG。"
    },
    {
        "title": "The 'Big Three' of Scientific Information: A comparative bibliometric review of Web of Science, Scopus, and OpenAlex",
        "summary": "The present comparative study examines the three main multidisciplinary bibliographic databases, Web of Science Core Collection, Scopus, and OpenAlex, with the aim of providing up-to-date evidence on coverage, metadata quality, and functional features to help inform strategic decisions in research assessment. The report is structured into two complementary methodological sections. First, it presents a systematic review of recent scholarly literature that investigates record volume, open-access coverage, linguistic diversity, reference coverage, and metadata quality; this is followed by an original bibliometric analysis of the 2015-2024 period that explores longitudinal distribution, document types, thematic profiles, linguistic differences, and overlap between databases. The text concludes with a ten-point executive summary and five recommendations.",
        "entry_id": "http://arxiv.org/abs/2601.21908v1",
        "pub_date": "2026-01-29",
        "translated_summary": "本研究对三大跨学科学术文献数据库——Web of Science 核心合集、Scopus 与 OpenAlex——进行横向比较，旨在用最新证据说明其覆盖范围、元数据质量与功能特性，为科研评估的战略决策提供参考。报告采用两种互为补充的方法：第一，系统综述了近期学术文献针对记录总量、开放获取覆盖、语言多样性、参考文献完整性及元数据质量的研究；第二，针对 2015–2024 年数据进行原创的文献计量分析，考察历年分布、文献类型、主题特征、语言差异及三库之间重合度。文末以十项执行摘要和五条建议作结。"
    },
    {
        "title": "LEMUR: Learned Multi-Vector Retrieval",
        "summary": "Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.",
        "entry_id": "http://arxiv.org/abs/2601.21853v1",
        "pub_date": "2026-01-29",
        "translated_summary": "由后期交互模型（如 ColBERT）产生的多向量表示，在信息检索任务中比单向量表示具有更高的检索质量。在多向量检索系统中，查询和文档均以“每个 token 一个嵌入”的形式编码，并通过 MaxSim 相似度度量二者的相关性。然而，多向量检索在显著提升召回率的同时，也带来了严重的延迟开销，因此亟需高效的近似最近邻搜索（ANNS）算法。本文提出 LEMUR——一个简单而高效的多向量相似度搜索框架。LEMUR 通过两次连续的问题规约实现加速：首先，将多向量相似度搜索重新表述为监督学习问题，可用单层隐藏层神经网络求解；随后，将模型推理进一步归约为潜在空间中的单向量相似度搜索，从而直接复用成熟的单向量 ANNS 方法加速检索。除在 ColBERTv2 嵌入上测试外，我们还把 LEMUR 应用于最新多向量文本模型和多向量视觉文档检索模型。实验表明，LEMUR 比现有最先进的多向量相似度搜索方法快一个数量级。"
    },
    {
        "title": "Trustworthy Intelligent Education: A Systematic Perspective on Progress, Challenges, and Future Directions",
        "summary": "In recent years, trustworthiness has garnered increasing attention and exploration in the field of intelligent education, due to the inherent sensitivity of educational scenarios, such as involving minors and vulnerable groups, highly personalized learning data, and high-stakes educational outcomes. However, existing research either focuses on task-specific trustworthy methods without a holistic view of trustworthy intelligent education, or provides survey-level discussions that remain high-level and fragmented, lacking a clear and systematic categorization. To address these limitations, in this paper, we present a systematic and structured review of trustworthy intelligent education. Specifically, We first organize intelligent education into five representative task categories: learner ability assessment, learning resource recommendation, learning analytics, educational content understanding, and instructional assistance. Building on this task landscape, we review existing studies from five trustworthiness perspectives, including safety and privacy, robustness, fairness, explainability, and sustainability, and summarize and categorize the research methodologies and solution strategies therein. Finally, we summarize key challenges and discuss future research directions. This survey aims to provide a coherent reference framework and facilitate a clearer understanding of trustworthiness in intelligent education.",
        "entry_id": "http://arxiv.org/abs/2601.21837v1",
        "pub_date": "2026-01-29",
        "translated_summary": "近年来，由于教育场景本身高度敏感——例如涉及未成年人和弱势群体、学习数据极具个性化、教育成果具有高利害性——可信性问题在智能教育领域受到越来越多的关注和探讨。然而，既有研究要么聚焦特定任务的可信方法，缺乏对智能教育可信问题的全局视角；要么仅停留在综述层面，讨论零散且过于宏观，缺乏明确而系统的分类体系。为弥补这一空白，本文对可信智能教育展开一次系统化、结构化的综述。具体而言，我们率先将智能教育划分为五大典型任务类别：学习者能力评估、学习资源推荐、学习分析、教育内容理解和教学辅导。在此任务版图基础上，我们从安全与隐私、鲁棒性、公平性、可解释性及可持续性五个可信维度回顾现有研究，并对其中采用的研究方法和解决策略进行归纳和分类。最后，本文总结了关键挑战并展望了未来研究方向。本综述旨在构建一个统一的参考框架，促进对智能教育中可信性问题的清晰理解。"
    },
    {
        "title": "The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation",
        "summary": "Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.",
        "entry_id": "http://arxiv.org/abs/2601.21805v1",
        "pub_date": "2026-01-29",
        "translated_summary": "跨域推荐（CDR）通过利用源域的辅助信号，为目标域推荐质量的提升提供了有效手段。然而，近期研究却发现 CDR 可能无意中加剧群体层面的不公平。本文对此进行了系统性的理论与实证分析，揭示了这一现象的根本原因：一方面存在“跨域差异传递”，即源域原有的群体级差异被系统性传导到目标域；另一方面存在“跨域信息增益导致的不公平”，即跨域知识带来的好处在不同群体间分配不均。针对这两大挑战，我们提出跨域公平增强框架（CDFA）。该框架首先通过自适应地引入无标签数据，平衡各群体的训练信号信息含量，从而抑制差异传递；其次借助信息论方法对跨域获得的知识增益进行再分配，确保各群体公平受益。在多个数据集和基线模型上的广泛实验表明，CDFA 显著缓解了 CDR 中的不公平问题，且在整体推荐效果不受损的情况下实现了进一步增益。"
    },
    {
        "title": "OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce",
        "summary": "In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\\% GMV in product-card, +15.32\\% Orders in Short-Video, and +2.78\\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.",
        "entry_id": "http://arxiv.org/abs/2601.21770v1",
        "pub_date": "2026-01-29",
        "translated_summary": "在生成式推荐的浪潮中，我们推出 OneMall——专为快手电商场景量身打造的端到端生成式推荐框架。OneMall 系统性地统一了电商的多渠道商品分发场景，包括商品卡片、短视频与直播。具体而言，它由三大关键组件构成，使整条模型训练流与大语言模型的预训练/后训练范式深度对齐：  \n(1) 电商语义 Tokenizer：我们提出一种 Tokenizer 解决方案，既能编码真实世界的语义，又能刻画跨场景的商品级业务关系；  \n(2) 基于 Transformer 的架构：以 Transformer 为核心主干，引入 Query-Former 压缩超长序列，利用 Cross Attention 融合多行为序列，并以稀疏 MoE 支撑可扩展的自回归生成；  \n(3) 强化学习管线：通过强化学习将检索与排序模型紧密耦合，让排序模型作为奖励信号，直接优化端到端的策略式检索模型。  \n\n大量实验表明，OneMall 在所有电商场景均取得显著提升：商品卡片 GMV +13.01%，短视频订单 +15.32%，直播订单 +2.78%。目前 OneMall 已全面上线，服务快手超过 4 亿日活用户。"
    },
    {
        "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
        "summary": "General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.",
        "entry_id": "http://arxiv.org/abs/2601.21759v1",
        "pub_date": "2026-01-29",
        "translated_summary": "通用开放域稠密检索系统通常需要在海量、多元且任务各异的语料库上进行训练。如何科学地对这些语料和任务进行采样？传统方法要么简单按数据量比例均匀采样，要么依赖人工先验。尽管训练数据采样策略对模型效果至关重要，但在面向嵌入模型的场景下，采样策略的最优性尚未被充分研究。\n\n为此，我们提出 Inf‑DDS，一种全新的、由强化学习驱动的采样框架。该方法利用基于影响力的轻量级奖励信号，自适应地对训练数据集重新赋权，显著降低 GPU 开销。Inf-DDS 以迭代方式持续优化采样策略，优先选择对目标开发集指标提升最有效的数据集。\n\n我们在广泛文本检索任务上评估了该方法。结果表明，Inf-DDS 在检索效果和跨任务适应性上均明显优于现有基于梯度的采样方式，而 GPU 计算成本仅为 1.5–4 分之一。当以此策略训练多语言 bge-m3 模型时，NDCG@10 提升 5.03；训练 all-MiniLM-L6-v2 时，即使以专家给出的初始权重为起点、在超大数据池上训练，NDCG@10 仍提升 0.94。"
    },
    {
        "title": "Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning",
        "summary": "Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.",
        "entry_id": "http://arxiv.org/abs/2601.21700v1",
        "pub_date": "2026-01-29",
        "translated_summary": "大型语言模型（LLM）在促进文化敏感决策方面的作用日益增强，然而，受预训练数据倾斜和缺乏结构化价值表征的影响，它们仍表现出显著的价值错位。现有方法虽能引导输出，但往往缺乏人口学情境基础，并将价值观视为独立、非结构化的信号，导致一致性与可解释性均受损。我们提出 OG-MAR——一套“基于本体的多智能体推理”框架。OG-MAR 从《世界价值观调查》中提炼受访者特定价值观，针对固定分类体系，借助能力问询手段捕获价值之间的关联关系，从而构建全球文化本体。推理阶段，OG-MAR 先检索与本体一致的关系，并召回人口统计属性相似的档案，据此实例化多位“价值—角色”智能体；随后由一名“裁判智能体”对这些智能体的输出进行综合，既强制保证本体一致性，又确保与目标群体的统计邻近度。我们在覆盖四大地域的社会调查基准测试上、以四种主流 LLM 为基底模型展开实验，结果显示：OG-MAR 在文化对齐度与鲁棒性两方面均优于具竞争性的基线，同时能生成更清晰可溯的推理脉络。"
    },
    {
        "title": "Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance",
        "summary": "Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.",
        "entry_id": "http://arxiv.org/abs/2601.21611v1",
        "pub_date": "2026-01-29",
        "translated_summary": "有效的相关性建模是电商搜索的核心，因为它能够让搜索结果精准对齐用户意图并提升客户体验。近期研究借助大语言模型（LLM）弥补了传统相关性模型对长尾、模糊查询处理能力的不足。链式思维（CoT）推理被引入后，通过多步逻辑进一步提升了准确性与可解释性。然而，仍有两项关键缺陷：第一，现有方法通常仅靠单一视角的 CoT 推理，难以涵盖电商相关性多面并重的特质（例如既要理解用户意图，又得满足属性级匹配，还需符合商业规则）；第二，尽管引入 CoT 的大模型推理能力丰富，但其推理延迟高，必须知识蒸馏才能上线，而现有蒸馏方法在推断阶段会把 CoT 结构全部舍弃，仅在训练时把它作为一次性辅助信号，导致推理优势无法落地。\n\n针对上述难题，本文提出一种新颖框架，让 CoT 语义贯穿整个优化链路。具体而言，教师模型采用多视角链式思维（MPCoT）生成多样化推理链，并用监督式微调（SFT）与直接偏好优化（DPO）联合训练，打造出更稳健的教师推理器。在蒸馏阶段，我们设计“潜层推理知识蒸馏”（LRKD），通过为轻量化学生模型配备一个推断即用的潜层推理抽取器，在毫秒级延迟内高效内化大模型的复杂推理能力。\n\n我们在日活数千万用户的电商搜索广告平台上进行了离线实验与在线 A/B 测试。结果显示，本文方法不仅带来显著的离线指标提升，还在商业转化和用户体验上都展现出明确增益。"
    },
    {
        "title": "LMK > CLS: Landmark Pooling for Dense Embeddings",
        "summary": "Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.",
        "entry_id": "http://arxiv.org/abs/2601.21525v1",
        "pub_date": "2026-01-29",
        "translated_summary": "表征学习对搜索、聚类、分类和重排序等诸多下游任务至关重要。当前最先进的序列编码器通常采用汇聚操作，将可变长度的符号序列压缩为单一向量，最常用的是特殊 [CLS] 标记或对所有标记嵌入做均值汇聚。本文发现，这些汇聚策略存在系统性缺陷：[CLS] 往往把信息集中到序列前端，从而弱化分布出现的证据；而均值汇聚则可能稀释局部显著信号，在短上下文场景下表现反而变差。\n\n为应对这些问题，我们提出 Landmark（LMK）汇聚方法：将序列划分为若干子段，在各子段间插入 landmark 标记，并以这些 landmark 标记嵌入的均值生成最终表征。该机制在不牺牲局部显著特征的前提下，通过对序列进行轻量级的子段化，显著提升了长上下文外推能力，仅需引入少量特殊标记。\n\n实验表明，LMK 汇聚在短上下文检索任务上与现有方法持平，而在长上下文任务上带来显著提升，因而是一种实用且可扩展的替代方案。"
    },
    {
        "title": "MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset",
        "summary": "Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.",
        "entry_id": "http://arxiv.org/abs/2601.21512v1",
        "pub_date": "2026-01-29",
        "translated_summary": "阿拉伯语语言与文化底蕴深厚，词库庞大，覆盖科学、宗教、文学等诸多领域。然而，目前仍缺乏能精确关联阿拉伯词汇与释义的大规模词汇数据集。我们发布了 MURAD（Multi-domain Unified Reverse Arabic Dictionary，跨领域统一反向阿拉伯语词典），这是一个包含 96,243 组“词汇—释义”对的开放词汇数据集。数据来源于权威辞书、教材等专业参考作品；采用混合处理框架进行抽取，融合直接文本解析、光学字符识别与自动重组，确保内容准确、表述清晰。每条记录同时给出目标词的标准阿拉伯语释义，并附带指明来源领域的元数据。数据集涵盖语言学、伊斯兰研究、数学、物理、心理学、工程等多个学科，既可用于计算语言学研究，也为辞书编纂提供支持，潜在应用包括反向词典建模、语义检索与教育工具。通过开放共享，我们旨在推动阿拉伯语自然语言处理的发展，并为阿拉伯词汇语义研究提供可复现的资源。"
    },
    {
        "title": "When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning",
        "summary": "Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.",
        "entry_id": "http://arxiv.org/abs/2601.21208v1",
        "pub_date": "2026-01-29",
        "translated_summary": "查询优化是检索增强生成（RAG）系统效能的关键环节。尽管基于强化学习（RL）的智能体与推理方法近期被证实对查询优化极具前景，现有研究大多仍局限于对单一查询的扩展与抽象。然而，在真实场景里，复杂用户查询屡见不鲜，往往需并行或级联的多种检索策略才能完成歧义消消解及问题分解——直接以 RL 处理这类查询会遭遇显著阻碍：确定子问题数量、重新排序并合并检索结果，不仅急剧扩大了搜索空间，更使奖励设计举步维艰，常常导致训练不稳定。\n\n为破解上述难题，本文提出全新 RL 框架——自适应复杂查询优化（ACQO）。该框架可自适应决策何时、以及如何扩展搜索过程，包含两大核心模块：\n1. 自适应查询重构（AQR），动态判断何时应将查询分解为多个子问；\n2. 排名-分数融合（RSF），保证检索结果稳健聚合，并为学习代理提供稳定奖励信号。\n\n为抑制训练震荡，我们引入课程式强化学习（CRL），通过两阶段策略逐步增加任务复杂度，从而稳定训练流程。\n\n综合实验表明，ACQO 在三项复杂查询基准上均达到最优性能，显著优于既定基线；同时具备更优的计算效率，可无缝接入各类检索架构，成为下一代 RAG 系统强劲且泛化的通用优化方案。"
    },
    {
        "title": "A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning",
        "summary": "Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.",
        "entry_id": "http://arxiv.org/abs/2601.21162v1",
        "pub_date": "2026-01-29",
        "translated_summary": "图检索增强生成（Graph-RAG）通过将文本库构建成知识图谱，并沿着关系结构来汇聚证据，从而提升多跳问答的性能。然而在实际部署中，有两个顽疾始终存在：其一，混合难度的查询负载导致“一刀切”的检索既在简单查询上浪费成本，又在困难多跳场景下力有不逮；其二，抽象过程造成信息丢失，图结构省略的那些细粒度限定词仍仅存储在原始文本中。我们提出 A2RAG，这是一个自适应且智能的 GraphRAG 框架，兼顾成本与可靠性。其核心包括：一个自适应控制器，能够实时判断证据的充分性，并在必需时才触发针对性精炼；以及一个智能检索器，能在多轮中逐步加大检索力度，并把图信号映射回原始文本，以在信息缺失和不完整图谱下保持鲁棒。在 HotpotQA 与 2WikiMultiHopQA 上的实验表明，A2RAG 在 Recall@2 指标上获得 +9.9/+11.8 的绝对提升，同时将令牌消耗与端到端延迟均削减约 50%，显著优于现有的迭代多跳基线。"
    },
    {
        "title": "SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation",
        "summary": "Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.",
        "entry_id": "http://arxiv.org/abs/2601.21105v1",
        "pub_date": "2026-01-28",
        "translated_summary": "近年来，自然语言用户画像因其可解释性增强，以及有望让推荐系统变得“可操控”而备受关注。借助可直接编辑的特性，用户能通过自然语言明确表达那些难以从过往行为推断出的偏好。然而，目前仍不清楚现有基于自然语言的推荐方法是否真的能遵循此类操控指令。尽管已有关于可操控性的评测在电影类型等广为人知的属性上取得了一定成功，但我们指出此类基准无法反映驱使研究人员开发可操控推荐系统的更丰富用户控制场景。为弥补这一缺口，我们提出 SteerEval——一个旨在衡量更细致、更多样化操控能力的评估框架，其干预维度从电影类型延伸至内容警告提示。我们利用该框架评估一族预训练的自然语言推荐器，探究对相对小众主题的操控潜力与局限，并比较不同画像及推荐层次干预对操控效果的影响。最后，我们基于研究发现给出具体的设计建议，并展望可操控推荐系统的后续研究方向。"
    },
    {
        "title": "$\\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval",
        "summary": "This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of \"distances\" or \"similarities,\" including the $\\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.",
        "entry_id": "http://arxiv.org/abs/2601.20844v2",
        "pub_date": "2026-01-28",
        "translated_summary": "本研究探讨将子集隶属关系（m 个元素及其大小 ≤ k 的所有子集，共 {m choose k} 个）映射到向量空间时所需的最小可行维度，即“最小可嵌入维度”（MED）。我们为多个不同的“距离”或“相似度”定义，包括 ℓ₂ 距离、内积和余弦相似度，从理论推导出确切的上下界，并通过实验结果加以验证。此外，在更具可实现性的场景中，我们将各子集表示为其所属元素嵌入的质心，并进行数值模拟。实验表明，MED 与待嵌入元素数量之间呈现对数级依赖关系。这意味着，基于嵌入的检索方法的主要限制并非几何约束，而是可学习性瓶颈，从而为后续算法设计指明方向。"
    },
    {
        "title": "OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning",
        "summary": "Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\\sim$90\\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.",
        "entry_id": "http://arxiv.org/abs/2601.23085v1",
        "pub_date": "2026-01-30",
        "translated_summary": "在面对包含多种约束的复杂信息需求时，检索系统必须把查询中编码的逻辑运算符（即与、或、非）作用于候选答案集合。现有的方法要么在神经嵌入中忽略这些约束，要么通过生成式推理过程加以近似，而这种方式常出现不一致且不可靠。专为结构化推理设计的神经-符号方法，则因通常假设查询无歧义并可获得完备证据，被局限于形式逻辑或数学题，难以符合信息检索中的常见场景。为此，我们提出 OrLog，一个神经-符号检索框架，把谓词级别的可合理度估计与逻辑推理解耦：大语言模型（LLM）在一次无解码的前向传播中，为原子谓词提供可合理度分数；再借助概率推理引擎，计算查询被满足的后验概率。我们在多个主干 LLM、不同外部知识获取程度以及多种逻辑约束条件下评估 OrLog，并将其与基线检索器、以 LLM 为推理器的方法对比。若配备实体描述，OrLog 相比 LLM 推理可显著提升 top-k 精度，尤其在析取查询优势更大，且推理更高效，每查询-实体对平均节省 ≈90% 的令牌。结果表明，无生成的谓词可合理度估算结合概率推理，可实现兼顾约束的检索，性能优于单一推理同时令牌消耗大幅降低。"
    },
    {
        "title": "BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models",
        "summary": "Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.\n  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.",
        "entry_id": "http://arxiv.org/abs/2601.22925v1",
        "pub_date": "2026-01-30",
        "translated_summary": "近年来，利用大语言模型（LLM）进行推荐的研究迅速兴起。这类方法通常采用监督式微调（SFT）使 LLM 适应推荐场景，并在推理阶段使用束搜索高效地检索前 B 个推荐物品。然而，我们发现一个关键的训练—推理不一致现象：SFT 优化的是正样本整体的概率，却无法保证这些物品即使整体概率很高也会被束搜索成功检索。由于束搜索的贪婪剪枝机制，一旦某个正样本的前缀概率不足，就会在其仍未完整生成的时候被提前丢弃。\n\n为消除这一不一致，我们提出 BEAR（Beam-Search-Aware Regularization）。它是一种新的微调目标，显式考虑训练过程中的束搜索行为。针对在每个训练实例上完整模拟束搜索计算成本过高的问题，BEAR 转而引入一个宽松的必要约束：在每一步解码中，正样本序列的每个词元都必须位于候选词元的前 B 位。该目标在几乎不给标准 SFT 增加额外计算开销的前提下，有效避免了错误剪枝。在四个真实世界数据集上的大量实验表明，BEAR 显著优于强劲基线。代码将在论文被接受后开源。"
    },
    {
        "title": "Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval",
        "summary": "Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.",
        "entry_id": "http://arxiv.org/abs/2601.22783v1",
        "pub_date": "2026-01-30",
        "translated_summary": "大规模生物多样性监测平台越来越依赖多模态野生动物观测数据。尽管近期涌现的基础模型能够为图像、音频和语言提取丰富的语义表征，但在海量档案中进行相关观测检索仍十分棘手：高维相似度搜索的计算开销巨大。为此，本文提出“紧凑超立方体嵌入”（compact hypercube embedding）框架，用于基于文本快速检索野生动物观测记录，只需使用紧凑的二进制表示即可在大型图像与音频数据库中高效实现文本检索。我们以跨视角码对齐哈希框架为基础，将其从单一模态扩展到多模态：把自然语言描述与视觉或声学观测对齐到统一的汉明空间。该方法调用经过预训练的野生动物基础模型（包括 BioCLIP 和 BioLingual），并借助参数高效的微调将其改造成轻量级哈希编码器。我们在 iNaturalist2024（文本-图像检索）和 iNatSounds2024（文本-音频检索）两个大规模基准以及多个声景数据集上评估性能，以测试领域漂移下的鲁棒性。实验表明，离散超立方体嵌入在多项指标上与连续嵌入性能相当甚至更佳，同时显著降低内存与搜索成本。此外，哈希目标函数进一步增强了底层编码器的表征能力，带来更优的检索效果与零样本泛化。这些结果证明，基于语言的二进制检索可为生物多样性监测系统提供可扩展、高效的海量野生动物档案搜索能力。"
    },
    {
        "title": "Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens",
        "summary": "Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.",
        "entry_id": "http://arxiv.org/abs/2601.22694v1",
        "pub_date": "2026-01-30",
        "translated_summary": "近期关于扩大排序模型规模的研究已为推荐系统和搜索引擎带来显著提升。然而，多数大规模排序系统仍依赖物品 ID 将每个物品视为独立离散符号，并映射为可学习的嵌入向量。随着物品频繁地出现与消失，这类嵌入难以高效训练与维护，其不稳定性阻碍了神经网络参数的有效学习，也限制了排序模型的扩展能力。本文证明，相较于物品 ID，语义令牌具备更强的规模扩展潜力。我们提出的 TRM（Token-based Ranking Model）框架，通过对令牌生成与应用的端到端流程进行优化，使稀疏存储降低 33%，同时 AUC 提升 0.85%。大量实验表明，随模型容量增加，TRM 能够持续优于现有最佳模型。此外，TRM 已在大型个性化搜索系统中成功上线，经 A/B 测试带来用户活跃天数提升 0.26%、修改查询率提升 0.75%。"
    },
    {
        "title": "PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing",
        "summary": "Short-video platforms rely on personalized recommendation, raising concerns about filter bubbles that narrow content exposure. Auditing such phenomena at scale is challenging because real user studies are costly and privacy-sensitive, and existing simulators fail to reproduce realistic behaviors due to their reliance on textual signals and weak personalization. We propose PersonaAct, a framework for simulating short-video users with persona-conditioned multimodal agents trained on real behavioral traces for auditing filter bubbles in breadth and depth. PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning, then trains agents on multimodal observations using supervised fine-tuning and reinforcement learning. We deploy trained agents for filter bubble auditing and evaluate bubble breadth via content diversity and bubble depth via escape potential. The evaluation demonstrates substantial improvements in fidelity over generic LLM baselines, enabling realistic behavior reproduction. Results reveal significant content narrowing over interaction. However, we find that Bilibili demonstrates the strongest escape potential. We release the first open multimodal short-video dataset and code to support reproducible auditing of recommender systems.",
        "entry_id": "http://arxiv.org/abs/2601.22547v1",
        "pub_date": "2026-01-30",
        "translated_summary": "短视频平台高度依赖个性化推荐，这引发了用户对“信息茧房”加剧、内容视野变窄的担忧。然而，要在平台规模上审计此类现象困难重重：真实用户实验成本高昂且涉及隐私，而现有模拟器过度依赖文本信号并实现微弱的个性化，难以再现现实用户行为。为此，我们提出 PersonaAct，一个可规模化的审计框架：借助“人格条件化的多模态智能体”来模拟短视频用户，使其在广度与深度两个维度测量信息茧房。PersonaAct 通过“自动访谈”来合成可解释的人格画像——将行为分析与结构化提问相结合，随后以真实行为痕迹为监督信号，对多模态观测进行监督微调和强化学习，训练出高保真代理。我们用训练好的代理开展茧房审计：以内容多样性衡量茧房“广度”，以“逃脱潜力”衡量茧房“深度”。实验表明，PersonaAct 在行为保真度上远超通用 LLM 基线，能够真实再现用户行为；在交互过程中，所有平台均出现了明显的内容窄化，而 B 站展现出最强的逃脱潜力。作为补充，我们开源了首个多模态短视频数据与代码，可为推荐系统可复现审计提供基础设施。"
    },
    {
        "title": "SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation",
        "summary": "Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.\n  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.",
        "entry_id": "http://arxiv.org/abs/2601.22543v1",
        "pub_date": "2026-01-30",
        "translated_summary": "云-端协同推荐将计算任务拆分给云端与用户设备两端：云端负责语义化的用户建模，设备端则利用最新交互记录以及云端下发的语义信号，在保护隐私的前提下实时地完成重排。云端部署的大语言模型（LLM）通过捕捉高层次的意图，使语义用户表征能够显著提升序列化推荐效果。然而，在现实规模下，每来一条请求都调用云端 LLM 重新生成表征往往不可行，因此设备端重排通常只能复用缓存的云端语义用户嵌入。我们经验性地发现一种“云端语义陈旧效应”：被反复复用的嵌入会逐渐偏离用户最新交互，从而导致可量化的排序性能下降。\n\n现有的大多数 LLM 赋能的云端-设备推荐系统都基于“按需提供云端语义”的思路设计，要么假设云端 LLM 可低延迟访问，要么坚持每次请求都重新生成语义嵌入。当每请求再生难以实现、只能复用缓存语义时，就面临两大技术挑战：(1) 如何判断缓存云端语义是否仍可用于设备端重排；(2) 在无法调用云端 LLM、仅有缓存语义的情境下，如何维持排序质量。\n\n为弥补这一空缺，我们提出了面向 LLM 赋能云-端推荐的语义校准框架 SCaLRec。首先，它估算缓存语义在用户最新交互后的可靠度。其次，提出一种设备端语义校准模块，仅基于当下交互证据，在设备上原位微调缓存语义嵌入，而无需每次请求都调用云端 LLM。在真实数据集上的实验表明，SCaLRec 在云端语义陈旧的情境下，始终优于多种强基线模型，持续提升推荐性能。"
    },
    {
        "title": "FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning",
        "summary": "Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.",
        "entry_id": "http://arxiv.org/abs/2601.22498v1",
        "pub_date": "2026-01-30",
        "translated_summary": "多模态推荐旨在通过利用图像、文本等丰富的物品内容来增强用户偏好建模。然而，主流系统在空间维度融合不同模态，既扰乱了信号的频域结构，也放大了对齐误差与冗余。本文引入谱域信息论视角，证明：经正交变换近似对带内协方差进行块对角化后，高斯信息瓶颈目标可在不同频带间解耦，从而为“先分离后融合”范式奠定理论基础。据此，我们提出 FITMM（Frequency-aware Information-Theoretic framework for Multimodal recommendation）：先构建图增强的物品表示，再进行逐模态的谱分解，得到正交的频带；随后在每个频带内生成轻量级的多模态分量；最后由残差式、任务自适应的门控机制聚合各频带形成最终表示。为控制冗余并提升泛化，我们在训练中引入频域信息瓶颈正则项，通过类似 Wiener 的收缩操作关闭弱势频带，从而在不同频带上动态分配容量。同时，我们提出跨模态谱一致性损失，在每一频带内实现模态对齐。模型最终与标准推荐损失联合优化。在三个真实数据集的广泛实验表明，FITMM 均显著优于现有先进基线。"
    },
    {
        "title": "Do AI Overviews Benefit Search Engines? An Ecosystem Perspective",
        "summary": "The integration of AI Overviews into search engines enhances user experience but diverts traffic from content creators, potentially discouraging high-quality content creation and causing user attrition that undermines long-term search engine profit. To address this issue, we propose a game-theoretic model of creator competition with costly effort, characterize equilibrium behavior, and design two incentive mechanisms: a citation mechanism that references sources within an AI Overview, and a compensation mechanism that offers monetary rewards to creators. For both cases, we provide structural insights and near-optimal profit-maximizing mechanisms. Evaluations on real click data show that although AI Overviews harm long-term search engine profit, interventions based on our proposed mechanisms can increase long-term profit across a range of realistic scenarios, pointing toward a more sustainable trajectory for AI-enhanced search ecosystems.",
        "entry_id": "http://arxiv.org/abs/2601.22493v1",
        "pub_date": "2026-01-30",
        "translated_summary": "将 AI 概览整合进搜索引擎，虽能提升用户体验，却会截走内容创作者的流量，抑制高质量内容的生产积极性，并带来用户流失的双重风险，进而长期削弱搜索引擎利润。我们为此构建了一个计及代价努力的内容创作者博弈竞争模型，刻画均衡行为，并设计两种补救机制：一是引用机制——在 AI 概览中明确标注信息源；二是补偿机制——向创作者提供现金奖励。两种情况下，我们都给出了结构性见解与近似最优的利润最大化设计。基于真实点击量数据的评估显示，尽管 AI 概览在缺乏干预时将损害长期利润，但采纳本文提出的机制可将长期利润在多种现实情景中显著提升，为 AI 增强型搜索生态开辟出更加可持续的发展路径。"
    },
    {
        "title": "Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning",
        "summary": "Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.",
        "entry_id": "http://arxiv.org/abs/2601.21700v2",
        "pub_date": "2026-01-29",
        "translated_summary": "大型语言模型（LLM）越来越支持具备文化敏感性的决策，但由于预训练数据倾斜且缺乏结构化的价值表征，往往出现价值错位。现有输出引导方法大多缺乏人口学依据，并将价值观视为相互独立、无结构的信号，从而导致一致性与可解释性降低。我们提出 OG-MAR——一种“本体引导的多智能体推理”框架。OG-MAR 首先从世界价值观调查（WVS）中总结受访者特有价值观，并在固定分类体系上通过“能力问题”诱导关系，构建全球文化本体。在推理阶段，框架检索与本体一致的关系和人口特征相似的资料，实例化多个“价值-人格”智能体；其输出由一名“裁判智能体”综合，强制保证本体一致和人口邻近。在以四个 LLM 主干为基座的区域社会调查基准上的实验表明，OG-MAR 相较竞争性基线提升了文化对齐度和鲁棒性，同时生成了更透明的推理轨迹。"
    },
    {
        "title": "Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation",
        "summary": "E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.\n  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.\n  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.\n  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.",
        "entry_id": "http://arxiv.org/abs/2602.00899v1",
        "pub_date": "2026-01-31",
        "translated_summary": "电商推荐与搜索通常依赖稀疏关键词匹配（如 BM25），当用户意图与商品元数据之间的词汇重叠极少时，该方法会因词汇不匹配而失效。我们将基于内容的推荐任务建模为“检索式推荐”：给定一段自然语言意图信号（查询或评论），从大规模商品库中依据语义相似度检索最相关的 top-K 商品。\n\n我们提出一种可扩展的稠密检索系统，该系统基于双塔编码器（two-tower bi-encoder），在 Amazon Reviews 2023 Fashion 子集上使用监督对比学习和多负例排序损失进行微调。训练样本以评论文本作为查询代理、商品元数据作为正例文档构建，并对 5 万条抽样交互进行微调，输入序列最大长度设为 500 tokens。\n\n为实现高效线上服务，我们将 FAISS 的 HNSW 索引与 ONNX Runtime 推理流水线相结合，并采用 INT8 动态量化。在包含 826,402 件商品的“评论→商品标题”基准测试中，本方法的 Recall@10 由 BM25 的 0.26 提升至 0.66，同时满足实际延迟和模型大小约束：批次大小为 1 时 CPU 推理中位延迟 6.1 毫秒，模型体积压缩至原来的 1/4。\n\n总体而言，我们提供了一套端到端、可复现的操作指南，实现领域自适应稠密检索从离线训练到 CPU 高效上线的全流程部署。"
    },
    {
        "title": "Unifying Adversarial Robustness and Training Across Text Scoring Models",
        "summary": "Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.",
        "entry_id": "http://arxiv.org/abs/2602.00857v1",
        "pub_date": "2026-01-31",
        "translated_summary": "当前，大语言模型的对抗鲁棒性研究在应用场景和攻击方法之间严重割裂，导致共通漏洞难以被系统识别。为此，我们在文本评分模型的统一视角下（包括稠密检索器、重排器和奖励模型），首次系统构建了对抗鲁棒性研究框架。该视角将攻击与对抗训练的适配，从开放领域生成转移到可直接验证的评分任务：只要无关或拒绝文本的得分高于相关或被选中文本，即可视为攻击成功。基于这一明确标准，我们发现现有对语言模型的对抗训练往往目光短浅，难以跨攻击范型泛化。据此，我们针对文本评分模型提出多种互补的对抗训练策略，并证明联合使用可显著增强鲁棒性，且同步提升任务性能。此外，我们在 RLHF 场景中验证了方法的实用价值：经对抗训练的奖励模型能有效对抗 reward hacking，并助力对齐程度更高的 LLM 训练。代码与模型已开源，供后续研究使用。"
    },
    {
        "title": "Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training",
        "summary": "Recent advances in embedding-based retrieval have enabled dense retrievers to serve as core infrastructure in many industrial systems, where a single retrieval backbone is often shared across multiple downstream applications. In such settings, retrieval quality directly constrains system performance and extensibility, while coupling model selection, deployment, and rollback decisions across applications.\n  In this paper, we present empirical findings and a system-level solution for optimizing retrieval components deployed as a shared backbone in production legal retrieval systems. We adopt a multi-stage optimization framework for dense retrievers and rerankers, and show that different retrieval components exhibit stage-dependent trade-offs. These observations motivate a component-wise, mixed-stage configuration rather than relying on a single uniformly optimal checkpoint. The resulting backbone is validated through end-to-end evaluation and deployed as a shared retrieval service supporting multiple industrial applications.",
        "entry_id": "http://arxiv.org/abs/2602.00805v1",
        "pub_date": "2026-01-31",
        "translated_summary": "近期，基于嵌入表示的检索技术取得显著进展，使得密集检索器已成为众多工业系统的核心基础设施，通常由单一检索骨干为多个下游应用共享。在此环境下，检索质量直接限制了整体系统的性能与可扩展性，并且各应用在模型选择、部署及回滚决策上相互耦合。  \n\n本文针对生产环境中的法律检索系统，提出了实证研究结果与系统级解决方案，旨在优化作为共享骨干部署的检索组件。我们采用多阶段优化框架，联合优化密集检索器与再排序器，发现各检索组件在不同阶段呈现阶段相关的性能权衡。基于这一观察，我们建议放弃单一“全局最优”检查点的做法，转而采用组件级、混合阶段配置。所得检索骨干经端到端评估验证，已作为共享检索服务部署，支撑多项工业应用。"
    },
    {
        "title": "SpeechLess: Micro-utterance with Personalized Spatial Memory-aware Assistant in Everyday Augmented Reality",
        "summary": "Speaking aloud to a wearable AR assistant in public can be socially awkward, and re-articulating the same requests every day creates unnecessary effort. We present SpeechLess, a wearable AR assistant that introduces a speech-based intent granularity control paradigm grounded in personalized spatial memory. SpeechLess helps users \"speak less,\" while still obtaining the information they need, and supports gradual explicitation of intent when more complex expression is required. SpeechLess binds prior interactions to multimodal personal context-space, time, activity, and referents-to form spatial memories, and leverages them to extrapolate missing intent dimensions from under-specified user queries. This enables users to dynamically adjust how explicitly they express their informational needs, from full-utterance to micro/zero-utterance interaction. We motivate our design through a week-long formative study using a commercial smart glasses platform, revealing discomfort with public voice use, frustration with repetitive speech, and hardware constraints. Building on these insights, we design SpeechLess, and evaluate it through controlled lab and in-the-wild studies. Our results indicate that regulated speech-based interaction, can improve everyday information access, reduce articulation effort, and support socially acceptable use without substantially degrading perceived usability or intent resolution accuracy across diverse everyday environments.",
        "entry_id": "http://arxiv.org/abs/2602.00793v1",
        "pub_date": "2026-01-31",
        "translated_summary": "在公共场合对可穿戴 AR 助手大声说话常令人感到尴尬，而每天都得重复同样的语音请求也徒增负担。本文提出 SpeechLess——一款依托“个性化空间记忆”的言语意图粒度控制范式的可穿戴 AR 助手。SpeechLess 让用户“少说话”就能获取所需信息，并在需要更复杂表达时，支持逐步、渐进地显式化意图。该系统把先前的交互与多模态个人情境（空间、时间、活动、指称对象）绑定，形成空间记忆，借此补全用户语音查询中残缺的意图维度，从而让用户在完整语句到微/零语句之间动态地调整其信息需求的表达方式。  \n我们通过一项持续一周的形成性研究（使用市售智能眼镜）来指导设计：研究发现用户在公共场合使用语音时感到不适、对重复说话感到厌烦，并且硬件存在制约。在此基础上，我们设计并实现 SpeechLess，并通过受控实验室研究与现场实地研究进行评估。结果表明，这种受控的语音交互能在各种日常环境中让信息获取更便利、降低表达负担、维持社交可接受性，同时不会显著牺牲可感知可用性或意图解析准确率。"
    },
    {
        "title": "Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting",
        "summary": "Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.",
        "entry_id": "http://arxiv.org/abs/2602.00758v1",
        "pub_date": "2026-01-31",
        "translated_summary": "搜索引擎的日期过滤器被广泛用于强制“截止点前检索”，以回顾性地评估基于搜索的预测模型。我们发现这种方法并不可靠：以 Google 搜索为例，使用 before: 指令过滤后，71% 的问题仍能返回至少一个包含“强后截止信息泄露”的网页；41% 的问题甚至直接出现了答案。我们让大型语言模型 gpt-oss-120b 在“泄露网页”上继续预测，结果预测准确度被人为推高（Brier 分数由 0.242 降至 0.108）。针对常见的泄露机制，我们归纳出：文章事后更新、相关推荐模块、有缺陷的元数据或时间戳，以及“缺失即为信号”等四种典型方式，并指出单纯依赖“日期过滤”无法保证时间维度的评测可信。因此，我们建议在检索阶段设置更严格的防护措施，或直接基于已冻结的、带时间戳的网络快照进行评估，以确保回顾性预测研究的可信度。"
    },
    {
        "title": "Towards Trustworthy Multimodal Recommendation",
        "summary": "Recent advances in multimodal recommendation have demonstrated the effectiveness of incorporating visual and textual content into collaborative filtering. However, real-world deployments raise an increasingly important yet underexplored issue: trustworthiness. On modern e-commerce platforms, multimodal content can be misleading or unreliable (e.g., visually inconsistent product images or click-bait titles), injecting untrustworthy signals into multimodal representations and making existing recommenders brittle under modality corruption. In this work, we take a step towards trustworthy multimodal recommendation from both a method and an analysis perspective. First, we propose a plug-and-play modality-level rectification component that mitigates untrustworthy modality features by learning soft correspondences between items and multimodal features. Using lightweight projections and Sinkhorn-based soft matching, the rectification suppresses mismatched modality signals while preserving semantic consistency, and can be integrated into existing multimodal recommenders without architectural modifications. Second, we present two practical insights on interaction-level trustworthiness under noisy collaborative signals: (i) training-set pseudo interactions can help or hurt performance under noise depending on prior-signal alignment; and (ii) propagation-graph pseudo edges can also help or hurt robustness, as message passing may amplify misalignment. Extensive experiments on multiple datasets and backbones under varying corruption levels demonstrate improved robustness from modality rectification and validate the above interaction-level observations.",
        "entry_id": "http://arxiv.org/abs/2602.00730v1",
        "pub_date": "2026-01-31",
        "translated_summary": "多模态推荐近期的进展证明了在协同过滤中融合视觉与文本内容的有效性。然而，在实际部署中，一个日益重要却尚未被充分研究的问题逐渐凸显：可信性。在现代电商平台上，多模态内容可能存在误导或不可靠（例如，物品图片视觉不一致、标题为“标题党”），将不可信信号注入多模态表征，使现有推荐器在面对模态数据损坏时表现出脆弱性。\n\n为此，本文从方法和分析两个角度出发，向可信多模态推荐迈出了第一步。首先，我们提出一个即插即用的模态级矫正组件。该组件通过为物品与多模态特征之间学习软对应关系，来削弱不可靠模态特征的影响。借助轻量级投影与基于 Sinkhorn 的软匹配，矫正能够在抑制误配模态信号的同时保持语义一致性，并且无需对已有架构进行修改即可嵌入现有多模态推荐器。\n\n其次，我们从交互级可信性角度提供了两条针对噪声协同信号的实践洞察：（i）在噪声环境下引入伪交互能否提升性能，取决于噪声信号与先验信号是否对齐；（ii）在传播图中加入伪边能否提升鲁棒性，也可能因消息传递进一步放大误对齐而适得其反。\n\n在多数据集、多骨干模型及不同损坏程度的广泛实验表明，模态矫正显著提升了鲁棒性，经验性地验证了上述交互级见解。"
    },
    {
        "title": "SWGCN: Synergy Weighted Graph Convolutional Network for Multi-Behavior Recommendation",
        "summary": "Multi-behavior recommendation paradigms have emerged to capture diverse user activities, forecasting primary conversions (e.g., purchases) by leveraging secondary signals like browsing history. However, current graph-based methods often overlook cross-behavioral synergistic signals and fine-grained intensity of individual actions. Motivated by the need to overcome these shortcomings, we introduce Synergy Weighted Graph Convolutional Network (SWGCN). SWGCN introduces two novel components: a Target Preference Weigher, which adaptively assigns weights to user-item interactions within each behavior, and a Synergy Alignment Task, which guides its training by leveraging an Auxiliary Preference Valuator. This task prioritizes interactions from synergistic signals that more accurately reflect user preferences. The performance of our model is rigorously evaluated through comprehensive tests on three open-source datasets, specifically Taobao, IJCAI, and Beibei. On the Taobao dataset, SWGCN yields relative gains of 112.49% and 156.36% in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), respectively. It also yields consistent gains on IJCAI and Beibei, confirming its robustness and generalizability across various datasets. Our implementation is open-sourced and can be accessed via https://github.com/FangdChen/SWGCN.",
        "entry_id": "http://arxiv.org/abs/2602.00727v1",
        "pub_date": "2026-01-31",
        "translated_summary": "多行为推荐范式应运而生，旨在利用浏览历史等二级信号捕捉多样化的用户行为，并预测主要转化（如购买）。然而，现有图方法往往忽视跨行为协同信号，也缺乏对个体行为细粒度强度的刻画。针对这些局限，本文提出 Synergy Weighted Graph Convolutional Network（SWGCN）。SWGCN引入两大创新组件：一是 Target Preference Weigher，自适应地为同一行为内的用户-物品交互分配权重；二是 Synergy Alignment Task，借助辅助偏好评估器（Auxiliary Preference Valuator）引导训练，使模型优先关注能够更准确反映用户偏好的协同信号。我们在三个开源数据集（Taobao、IJCAI 和 Beibei）上进行了全面评估：在 Taobao 上，SWGCN 的命中率（HR）和归一化折损累计增益（NDCG）分别相对提升 112.49% 和 156.36%，并于 IJCAI 和 Beibei 保持一致的增益，验证了其跨数据集的鲁棒性与泛化能力。代码已开源：https://github.com/FangdChen/SWGCN。"
    },
    {
        "title": "From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development",
        "summary": "Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.",
        "entry_id": "http://arxiv.org/abs/2602.00699v1",
        "pub_date": "2026-01-31",
        "translated_summary": "本体（Ontology）对于领域知识的结构化、可访问性提升以及共享与重用具有关键作用。然而，传统本体构建依赖人工标注与经典自然语言处理（NLP）技术，尤其在铸造制造等专业领域，过程既费时又昂贵。大语言模型（LLM）的兴起为自动知识抽取提供了新途径。本研究提出三种基于 LLM 的方法——预训练 LLM 驱动法、上下文学习（ICL）法与微调法——在少量数据条件下，从领域文本中提取术语及其关系。我们比较了三种方法的性能，并选用效果最佳者构建铸造本体，结果得到了领域专家的验证。"
    },
    {
        "title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment",
        "summary": "Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.",
        "entry_id": "http://arxiv.org/abs/2602.00682v1",
        "pub_date": "2026-01-31",
        "translated_summary": "多模态推荐系统通常将用户行为与物品的多模态数据相结合，以捕捉更加精确的用户偏好。随着大模型（LMs）兴起，多模态推荐也愈发借助其在语义理解与情境推理方面的优势。然而，大模型的表示天然面向通用语义任务，而推荐模型却严重依赖用户/物品稀疏标识（ID）特征。现有研究忽视了二者在表示空间上的根本差异，导致多模态表征不兼容、推荐效果不佳。为弥合这一鸿沟，我们提出 RecGOAT——一种简洁而新颖的双语义对齐框架，用于 LLM 增强的多模态推荐，并在理论上保证对齐能力。首先，RecGOAT 利用图注意力网络，建模物品–物品、用户–物品及用户–用户关系，借助大模型生成的用户/物品语义表示和交互历史来丰富协同语义。其次，我们设计了一个双粒度渐进式“多模态-标识”对齐框架，分别通过跨模态对比学习（CMCL）实现实例级语义对齐，并通过最优自适应传输（OAT）完成分布级语义对齐。理论上，我们证明统一后的大幅提升的语义一致性和全面性。在三个公开基准上的大量实验表明 RecGOAT 达到业内最优性能，实验结果进一步验证了我们的理论洞见。此外，该模型在大型在线广告平台上线的效果验证其工业级推荐场景的有效性与可扩展性。代码开源：https://github.com/6lyc/RecGOAT-LLM4Rec。"
    },
    {
        "title": "Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation",
        "summary": "Audio-to-image retrieval offers an interpretable alternative to audio-only classification for bioacoustic species recognition, but learning aligned audio-image representations is challenging due to the scarcity of paired audio-image data. We propose a simple and data-efficient approach that enables audio-to-image retrieval without any audio-image supervision. Our proposed method uses text as a semantic intermediary: we distill the text embedding space of a pretrained image-text model (BioCLIP-2), which encodes rich visual and taxonomic structure, into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder with a contrastive objective. This distillation transfers visually grounded semantics into the audio representation, inducing emergent alignment between audio and image embeddings without using images during training. We evaluate the resulting model on multiple bioacoustic benchmarks. The distilled audio encoder preserves audio discriminative power while substantially improving audio-text alignment on focal recordings and soundscape datasets. Most importantly, on the SSW60 benchmark, the proposed approach achieves strong audio-to-image retrieval performance exceeding baselines based on zero-shot model combinations or learned mappings between text embeddings, despite not training on paired audio-image data. These results demonstrate that indirect semantic transfer through text is sufficient to induce meaningful audio-image alignment, providing a practical solution for visually grounded species recognition in data-scarce bioacoustic settings.",
        "entry_id": "http://arxiv.org/abs/2602.00681v1",
        "pub_date": "2026-01-31",
        "translated_summary": "音频-图像检索为生物声学物种识别提供了一种比纯音频分类更可解释的替代方式，但由于配对音频-图像数据稀缺，学习对齐的音频-图像表征极具挑战性。我们提出一种简单且数据高效的方法，无需任何音频-图像监督即可实现音频到图像的检索。其核心思想是将文本作为语义桥梁：将预训练图文模型 BioCLIP-2 的文本嵌入空间所蕴含的丰富视觉与分类学结构，蒸馏到预训练音频-文本模型 BioLingual 中。我们对其音频编码器进行微调，采用对比学习目标，把基于视觉的语义知识注入音频表征，在训练过程中完全不使用图像即可自发产生音频与图像嵌入的有效对齐。\n\n我们在多个生物声学基准数据集上评估所得模型。经蒸馏的音频编码器在保持音频判别能力的同时，显著提升了焦点录音和环境声景数据集中的音频-文本对准。最值得注意的是，在 SSW60 基准测试中，即使从未见过配对音频-图像数据，该方法仍取得了强劲的音频→图像检索性能，明显优于基于零样本模型组合或文本嵌入映射学习的基线。结果表明，通过文本进行的间接语义迁移足以促成有意义的音频-图像对齐，为数据稀缺的生物声学场景中实现具有视觉依据的物种识别提供了一种实用方案。"
    },
    {
        "title": "Towards Sample-Efficient and Stable Reinforcement Learning for LLM-based Recommendation",
        "summary": "While Long Chain-of-Thought (Long CoT) reasoning has shown promise in Large Language Models (LLMs), its adoption for enhancing recommendation quality is growing rapidly. In this work, we critically examine this trend and argue that Long CoT is inherently ill-suited for the sequential recommendation domain. We attribute this misalignment to two primary factors: excessive inference latency and the lack of explicit cognitive reasoning patterns in user behavioral data. Driven by these observations, we propose pivoting away from the CoT structure to directly leverage its underlying mechanism: Reinforcement Learning (RL), to explore the item space. However, applying RL directly faces significant obstacles, notably low sample efficiency-where most actions fail to provide learning signals-and training instability. To overcome these limitations, we propose RISER, a novel Reinforced Item Space Exploration framework for Recommendation. RISER is designed to transform non-learnable trajectories into effective pairwise preference data for optimization. Furthermore, it incorporates specific strategies to ensure stability, including the prevention of redundant rollouts and the constraint of token-level update magnitudes. Extensive experiments on three real-world datasets show that RISER significantly outperforms competitive baselines, establishing a robust paradigm for RL-enhanced LLM recommendation. Our code will be available at https://anonymous.4open.science/r/RISER/.",
        "entry_id": "http://arxiv.org/abs/2602.00632v1",
        "pub_date": "2026-01-31",
        "translated_summary": "尽管长链式思维（Long CoT）推理在大型语言模型（LLM）中已显示出潜力，但利用其提升推荐质量的研究正迅速升温。本文对此趋势进行了批判性审视，指出将 Long CoT 直接应用于序列推荐任务存在天然的错位。我们发现，其根本缺陷在于两点：过高的推理延迟，以及用户行为数据中缺乏显式的认知推理模式。\n\n基于上述观察，我们主张摒弃显式 CoT 结构，转而回归其底层机制：强化学习（RL），直接在物品空间进行探索。然而，直接应用 RL 面临两大挑战：1. 样本效率低下，大多数动作无法提供有效的学习信号；2. 训练过程不稳定。为突破这些局限，我们提出 RISER（Reinforced Item Space Exploration framework for Recommendation），一种新颖的强化物品空间探索框架。RISER 能够将原本无可学习的轨迹转化为有效的成对偏好数据，从而提升优化效率。此外，它通过抑制冗余 rollout 与限制 token 级更新幅度等策略，确保训练稳定。\n\n在三个真实世界数据集上的大量实验表明，RISER 显著超越多种竞争基线，为利用 RL 赋能 LLM 推荐开辟了稳健新范式。代码已开源：https://anonymous.4open.science/r/RISER/"
    },
    {
        "title": "Equity vs. Equality: Optimizing Ranking Fairness for Tailored Provider Needs",
        "summary": "Ranking plays a central role in connecting users and providers in Information Retrieval (IR) systems, making provider-side fairness an important challenge. While recent research has begun to address fairness in ranking, most existing approaches adopt an equality-based perspective, aiming to ensure that providers with similar content receive similar exposure. However, it overlooks the diverse needs of real-world providers, whose utility from ranking may depend not only on exposure but also on outcomes like sales or engagement. Consequently, exposure-based fairness may not accurately capture the true utility perceived by different providers with varying priorities. To this end, we introduce an equity-oriented fairness framework that explicitly models each provider's preferences over key outcomes such as exposure and sales, thus evaluating whether a ranking algorithm can fulfill these individualized goals while maintaining overall fairness across providers. Based on this framework, we develop EquityRank, a gradient-based algorithm that jointly optimizes user-side effectiveness and provider-side equity. Extensive offline and online simulations demonstrate that EquityRank offers improved trade-offs between effectiveness and fairness and adapts to heterogeneous provider needs.",
        "entry_id": "http://arxiv.org/abs/2602.00495v1",
        "pub_date": "2026-01-31",
        "translated_summary": "排名在信息检索（IR）系统中是连接用户与供给方的关键环节，因此供给端的公平性问题至关重要。虽然已有研究开始关注排名的公平性，但大多数方法依然基于“平等主义”视角，仅致力于让提供相似内容的供给方获得同等的曝光量。然而，这种方法忽视了现实世界中供给方的多元需求——他们不仅关注曝光，还关心销量、互动等实际收益。因此，仅以曝光量为衡量尺度的“平等”并不能反映不同供给方对效用的真实感受。为此，我们提出一个以“公正”为导向的公平框架：该框架显式建模每个供给方在曝光、销量等关键结果上的个性化偏好，从而评估排名算法在满足不同供给方目标的同时，能否在全体供给方间实现整体公正。在此框架基础上，我们设计了 EquityRank——一种基于梯度的算法，它联合优化用户端的检索效果和供给端的公正。大量离线与在线实验表明，EquityRank 在效率与公平之间取得更佳权衡，并能自适应多样化的供给方需求。"
    },
    {
        "title": "RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing",
        "summary": "Retrieval-Augmented Generation (RAG) has become a core paradigm for grounding large language models with external knowledge. Despite extensive efforts exploring diverse retrieval strategies, existing studies predominantly focus on query-side complexity or isolated method improvements, lacking a systematic understanding of how RAG paradigms behave across different query-corpus contexts and effectiveness-efficiency trade-offs. In this work, we introduce RAGRouter-Bench, the first dataset and benchmark designed for adaptive RAG routing. RAGRouter-Bench revisits retrieval from a query-corpus compatibility perspective and standardizes five representative RAG paradigms for systematic evaluation across 7,727 queries and 21,460 documents spanning diverse domains. The benchmark incorporates three canonical query types together with fine-grained semantic and structural corpus metrics, as well as a unified evaluation for both generation quality and resource consumption. Experiments with DeepSeek-V3 and LLaMA-3.1-8B demonstrate that no single RAG paradigm is universally optimal, that paradigm applicability is strongly shaped by query-corpus interactions, and that increased advanced mechanism does not necessarily yield better effectiveness-efficiency trade-offs. These findings underscore the necessity of routing-aware evaluation and establish a foundation for adaptive, interpretable, and generalizable next-generation RAG systems.",
        "entry_id": "http://arxiv.org/abs/2602.00296v1",
        "pub_date": "2026-01-30",
        "translated_summary": "检索增强生成（RAG）已成为将外部知识融入大型语言模型的核心范式。尽管已有研究在多样化检索策略方面进行了大量探索，但现有工作大多聚焦在查询端的复杂性，或对单一方法的孤立式改进，未能系统探究各类RAG范式在不同“查询—语料库”环境下的表现，以及它们在效果与效率之间的权衡关系。本文提出 RAGRouter-Bench——首个面向自适应 RAG 路由的数据集与评测基准。该基准重新审视检索问题，从“查询—语料库兼容性”这一崭新视角出发，统一规范了五种具有代表性的 RAG 范式，并在 7,727 条查询和 21,460 份覆盖多领域文献的语料库上进行系统评估。其设计包含三类经典查询类型、兼顾语义与结构的细粒度语料库度量，以及针对生成质量与资源开销的统一评测机制。基于 DeepSeek-V3 与 LLaMA-3.1-8B 的实验结果显示：不存在一种放之四海而皆准的 RAG 范式，范式的适用性由查询与语料库的交互特性高度决定，引入更复杂的高级机制未必带来更佳的效果—效率权衡。这些发现凸显了路由感知评估的必要性，并为下一代自适应、可解释且具有普适性的 RAG 系统奠定了研究基础。"
    },
    {
        "title": "Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents",
        "summary": "We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.",
        "entry_id": "http://arxiv.org/abs/2602.03439v1",
        "pub_date": "2026-02-03",
        "translated_summary": "我们将“本体到工具编译（ontology-to-tools compilation）”提出为一种用于将大型语言模型（LLM）与形式化领域知识耦合的原则性方法。在 The World Avatar（TWA）框架中，本体规范被编译为可执行的工具接口，LLM 代理必须使用这些接口来创建和修改知识图谱实例，从而在生成阶段即强制性约束语义，而非事后验证。通过扩展 TWA 的语义化代理组合框架，Model Context Protocol（MCP）及其配套代理成为知识图谱生态的核心组成部分，使得生成模型、符号约束与外部资源之间的结构化交互成为可能。一个基于代理的工作流可将本体转化为具有本体意识的工具，并以迭代方式调用这些工具，从非结构化科学文本中提取、验证并修复结构化知识。以金属有机多面体合成文献为案例，我们展示了可执行的本体语义如何引导 LLM 行为，减少人工模式与提示工程工作量，从而在生成式系统中嵌入形式化知识的通用范式得以确立。"
    },
    {
        "title": "Failure is Feedback: History-Aware Backtracking for Agentic Traversal in Multimodal Graphs",
        "summary": "Open-domain multimodal document retrieval aims to retrieve specific components (paragraphs, tables, or images) from large and interconnected document corpora. Existing graph-based retrieval approaches typically rely on a uniform similarity metric that overlooks hop-specific semantics, and their rigid pre-defined plans hinder dynamic error correction. These limitations suggest that a retriever should adapt its reasoning to the evolving context and recover intelligently from dead ends. To address these needs, we propose Failure is Feedback (FiF), which casts subgraph retrieval as a sequential decision process and introduces two key innovations. (i) We introduce a history-aware backtracking mechanism; unlike standard backtracking that simply reverts the state, our approach piggybacks on the context of failed traversals, leveraging insights from previous failures. (ii) We implement an economically-rational agentic workflow. Unlike conventional agents with static strategies, our orchestrator employs a cost-aware traversal method to dynamically manage the trade-off between retrieval accuracy and inference costs, escalating to intensive LLM-based reasoning only when the prior failure justifies the additional computational investment. Extensive experiments show that FiF achieves state-of-the-art retrieval on the benchmarks of MultimodalQA, MMCoQA and WebQA.",
        "entry_id": "http://arxiv.org/abs/2602.03432v1",
        "pub_date": "2026-02-03",
        "translated_summary": "开放域多模态文档检索的目标，是从庞大且相互关联的文档库中定位特定元素（段落、表格或图像）。现有的图式检索方法普遍采用单一的相似度度量，忽视了“跳数”所蕴含的语义差异；其僵硬的预设规划也阻碍了动态错误修正。上述缺陷表明，检索器应能根据不断演变的上下文调整推理过程，并在走入“死胡同”时实现智能恢复。为此，我们提出“失败即反馈”（Failure is Feedback，FiF）框架，将子图检索抽象为顺序决策过程，并引入两项核心创新：  \n（1）提出“历史感知回溯”机制。与传统回溯仅简单回退状态不同，FiF 在回退时叠加失败遍历的上下文，从过往失败中提取有价值的线索。  \n（2）实现“经济理性”的智能体工作流。与采用固定策略的传统智能体不同，我们的编排器以代价敏感的遍历算法，动态权衡检索精度与推理开销；仅当先前失败证明有必要时，才升级调用高成本的 LLM 推理。  \n\n在 MultimodalQA、MMCoQA 与 WebQA 三大基准上的大量实验表明，FiF 达到了当前最优的检索性能。"
    },
    {
        "title": "RankSteer: Activation Steering for Pointwise LLM Ranking",
        "summary": "Large language models (LLMs) have recently shown strong performance as zero-shot rankers, yet their effectiveness is highly sensitive to prompt formulation, particularly role-play instructions. Prior analyses suggest that role-related signals are encoded along activation channels that are largely separate from query-document representations, raising the possibility of steering ranking behavior directly at the activation level rather than through brittle prompt engineering. In this work, we propose RankSteer, a post-hoc activation steering framework for zero-shot pointwise LLM ranking. We characterize ranking behavior through three disentangled and steerable directions in representation space: a \\textbf{decision direction} that maps hidden states to relevance scores, an \\textbf{evidence direction} that captures relevance signals not directly exploited by the decision head, and a \\textbf{role direction} that modulates model behavior without injecting relevance information. Using projection-based interventions at inference time, RankSteer jointly controls these directions to calibrate ranking behavior without modifying model weights or introducing explicit cross-document comparisons. Experiments on TREC DL 20 and multiple BEIR benchmarks show that RankSteer consistently improves ranking quality using only a small number of anchor queries, demonstrating that substantial ranking capacity remains under-utilized in pointwise LLM rankers. We further provide a geometric analysis revealing that steering improves ranking by stabilizing ranking geometry and reducing dispersion, offering new insight into how LLMs internally represent and calibrate relevance judgments.",
        "entry_id": "http://arxiv.org/abs/2602.03422v1",
        "pub_date": "2026-02-03",
        "translated_summary": "最近，大语言模型（LLM）展现出强大的零样本文档排序能力，但其效果对提示措辞高度敏感，尤其是角色扮演指令。已有研究发现，与角色相关的信号在激活空间中主要沿独立于查询—文档表示的通道编码，使人们开始思考可否绕过脆弱的手工提示，直接在激活层面干预排序行为。\n\n本文提出 RankSteer，一个用于零样本点式 LLM 排序的事后激活操控框架。我们将排序行为解耦为表示空间中三个可独立控制的方向：1. **决策方向**，将隐藏状态映射为相关性分数；2. **证据方向**，捕捉被决策头并未直接利用的相关性信号；3. **角色方向**，在不注入真实性信息的前提下调节模型行为。通过在推断阶段执行基于投影的干预，RankSteer 可联合调控这三个方向来校准排序行为，而无需改动模型权重或引入显式的跨文档比较。\n\n在 TREC DL20 以及多个 BEIR 基准上的实验表明，仅用少量锚查询，RankSteer 就能持续显著提升排序质量，说明点式 LLM 排序器尚有巨大潜能未被利用。进一步的几何分析揭示，操控操作通过稳定排序空间的几何结构并降低其分散度来提升效果，从而为我们理解 LLM 内部表征与校准相关性判断的方式提供了崭新视角。"
    },
    {
        "title": "AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation",
        "summary": "Clothing recommendation extends beyond merely generating personalized outfits; it serves as a crucial medium for aesthetic guidance. However, existing methods predominantly rely on user-item-outfit interaction behaviors while overlooking explicit representations of clothing aesthetics. To bridge this gap, we present the AesRec benchmark dataset featuring systematic quantitative aesthetic annotations, thereby enabling the development of aesthetics-aligned recommendation systems. Grounded in professional apparel quality standards and fashion aesthetic principles, we define a multidimensional set of indicators. At the item level, six dimensions are independently assessed: silhouette, chromaticity, materiality, craftsmanship, wearability, and item-level impression. Transitioning to the outfit level, the evaluation retains the first five core attributes while introducing stylistic synergy, visual harmony, and outfit-level impression as distinct metrics to capture the collective aesthetic impact. Given the increasing human-like proficiency of Vision-Language Models in multimodal understanding and interaction, we leverage them for large-scale aesthetic scoring. We conduct rigorous human-machine consistency validation on a fashion dataset, confirming the reliability of the generated ratings. Experimental results based on AesRec further demonstrate that integrating quantified aesthetic information into clothing recommendation models can provide aesthetic guidance for users while fulfilling their personalized requirements.",
        "entry_id": "http://arxiv.org/abs/2602.03416v1",
        "pub_date": "2026-02-03",
        "translated_summary": "服装推荐早已不只是生成个性化穿搭，更是一种美学指引的核心载体。然而，现有研究几乎完全依赖用户-单品-穿搭的交互数据，缺乏对服饰美学特征的显式建模。为填补这一空白，我们推出 AesRec 基准数据集，首次提供系统性的服饰美学量化标注，并以此支撑“美学对齐”的推荐系统研究。\n\n在标注方案的构建上，我们严格遵循行业服装质量评价规范与时尚美学原则，设计了一套多维度指标体系：\n\n- 单品层面：分别从“廓形、色度、材质、工艺、可穿性和单品整体印象”六个独立维度进行评分。  \n- 穿搭层面：保留前五个核心属性，并新增“风格协同、视觉和谐、整体印象”三项指标，以捕捉组合后的总体美学效果。\n\n借助日趋成熟的多模态大模型（Vision-Language Models），我们实现了大规模美学评分。为验证评分可靠性，我们在公开时尚数据集上开展严谨的人机一致性校验，结果显示机器评分与人工评分之间具有高度一致性。\n\n在 AesRec 基础上开展的实验进一步证明：将量化后的美学信息融入推荐模型，不仅能为用户提供审美引导，也同时满足其个性化需求。"
    },
    {
        "title": "Beyond Exposure: Optimizing Ranking Fairness with Non-linear Time-Income Functions",
        "summary": "Ranking is central to information distribution in web search and recommendation. Nowadays, in ranking optimization, the fairness to item providers is viewed as a crucial factor alongside ranking relevance for users. There are currently numerous concepts of fairness and one widely recognized fairness concept is Exposure Fairness. However, it relies primarily on exposure determined solely by position, overlooking other factors that significantly influence income, such as time. To address this limitation, we propose to study ranking fairness when the provider utility is influenced by other contextual factors and is neither equal to nor proportional to item exposure. We give a formal definition of Income Fairness and develop a corresponding measurement metric. Simulated experiments show that existing-exposure-fairness-based ranking algorithms fail to optimize the proposed income fairness. Therefore, we propose the Dynamic-Income-Derivative-aware Ranking Fairness algorithm, which, based on the marginal income gain at the present timestep, uses Taylor-expansion-based gradients to simultaneously optimize effectiveness and income fairness. In both offline and online settings with diverse time-income functions, DIDRF consistently outperforms state-of-the-art methods.",
        "entry_id": "http://arxiv.org/abs/2602.03345v1",
        "pub_date": "2026-02-03",
        "translated_summary": "排序是网络搜索与推荐中信息分发的核心环节。当下，在排序优化中，除相关性外，对内容提供方的公平保障已被视为同等重要。目前已提出多种公平性定义，其中广受认可的是“曝光公平性”。然而，该概念仅以位置决定的曝光量为衡量依据，忽视了时间等对流量变现影响更大的关键因素。为弥补这一局限，本文提出当提供者收益受其他情境因素驱动、且既不等同于也不与物品曝光成正比时，探讨新型排序公平问题。我们正式定义了“收益公平性”，并给出相应的度量指标。仿真实验表明，现有以曝光公平为核心的排序算法无法优化新定义的收益公平。为此，我们提出动态收益导数感知排序公平算法（DIDRF）。该算法在当前时刻计算边际收益增益，并基于泰勒展开的梯度同步优化有效性和收益公平。在多种时间—收益函数下，无论离线还是在线场景，DIDRF均持续优于现有最佳方案。"
    },
    {
        "title": "SCASRec: A Self-Correcting and Auto-Stopping Model for Generative Route List Recommendation",
        "summary": "Route recommendation systems commonly adopt a multi-stage pipeline involving fine-ranking and re-ranking to produce high-quality ordered recommendations. However, this paradigm faces three critical limitations. First, there is a misalignment between offline training objectives and online metrics. Offline gains do not necessarily translate to online improvements. Actual performance must be validated through A/B testing, which may potentially compromise the user experience. Second, redundancy elimination relies on rigid, handcrafted rules that lack adaptability to the high variance in user intent and the unstructured complexity of real-world scenarios. Third, the strict separation between fine-ranking and re-ranking stages leads to sub-optimal performance. Since each module is optimized in isolation, the fine-ranking stage remains oblivious to the list-level objectives (e.g., diversity) targeted by the re-ranker, thereby preventing the system from achieving a jointly optimized global optimum. To overcome these intertwined challenges, we propose \\textbf{SCASRec} (\\textbf{S}elf-\\textbf{C}orrecting and \\textbf{A}uto-\\textbf{S}topping \\textbf{Rec}ommendation), a unified generative framework that integrates ranking and redundancy elimination into a single end-to-end process. SCASRec introduces a stepwise corrective reward (SCR) to guide list-wise refinement by focusing on hard samples, and employs a learnable End-of-Recommendation (EOR) token to terminate generation adaptively when no further improvement is expected. Experiments on two large-scale, open-sourced route recommendation datasets demonstrate that SCASRec establishes an SOTA in offline and online settings. SCASRec has been fully deployed in a real-world navigation app, demonstrating its effectiveness.",
        "entry_id": "http://arxiv.org/abs/2602.03324v1",
        "pub_date": "2026-02-03",
        "translated_summary": "路线推荐系统通常采用包含精排序（fine-ranking）与重排序（re-ranking）的多阶段流水线，以输出高质量的有序推荐。然而，该范式面临三大关键局限：  \n第一，离线训练目标与在线指标存在错位。离线指标的提升无法保证线上效果，必须通过 A/B 测试来验证，这一过程潜在牺牲用户体验。  \n第二，去冗余依赖刚性手工规则，难以适应用户意图的高方差以及真实场景的复杂无结构化特征。  \n第三，精排序与重排序阶段的人为割裂导致整体次优。由于各模块独立优化，精排序阶段对重排序旨在提升的列表级目标（如多样性）一无所知，系统无法获得联合优化的全局最优。\n\n为突破这三大缠绕的挑战，我们提出 SCASRec（Self-Correcting and Auto-Stopping Recommendation），统一的生成式框架，将排序与去冗余整合成端到端的全过程。SCASRec 引入逐步纠偏奖励（SCR），聚焦难样本以引导列表级精修；同时，通过学习型 End-of-Recommendation（EOR）标记，可在无收益增长时自适应终止生成。在两个大规模公开路线推荐数据集上的实验表明，SCASRec 在离线及在线场景均刷新 SOTA。此外，SCASRec 已在实际导航应用中全量上线，证明了其有效性。"
    },
    {
        "title": "Learning to Select: Query-Aware Adaptive Dimension Selection for Dense Retrieval",
        "summary": "Dense retrieval represents queries and docu-002 ments as high-dimensional embeddings, but003 these representations can be redundant at the004 query level: for a given information need, only005 a subset of dimensions is consistently help-006 ful for ranking. Prior work addresses this via007 pseudo-relevance feedback (PRF) based dimen-008 sion importance estimation, which can produce009 query-aware masks without labeled data but010 often relies on noisy pseudo signals and heuris-011 tic test-time procedures. In contrast, super-012 vised adapter methods leverage relevance labels013 to improve embedding quality, yet they learn014 global transformations shared across queries015 and do not explicitly model query-aware di-016 mension importance. We propose a Query-017 Aware Adaptive Dimension Selection frame-018 work that learns to predict per-dimension im-019 portance directly from query embedding. We020 first construct oracle dimension importance dis-021 tributions over embedding dimensions using022 supervised relevance labels, and then train a023 predictor to map a query embedding to these024 label-distilled importance scores. At inference,025 the predictor selects a query-aware subset of026 dimensions for similarity computation based027 solely on the query embedding, without pseudo-028 relevance feedback. Experiments across multi-029 ple dense retrievers and benchmarks show that030 our learned dimension selector improves re-031 trieval effectiveness over the full-dimensional032 baseline as well as PRF-based masking and033 supervised adapter baselines.",
        "entry_id": "http://arxiv.org/abs/2602.03306v1",
        "pub_date": "2026-02-03",
        "translated_summary": "稠密检索将查询和文档表示为高维嵌入向量，但在查询层面上这些表征往往存在冗余：对某一信息需求而言，只有一少部分维度能持续地对排序起作用。已有研究利用伪相关反馈（PRF）估计维度重要性，可无需标注数据即可生成“查询自适应掩码”，但这类方法常受伪信号噪声和启发式测试流程的干扰。相反，有监督适配器方法借助相关性标签提升嵌入质量，但它们学习的是跨查询共享的全局变换，并未显式刻画查询维度的重要程度。\n\n我们提出一种查询自适应维度选择框架（Query-Aware Adaptive Dimension Selection），直接从查询嵌入预测每个维度的重要性。具体地：  \n（1）使用有监督的相关性标签构造一组嵌入维度的“黄金重要性分布”；  \n（2）训练一个预测器，将查询嵌入映射到这些标签提炼出的重要性得分；  \n（3）推理时，预测器仅凭查询嵌入就可选出面向该查询的维度子集用于相似度计算，无需任何伪相关反馈。\n\n跨多种稠密检索器与基准数据集的实验表明，我们学习的维度选择器不仅超越了全维基线，也优于 PRF 掩码和有监督适配器基线，显著提升检索效果。"
    },
    {
        "title": "To Search or Not to Search: Aligning the Decision Boundary of Deep Search Agents via Causal Intervention",
        "summary": "Deep search agents, which autonomously iterate through multi-turn web-based reasoning, represent a promising paradigm for complex information-seeking tasks. However, current agents suffer from critical inefficiency: they conduct excessive searches as they cannot accurately judge when to stop searching and start answering. This stems from outcome-centric training that prioritize final results over the search process itself. We identify the root cause as misaligned decision boundaries, the threshold determining when accumulated information suffices to answer. This causes over-search (redundant searching despite sufficient knowledge) and under-search (premature termination yielding incorrect answers). To address these errors, we propose a comprehensive framework comprising two key components. First, we introduce causal intervention-based diagnosis that identifies boundary errors by comparing factual and counterfactual trajectories at each decision point. Second, we develop Decision Boundary Alignment for Deep Search agents (DAS), which constructs preference datasets from causal feedback and aligns policies via preference optimization. Experiments on public datasets demonstrate that decision boundary errors are pervasive across state-of-the-art agents. Our DAS method effectively calibrates these boundaries, mitigating both over-search and under-search to achieve substantial gains in accuracy and efficiency. Our code and data are publicly available at: https://github.com/Applied-Machine-Learning-Lab/WWW2026_DAS.",
        "entry_id": "http://arxiv.org/abs/2602.03304v1",
        "pub_date": "2026-02-03",
        "translated_summary": "具备在多轮网络推理过程中自主迭代的深度搜索智能体，为复杂信息检索任务提供了极具前景的范式。然而，现有智能体存在一个关键低效问题：由于无法准确判断何时停止搜索并转而作答，它们往往执行过多搜索，其根源在于以结果为导向的训练过分追求最终答案，而忽视了搜索过程本身。我们把这一问题的根本原因归结于决策边界错位，即用于判断已累积信息是否足以回答的阈值出现偏差，从而导致“过搜”（即使已获得足够知识仍继续冗余搜索）和“欠搜”（信息不足即提前终止并给出错误答案）两种失效模式。\n\n为纠正这些错误，我们提出一个包含两大核心组件的综合框架。首先，我们引入基于因果干预的诊断方法：在每个决策节点对比实际轨迹与反事实轨迹，以此精准定位边界误差。其次，我们开发了面向深度搜索智能体的决策边界对齐方法（DAS）：利用因果反馈构建偏好数据集，并通过偏好优化对齐策略。\n\n在公开数据集上的实验表明，决策边界错误普遍存在于当前最先进的搜索智能体中。我们提出的 DAS 方法能够有效校准这些边界，缓解过搜与欠搜，从而在准确率与效率上实现显著提升。代码与数据已开源：https://github.com/Applied-Machine-Learning-Lab/WWW2026_DAS"
    },
    {
        "title": "Distribution-Aware End-to-End Embedding for Streaming Numerical Features in Click-Through Rate Prediction",
        "summary": "This paper explores effective numerical feature embedding for Click-Through Rate prediction in streaming environments. Conventional static binning methods rely on offline statistics of numerical distributions; however, this inherently two-stage process often triggers semantic drift during bin boundary updates. While neural embedding methods enable end-to-end learning, they often discard explicit distributional information. Integrating such information end-to-end is challenging because streaming features often violate the i.i.d. assumption, precluding unbiased estimation of the population distribution via the expectation of order statistics. Furthermore, the critical context dependency of numerical distributions is often neglected. To this end, we propose DAES, an end-to-end framework designed to tackle numerical feature embedding in streaming training scenarios by integrating distributional information with an adaptive modulation mechanism. Specifically, we introduce an efficient reservoir-sampling-based distribution estimation method and two field-aware distribution modulation strategies to capture streaming distributions and field-dependent semantics. DAES significantly outperforms existing approaches as demonstrated by extensive offline and online experiments and has been fully deployed on a leading short-video platform with hundreds of millions of daily active users.",
        "entry_id": "http://arxiv.org/abs/2602.03223v1",
        "pub_date": "2026-02-03",
        "translated_summary": "本文研究了面向流式环境的点击率（CTR）预测中数值型特征的高效嵌入方法。传统的静态分箱依赖离线的数值分布统计，但这种固有的两阶段流程在更新分箱边界时经常引发语义漂移。神经型嵌入方法虽支持端到端学习，却常忽略显式分布信息；而要在端到端地集成这类信息，由于流式特征往往违背独立同分布假设，无法通过次序统计量的期望对总体分布进行无偏估计，因此极具挑战。此外，数值分布的关键上下文依赖性也常被忽视。为此，本文提出DAES——一种面向流式训练场景的端到端数值嵌入框架，通过自适应调制机制集成分布信息。具体而言，我们设计了一种基于蓄水池采样的高效分布估计方法，以及两种字段感知的分布调制策略，用于捕获流式分布及其与业务字段相关的语义。实验表明，无论离线还是在线评测，DAES均显著优于现有方法，并已在一家日活数亿的头部短视频平台全面落地。"
    },
    {
        "title": "PAMAS: Self-Adaptive Multi-Agent System with Perspective Aggregation for Misinformation Detection",
        "summary": "Misinformation on social media poses a critical threat to information credibility, as its diverse and context-dependent nature complicates detection. Large language model-empowered multi-agent systems (MAS) present a promising paradigm that enables cooperative reasoning and collective intelligence to combat this threat. However, conventional MAS suffer from an information-drowning problem, where abundant truthful content overwhelms sparse and weak deceptive cues. With full input access, agents tend to focus on dominant patterns, and inter-agent communication further amplifies this bias. To tackle this issue, we propose PAMAS, a multi-agent framework with perspective aggregation, which employs hierarchical, perspective-aware aggregation to highlight anomaly cues and alleviate information drowning. PAMAS organizes agents into three roles: Auditors, Coordinators, and a Decision-Maker. Auditors capture anomaly cues from specialized feature subsets; Coordinators aggregate their perspectives to enhance coverage while maintaining diversity; and the Decision-Maker, equipped with evolving memory and full contextual access, synthesizes all subordinate insights to produce the final judgment. Furthermore, to improve efficiency in multi-agent collaboration, PAMAS incorporates self-adaptive mechanisms for dynamic topology optimization and routing-based inference, enhancing both efficiency and scalability. Extensive experiments on multiple benchmark datasets demonstrate that PAMAS achieves superior accuracy and efficiency, offering a scalable and trustworthy way for misinformation detection.",
        "entry_id": "http://arxiv.org/abs/2602.03158v1",
        "pub_date": "2026-02-03",
        "translated_summary": "社交媒体上的错误信息因其种类繁多、依赖上下文而极难检测，已对信息可信性构成严重威胁。借助大语言模型的多智能体系统（MAS）提供了一种协同推理、集结集体智能的新范式来应对这一挑战。然而，传统MAS面临“信息稀释”困境：海量的真实内容会淹没稀疏而微弱的欺骗信号。当智能体全部读取原始输入时，会聚焦于主导模式，而智能体间的通信又进一步放大该偏差。\n\n为此，我们提出PAMAS：一种带有“视角聚合”的多智能体框架，通过分层、感知视角的汇聚，突出异常线索，缓解信息稀释。PAMAS将智能体划分为三种角色：审查者（Auditors）、协调者（Coordinators）与决策者（Decision-Maker）。审查者从专门划分的特征子集中捕捉异常线索；协调者聚合各审查者视角，既扩大关注范围，又保持多样性；决策者拥有演化记忆与全局上下文访问权限，综合子层洞察，给出最终判断。此外，为提升多智能体协作效率，PAMAS引入自适应机制，支持动态拓扑优化与基于路由的推理，兼顾高效性与可扩展性。\n\n在多套基准数据集上的大量实验表明，PAMAS在准确率和效率上均显著优于现有方法，为可扩展、可信的错误信息检测提供了新途径。"
    },
    {
        "title": "From Speech-to-Spatial: Grounding Utterances on A Live Shared View with Augmented Reality",
        "summary": "We introduce Speech-to-Spatial, a referent disambiguation framework that converts verbal remote-assistance instructions into spatially grounded AR guidance. Unlike prior systems that rely on additional cues (e.g., gesture, gaze) or manual expert annotations, Speech-to-Spatial infers the intended target solely from spoken references (speech input). Motivated by our formative study of speech referencing patterns, we characterize recurring ways people specify targets (Direct Attribute, Relational, Remembrance, and Chained) and ground them to our object-centric relational graph. Given an utterance, referent cues are parsed and rendered as persistent in-situ AR visual guidance, reducing iterative micro-guidance (\"a bit more to the right\", \"now, stop.\") during remote guidance. We demonstrate the use cases of our system with remote guided assistance and intent disambiguation scenarios. Our evaluation shows that Speechto-Spatial improves task efficiency, reduces cognitive load, and enhances usability compared to a conventional voice-only baseline, transforming disembodied verbal instruction into visually explainable, actionable guidance on a live shared view.",
        "entry_id": "http://arxiv.org/abs/2602.03059v1",
        "pub_date": "2026-02-03",
        "translated_summary": "我们提出 Speech-to-Spatial，一种将口头远程协助指令转化为具有空间锚定的 AR 引导的指代消歧框架。与以往还需额外线索（例如手势、凝视）或需人工专家标注的系统不同，Speech-to-Spatial 仅凭口语中对目标的口头指代（语音输入）即可推断用户意图。基于我们对语音指代模式的形成性研究，我们归纳出四种反复出现的指代方式——直接属性、关系描述、旧事回忆以及链式指代，并将其锚定到一个以对象为中心的关联图。系统解析语句中的指代线索，并渲染成持续在场的 AR 可视化引导，显著减少了远程协助中的迭代式微观引导（如“再往右一点”“现在停”）。我们在远程辅助和意图消歧场景下验证了其应用案例。评估结果表明，与纯语音基线相比，Speech-to-Spatial 提高了任务效率、减轻了认知负荷，并显著提升了易用性，将“隔空”的口头指令转变为可在共享实时视图上直观解释、立即可操作的视觉导引。"
    },
    {
        "title": "ALPBench: A Benchmark for Attribution-level Long-term Personal Behavior Understanding",
        "summary": "Recent advances in large language models have highlighted their potential for personalized recommendation, where accurately capturing user preferences remains a key challenge. Leveraging their strong reasoning and generalization capabilities, LLMs offer new opportunities for modeling long-term user behavior. To systematically evaluate this, we introduce ALPBench, a Benchmark for Attribution-level Long-term Personal Behavior Understanding. Unlike item-focused benchmarks, ALPBench predicts user-interested attribute combinations, enabling ground-truth evaluation even for newly introduced items. It models preferences from long-term historical behaviors rather than users' explicitly expressed requests, better reflecting enduring interests. User histories are represented as natural language sequences, allowing interpretable, reasoning-based personalization. ALPBench enables fine-grained evaluation of personalization by focusing on the prediction of attribute combinations task that remains highly challenging for current LLMs due to the need to capture complex interactions among multiple attributes and reason over long-term user behavior sequences.",
        "entry_id": "http://arxiv.org/abs/2602.03056v1",
        "pub_date": "2026-02-03",
        "translated_summary": "最新进展表明，大型语言模型在个性化推荐中展现出巨大潜力，而精准捕捉用户偏好仍是核心难题。凭借其强大的推理与泛化能力，大模型为建模长期用户行为带来新机遇。为此，我们系统性评测并推出ALPBench——首个属性层级的长期个人行为理解基准。与仅聚焦单个商品的基准不同，ALPBench要求预测用户感兴趣的属性组合，即便对新商品也可进行基于属性真实值的评估。该基准通过挖掘用户长期历史行为而非显性查询来建模偏好，更真实地反映持久兴趣。用户历史被表示为自然语言序列，支持可解释、基于推理的个性化。ALPBench通过属性组合预测任务实现更细粒度的评估，该任务要求模型捕捉多属性间的复杂交互，并对长期行为序列进行推理，因而对现有大模型构成极大挑战。"
    },
    {
        "title": "Efficiency Optimizations for Superblock-based Sparse Retrieval",
        "summary": "Learned sparse retrieval (LSR) is a popular method for first-stage retrieval because it combines the semantic matching of language models with efficient CPU-friendly algorithms. Previous work aggregates blocks into \"superblocks\" to quickly skip the visitation of blocks during query processing by using an advanced pruning heuristic. This paper proposes a simple and effective superblock pruning scheme that reduces the overhead of superblock score computation while preserving competitive relevance. It combines this scheme with a compact index structure and a robust zero-shot configuration that is effective across LSR models and multiple datasets. This paper provides an analytical justification and evaluation on the MS MARCO and BEIR datasets, demonstrating that the proposed scheme can be a strong alternative for efficient sparse retrieval.",
        "entry_id": "http://arxiv.org/abs/2602.02883v1",
        "pub_date": "2026-02-02",
        "translated_summary": "习得的稀疏检索（LSR）作为一种初始检索阶段的方法备受关注，它既能在语言模型中实现语义匹配，又可借助对 CPU 友好的高效算法。以往研究将多个倒排块聚合成“超块”，通过先进的剪枝启发式规则可快速跳过于查询处理中无需探访的块。本文提出一种简单而有效的超块剪枝方案，可在大幅降低超块分数计算开销的同时保持检索结果的竞争力。该方案进一步结合了紧凑的索引结构与经过广泛验证的零样本配置，在多种 LSR 模型及多个数据集上均表现稳健。论文从理论上分析了方案动机，并在 MS MARCO 与 BEIR 数据集上进行了实验评估，结果表明该方法可作为高效稀疏检索的有力替代方案。"
    },
    {
        "title": "Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval",
        "summary": "Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-$K$ identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5$\\times$, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.",
        "entry_id": "http://arxiv.org/abs/2602.02827v1",
        "pub_date": "2026-02-02",
        "translated_summary": "像 ColBERT 这类多向量晚期交互式检索器在检索质量上已达到顶尖水平，但其查询时延代价主要由对所有候选文档逐词元进行 token-level MaxSim 计算决定。尽管用单向量近似晚期交互可降低开销，但常带来显著精度下降。本文提出 Col-Bandit，一种查询时裁剪算法，通过将重排建模为有限总体 Top-K 识别问题，大幅削减该计算负担。Col-Bandit 为仅部分观测到的文档分数维护带不确定性统计的上下界，并在可调控的放宽准则下，仅按需暴露确认排行结果所需的 (文档, 查询词元) MaxSim 条目。与在离线阶段粗粒度地裁剪整篇文档或整句词元的方法不同，Col-Bandit 在运行时即时稀疏化交互矩阵。它作为零训练、即插即用的层，无需任何索引修改、离线预处理或模型重训。在文本（BEIR）与多模态（REAL-MM-RAG）基准上的实验表明，Col-Bandit 在保持排序保真的同时，将 MaxSim FLOPs 最高缩减至原来的 1/5，表明稠密晚期交互评分中存在大量冗余，可在查询阶段被高效识别并裁剪。"
    },
    {
        "title": "RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval",
        "summary": "Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.",
        "entry_id": "http://arxiv.org/abs/2602.02444v1",
        "pub_date": "2026-02-02",
        "translated_summary": "重排序是现代检索系统的关键环节，通常由高效的初阶段检索器与更具表现力的模型配合，进一步精炼结果。尽管大型推理模型推动了以文本为主的重排序迅速发展，但基于推理的视频重排序尚未得到充分探索。为填补这一空白，本文提出 RANKVIDEO——一种面向视频检索的推理式重排器。该模型通过直接对视频内容进行推理，显式地评估查询与视频的相关性。RANKVIDEO 采用两阶段课程训练：首先进行以感知为基础的监督微调，随后进行重排序训练，融合点式、成对式目标以及教师置信度蒸馏。辅以可合成高难度推理样本的数据生成管线。在大型基准数据集 MultiVENT 2.0 上的实验表明，RANKVIDEO 在两阶段框架中持续显著提升检索性能，nDCG@10 平均提升 31%，性能优于仅用文本或视觉-语言的基线方法，同时更加高效。"
    },
    {
        "title": "WideSeek: Advancing Wide Research via Multi-Agent Scaling",
        "summary": "Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.",
        "entry_id": "http://arxiv.org/abs/2602.02636v1",
        "pub_date": "2026-02-02",
        "translated_summary": "搜索智能正在由“深度研究”演进到“广度研究”——一种在复杂多重约束下同时检索并综合全域信息的新范式。然则，该领域受限于缺乏针对搜索广度量身定制的基准与优化方法而进展缓慢。为破解此瓶颈，我们从“数据流水线”与“智能体优化”两方面深入剖析广度研究：其一，依托多阶段、严谨构建的流水线，创建具有目标信息量、逻辑约束与学科领域多维多样性的通用广度信息需求基准 WideSeekBench；其二，提出动态分层多智能体架构 WideSeek，可依据任务需求自动衍生并行子智能体，并设计将多智能体轨迹线性化的统一训练框架，以端到端强化学习实现系统优化。实验结果表明，WideSeek 与多智能体 RL 均具显著成效，彰显“智能体数量扩展”是推进广度研究范式的有效途径。"
    },
    {
        "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing",
        "summary": "How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.",
        "entry_id": "http://arxiv.org/abs/2602.02386v1",
        "pub_date": "2026-02-02",
        "translated_summary": "大语言模型（LLM）实践者在不浪费经费的前提下，该如何为任务挑选合适模型？本文提出 BELLA——一种“基于可解释技能画像的预算友好型 LLM 甄选框架”。常见基准以笼统指标报告整体性能，掩盖了任务究竟需要哪些细粒度能力，以及更便宜的模型能否胜任。为弥合这一缺口，BELLA 分三个阶段：  \n1) 通过“批评-式”画像将 LLM 输出拆解，抽取任务所需的微观技能；  \n2) 将技能聚类为结构化的能力矩阵；  \n3) 利用多目标优化，在预算约束内选优模型，最大化性能。  \n\nBELLA 还能用自然语言给出推荐理由，补足现有黑盒路由系统缺乏的透明性。我们详述框架架构，将其置于 LLM 路由与评估的视野中，并以金融推理为例论述其应用——该领域的任务需求多样，不同模型成本差异显著。BELLA 让从业者在部署 LLM 时，做出兼顾性能与成本的理性权衡。"
    },
    {
        "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
        "summary": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.",
        "entry_id": "http://arxiv.org/abs/2602.02343v1",
        "pub_date": "2026-02-02",
        "translated_summary": "控制大型语言模型（LLM）的方法——包括局部权重微调、基于 LoRA 的适配以及基于激活的干预——通常被孤立地研究，这掩盖了它们之间的联系，也使得比较变得困难。本文提出一种统一视角，将上述干预均视为由控制信号触发的动态权重更新，并将它们置于同一概念框架之中。基于此视角，我们进一步提出了一种统一的偏好-效用分析框架：把控制效果拆分为“偏好”（即朝向目标概念的倾向）和“效用”（即连贯且任务有效的生成），并利用极性配对的对比示例把两者在同一个对数几率(log-odds)尺度上加以量化。跨方法的实验一致发现偏好与效用之间存在权衡：控制强度增加可提升偏好，但可预测地降低效用。我们从激活流形的角度解释了这一现象：干预沿目标概念方向移动表征可提升偏好；一旦表征被推到模型“有效生成分布”之外，效用就会下降。依据上述分析，我们提出一种新的引导方法 SPLIT，它能在提升偏好的同时更好地保全效用。代码开源：https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md"
    },
    {
        "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
        "summary": "Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.",
        "entry_id": "http://arxiv.org/abs/2602.02338v1",
        "pub_date": "2026-02-02",
        "translated_summary": "基于语义 ID（SID）的推荐是扩展序列式推荐系统前景可观的新范式，然而现有方法基本沿用“以语义为中心”的流水线：先从基础模型学得物品嵌入，再用通用量化方案将其离散化。这种做法与生成式推荐的目标不匹配——语义嵌入与协同信号耦合薄弱，而通用量化难以充分削减序列生成中的不确定性。为解决此问题，我们提出 ReSID：一个完全不依赖 LLM、内生于推荐场景并拥有理论保证的 SID 框架，从信息保持与序列可预测性的角度重新思考表征学习与量化。ReSID 由两部分构成：(i) 字段感知掩码自编码器（FAMAE），用结构化特征学习仅满足协同预测即可的紧凑表示；(ii) 全局对齐正交量化（GAOQ），通过联合消减语义歧义与前缀条件下的不确定性，输出既紧凑又可预测的 SID 序列。理论分析与涵盖 10 个数据集的广泛实验一致验证 ReSID 的有效性：其以 10% 以上的平均提升超越所有强序列及 SID 生成基线，并将序列标记化的存储/计算开销减少最多 122 倍。代码：https://github.com/FuCongResearchSquad/ReSID。"
    },
    {
        "title": "Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study",
        "summary": "Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.",
        "entry_id": "http://arxiv.org/abs/2602.02208v1",
        "pub_date": "2026-02-02",
        "translated_summary": "大型语言模型在知识密集型领域展现出潜力，但在农业领域的应用仍受限于弱基础支撑、以英语为主的训练数据以及缺乏真实世界评估等问题。在低资源语言场景中，这些问题尤为突出：尽管存在高质量的领域文档，通用模型却难以有效获取。本文提出 AgriHubi，一个面向芬兰语农业决策支持的领域适应 RAG（Retrieval-Augmented Generation）系统。它将芬兰语农业文献与开放 PORO 系列模型集成，并通过显式来源引用和用户反馈实现迭代优化。经过八轮系统升级后，AgriHubi 在两项用户研究中的评估结果表明：其回答的完整性、语言准确度及可感知的可信度均有显著提升。研究同时揭示了在部署更大型模型时，响应质量与延迟之间需要权衡的实际问题。本研究为在低资源语言环境中设计与评估领域特定 RAG 系统提供了实证参考。"
    },
    {
        "title": "Multi-Source Retrieval and Reasoning for Legal Sentencing Prediction",
        "summary": "Legal judgment prediction (LJP) aims to predict judicial outcomes from case facts and typically includes law article, charge, and sentencing prediction. While recent methods perform well on the first two subtasks, legal sentencing prediction (LSP) remains difficult due to its need for fine-grained objective knowledge and flexible subjective reasoning. To address these limitations, we propose $MSR^2$, a framework that integrates multi-source retrieval and reasoning in LLMs with reinforcement learning. $MSR^2$ enables LLMs to perform multi-source retrieval based on reasoning needs and applies a process-level reward to guide intermediate subjective reasoning steps. Experiments on two real-world datasets show that $MSR^2$ improves both accuracy and interpretability in LSP, providing a promising step toward practical legal AI. Our code is available at https://anonymous.4open.science/r/MSR2-FC3B.",
        "entry_id": "http://arxiv.org/abs/2602.04690v1",
        "pub_date": "2026-02-04",
        "translated_summary": "法律判决预测（LJP）旨在依据案件事实预测司法裁判结果，通常包括法条预测、罪名预测与刑期预测三个子任务。现有方法在前两子任务上表现优异，但法律量刑预测（LSP）由于需要精细的客观知识及灵活的主观推理而依然困难。为克服这一痛点，本文提出 $MSR^2$，将多源检索与大语言模型（LLM）推理通过强化学习有机融合。$MSR^2$使LLM可根据推理需求动态开展多源检索，并通过过程级奖励引导主观推理的中间步骤。在两个真实数据集上的实验表明，$MSR^2$有效提升了LSP的准确性与可解释性，为迈向实用的法律人工智能迈出了关键一步。代码开源：https://anonymous.4open.science/r/MSR2-FC3B"
    },
    {
        "title": "AIANO: Enhancing Information Retrieval with AI-Augmented Annotation",
        "summary": "The rise of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) has rapidly increased the need for high-quality, curated information retrieval datasets. These datasets, however, are currently created with off-the-shelf annotation tools that make the annotation process complex and inefficient. To streamline this process, we developed a specialized annotation tool - AIANO. By adopting an AI-augmented annotation workflow that tightly integrates human expertise with LLM assistance, AIANO enables annotators to leverage AI suggestions while retaining full control over annotation decisions. In a within-subject user study ($n = 15$), participants created question-answering datasets using both a baseline tool and AIANO. AIANO nearly doubled annotation speed compared to the baseline while being easier to use and improving retrieval accuracy. These results demonstrate that AIANO's AI-augmented approach accelerates and enhances dataset creation for information retrieval tasks, advancing annotation capabilities in retrieval-intensive domains.",
        "entry_id": "http://arxiv.org/abs/2602.04579v1",
        "pub_date": "2026-02-04",
        "translated_summary": "随着大语言模型（LLM）和检索增强生成（RAG）技术的兴起，对高质量、精心策划的信息检索数据集的需求急剧增加。然而，现有的数据标注依然采用传统的现成的标注工具，使得整个流程复杂而低效。为了简化这一过程，我们设计并开发了一款专门定制的标注工具——AIANO。通过采用“人工智能辅助标注”工作流，AIANO 将人类专家的知识与大模型的能力深度整合：在保留人对最终标注结果完全控制权的同时，让标注者能够灵活运用模型生成的建议。在一项被试内用户实验（n = 15）中，参与者在分别使用基线工具和 AIANO 进行问答数据集标注时发现，AIANO 几乎将标注速度提升了一倍，且操作更为简便，同时显著提高了检索准确率。实验结果表明，AIANO 的 AI 增强标注范式为信息检索任务的数据构建提速、提质的双重目标提供了有力支撑，推动了对检索密集型领域的数据标注能力的进一步拓展。"
    },
    {
        "title": "VK-LSVD: A Large-Scale Industrial Dataset for Short-Video Recommendation",
        "summary": "Short-video recommendation presents unique challenges, such as modeling rapid user interest shifts from implicit feedback, but progress is constrained by a lack of large-scale open datasets that reflect real-world platform dynamics. To bridge this gap, we introduce the VK Large Short-Video Dataset (VK-LSVD), the largest publicly available industrial dataset of its kind. VK-LSVD offers an unprecedented scale of over 40 billion interactions from 10 million users and almost 20 million videos over six months, alongside rich features including content embeddings, diverse feedback signals, and contextual metadata. Our analysis supports the dataset's quality and diversity. The dataset's immediate impact is confirmed by its central role in the live VK RecSys Challenge 2025. VK-LSVD provides a vital, open dataset to use in building realistic benchmarks to accelerate research in sequential recommendation, cold-start scenarios, and next-generation recommender systems.",
        "entry_id": "http://arxiv.org/abs/2602.04567v1",
        "pub_date": "2026-02-04",
        "translated_summary": "短视频推荐面临独特挑战，例如仅凭隐式反馈来捕捉用户兴趣瞬间变化，而相关进展因缺少能真实反映线上平台动态的大规模开放数据集而受阻。为弥合这一空白，我们推出 VK Large Short-Video Dataset（VK-LSVD），这是同类中规模空前的公开工业级短视频数据集。VK-LSVD 在六个月里收集了来自 1000 万用户、近 2000 万条视频产生的超 400 亿次互动记录，并附带丰富的内容嵌入、多元反馈信号与上下文元数据。我们通过详尽分析确认其质量与多样性。该数据集已作为 VK RecSys Challenge 2025 的核心竞赛资源，直接推动了前沿研究。VK-LSVD 将为序列化推荐、冷启动场景以及下一代推荐系统构建更加逼真的基准，成为加速相关研究的关键开放资源。"
    },
    {
        "title": "Unmasking Superspreaders: Data-Driven Approaches for Identifying and Comparing Key Influencers of Conspiracy Theories on X.com",
        "summary": "Conspiracy theories can threaten society by spreading misinformation, deepening polarization, and eroding trust in democratic institutions. Social media often fuels the spread of conspiracies, primarily driven by two key actors: Superspreaders -- influential individuals disseminating conspiracy content at disproportionately high rates, and Bots -- automated accounts designed to amplify conspiracies strategically. To counter the spread of conspiracy theories, it is critical to both identify these actors and to better understand their behavior. However, a systematic analysis of these actors as well as real-world-applicable identification methods are still lacking. In this study, we leverage over seven million tweets from the COVID-19 pandemic to analyze key differences between Human Superspreaders and Bots across dimensions such as linguistic complexity, toxicity, and hashtag usage. Our analysis reveals distinct communication strategies: Superspreaders tend to use more complex language and substantive content while relying less on structural elements like hashtags and emojis, likely to enhance credibility and authority. By contrast, Bots favor simpler language and strategic cross-usage of hashtags, likely to increase accessibility, facilitate infiltration into trending discussions, and amplify reach. To counter both Human Superspreaders and Bots, we propose and evaluate 27 novel metrics for quantifying the severity of conspiracy theory spread. Our findings highlight the effectiveness of an adapted H-Index for computationally feasible identification of Human Superspreaders. By identifying behavioral patterns unique to Human Superspreaders and Bots as well as providing suitable identification methods, this study provides a foundation for mitigation strategies, including platform moderation policies, temporary and permanent account suspensions, and public awareness campaigns.",
        "entry_id": "http://arxiv.org/abs/2602.04546v1",
        "pub_date": "2026-02-04",
        "translated_summary": "阴谋论通过传播错误信息、加剧社会极化、侵蚀对民主制度的信任，已构成对社会的威胁。社交媒体通常是阴谋论扩散的“催化剂”，其中有两类关键行为者：  \n- 超级传播者——以远超一般用户的速率发布阴谋内容的有影响力的个人用户；  \n- 机器人账号——被设计为策略放大阴谋论、执行自动任务的脚本账号。  \n\n为了遏制阴谋论扩散，首先必须识别这两类行为者并深入理解其行为模式；然而目前的系统性研究尚不充分，也缺乏可直接落地的识别方法。  \n\n本研究收集了逾700万条与新冠疫情相关的推文，围绕语言复杂度、推文毒性及标签使用等多个维度，系统比较“人类超级传播者”与“机器人账号”的差异。  \n\n研究发现两套泾渭分明的传播策略：  \n1) 超级传播者倾向使用结构更复杂、语义更丰富的文本，而较少依赖标签、表情等结构元素，以凸显可信度和专业权威。  \n2) 机器人则偏好简单语言，策略性地交叉使用热门标签，目的是降低信息门槛、插入热门话题并最大化传播面。  \n\n为同时遏制两类行为者，我们提出并评估了27个量化阴谋论扩散严重程度的新指标。实验表明，一种经过改良的“H-指数”方法可在计算可行的条件下有效识别人类超级传播者。  \n\n通过刻画人类超级传播者与机器人账号独有的行为特征，并提供可操作的识别工具，本研究为平台治理（含审核政策、临时及永久封禁）、以及公众意识提升活动等各项缓解策略提供了实证基础。"
    },
    {
        "title": "DOS: Dual-Flow Orthogonal Semantic IDs for Recommendation in Meituan",
        "summary": "Semantic IDs serve as a key component in generative recommendation systems. They not only incorporate open-world knowledge from large language models (LLMs) but also compress the semantic space to reduce generation difficulty. However, existing methods suffer from two major limitations: (1) the lack of contextual awareness in generation tasks leads to a gap between the Semantic ID codebook space and the generation space, resulting in suboptimal recommendations; and (2) suboptimal quantization methods exacerbate semantic loss in LLMs. To address these issues, we propose Dual-Flow Orthogonal Semantic IDs (DOS) method. Specifically, DOS employs a user-item dual flow-framework that leverages collaborative signals to align the Semantic ID codebook space with the generation space. Furthermore, we introduce an orthogonal residual quantization scheme that rotates the semantic space to an appropriate orientation, thereby maximizing semantic preservation. Extensive offline experiments and online A/B testing demonstrate the effectiveness of DOS. The proposed method has been successfully deployed in Meituan's mobile application, serving hundreds of millions of users.",
        "entry_id": "http://arxiv.org/abs/2602.04460v1",
        "pub_date": "2026-02-04",
        "translated_summary": "语义ID是生成式推荐系统的核心要素，它既借助大型语言模型（LLM）融入开放世界知识，又能压缩语义空间以降低生成的难度。然而，现有方法存在两大瓶颈：（1）生成任务缺乏足够的上下文感知，导致语义ID码本空间与真实生成空间之间存在错位，从而带来次优推荐；（2）量化方法不理想，进一步加剧了LLM中的语义损失。为此，我们提出双重正交流语义ID（DOS）框架。具体而言，DOS基于用户–商品双流结构，利用协同信号将语义ID码本空间与生成空间对齐；并设计正交残差量化方案，通过旋转语义空间至更优方向，实现语义保留最大化。大量离线实验与在线A/B测试均验证了DOS的有效性。目前该方法已成功部署于美团移动端应用，服务数亿用户。"
    },
    {
        "title": "SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval",
        "summary": "Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text. Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval. However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image. We propose SDR-CIR, a training-free Semantic Debias Ranking method based on CoT reasoning. First, Selective CoT guides the MLLM to extract visual content relevant to the modification text during image understanding, thereby reducing visual noise at the source. We then introduce a Semantic Debias Ranking with two steps, Anchor and Debias, to mitigate semantic bias. In the Anchor step, we fuse reference image features with target description features to reinforce useful semantics and supplement omitted cues. In the Debias step, we explicitly model the visual semantic contribution of the reference image to the description and incorporate it into the similarity score as a penalty term. By supplementing omitted cues while suppressing redundancy, SDR-CIR mitigates semantic bias and improves retrieval performance. Experiments on three standard CIR benchmarks show that SDR-CIR achieves state-of-the-art results among one-stage methods while maintaining high efficiency. The code is publicly available at https://github.com/suny105/SDR-CIR.",
        "entry_id": "http://arxiv.org/abs/2602.04451v1",
        "pub_date": "2026-02-04",
        "translated_summary": "组成式图像检索（CIR）旨在依据由参考图像和修改文本构成的查询来检索目标图像。近期无需训练的零样本方法常借助多模态大语言模型（MLLM）与思维链（CoT）生成目标图像的描述以进行检索。然而，由于零样本 CIR 固有的模糊匹配特性，此类方法生成的描述往往存在相对目标图像的语义偏差。我们提出 SDR-CIR，一种基于 CoT 推理、无需训练的语义去偏排序方法。首先，选择性 CoT 引导 MLLM 在图像理解阶段仅提取与修改文本相关的视觉内容，从源头抑制视觉噪声。随后，我们设计“锚定”与“去偏”两步语义去偏排序以消除语义偏差。“锚定”阶段通过融合参考图像特征与目标描述特征，强化有效语义并补全遗漏线索；“去偏”阶段显式建模参考图像对描述的视觉语义贡献，并将其作为惩罚项加入相似度计算。通过补全必要线索并抑制冗余信息，SDR-CIR 有效缓解语义偏差并提升检索性能。在三个标准 CIR 基准上的实验表明，SDR-CIR 以单阶段形式取得当前最优，同时保持高效率。代码开源：https://github.com/suny105/SDR-CIR"
    },
    {
        "title": "MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation",
        "summary": "The integration of reinforcement learning (RL) into large language models (LLMs) has opened new opportunities for recommender systems by eliciting reasoning and improving user preference modeling. However, RL-based LLM recommendation faces significant efficiency challenges, making full-data training costly. Existing data selection methods define sample value based on learnability or representativeness, yet their loss- or gradient-driven or dataset coverage-driven criteria often misalign with RL learning dynamics, resulting in suboptimal performance. To address this, we propose MiniRec, a data selection framework tailored for RL-based LLM recommendation. MiniRec evaluates sample learnability using key RL signals -- rewards -- pruning samples that are too easy (too high reward) or too difficult (consistently low reward). It assesses representativeness by aligning sample gradients with the approximated \"ideal\" global RL optimization trajectory, selecting samples that mainly drive model updates, and it also enforces diversity to reduce redundancy. Combined with a curriculum learning strategy from easy to hard samples, MiniRec significantly reduces training cost while largely preserving performance. Extensive experiments demonstrate MiniRec's effectiveness, highlighting the importance of reward-aligned, trajectory-informed data selection in RL-based LLM recommendation.",
        "entry_id": "http://arxiv.org/abs/2602.04278v1",
        "pub_date": "2026-02-04",
        "translated_summary": "将强化学习（RL）引入大型语言模型（LLM），为推荐系统带来了建模推理与用户偏好的全新机遇。然而，基于 RL 的 LLM 推荐受限于训练效率，在全量数据上训练代价高昂。现有样本选择方法通常以可学性或代表性作为样本价值指标，但其依赖的损失/梯度或数据覆盖标准往往与 RL 的学习动态不一致，导致性能次优。为此，我们提出 MiniRec——专为基于 RL 的 LLM 推荐设计的数据选择框架。MiniRec 利用关键的 RL 信号“奖励”评估样本可学性，剔除奖励过高（过易）或持续偏低（过难）的数据；评估代表性时则将近似得到的「理想」全局 RL 优化轨迹作为参照，仅保留能显著推动模型更新的样本，并引入多样性约束以减少冗余。再配合由易到难的渐进式课程学习策略，MiniRec 在大幅削减训练成本的同时几乎无损性能。大量实验证实了 MiniRec 的有效性，突显了与奖励对齐、由轨迹指导的数据选择在基于 RL 的 LLM 推荐中的重要性。"
    },
    {
        "title": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
        "summary": "Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. An effective multimodal retriever needs to handle two main challenges: (1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and (2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. To address these challenges, we propose LILaC, a multimodal retrieval framework featuring two core innovations. First, we introduce a layered component graph, explicitly representing multimodal information at two layers - each representing coarse and fine granularity - facilitating efficient yet precise reasoning. Second, we develop a late-interaction-based subgraph retrieval method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction. Extensive experiments demonstrate that LILaC achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning. We make the artifacts publicly available at github.com/joohyung00/lilac.",
        "entry_id": "http://arxiv.org/abs/2602.04263v1",
        "pub_date": "2026-02-04",
        "translated_summary": "多模态文档检索旨在从包含文本、表格与视觉元素的文档中检索与查询相关的组件。一个有效的多模态检索器需克服两大挑战：（1）缓解因固定、单粒度检索单元带来的无关内容干扰；（2）通过充分捕捉组件在文档内部及跨文档间的语义关系，支持多跳推理。为此，本文提出 LILaC：一个以两项核心创新为特色的多模态检索框架。首先，我们引入分层组件图，在两个粒度层（粗粒度与细粒度）上显式建模多模态信息，从而实现高效且精准的推理。其次，我们提出基于后期交互的子图检索方法，这是一种基于边的策略：先识别粗粒度节点以快速生成候选，再通过后期交互执行细粒度推理。大量实验表明，LILaC 在全部五项基准测试中均达到当前最佳性能，且无需额外微调。相关代码与模型已开源：https://github.com/joohyung00/lilac"
    },
    {
        "title": "Following the TRAIL: Predicting and Explaining Tomorrow's Hits with a Fine-Tuned LLM",
        "summary": "Large Language Models (LLMs) have been widely applied across multiple domains for their broad knowledge and strong reasoning capabilities. However, applying them to recommendation systems is challenging since it is hard for LLMs to extract user preferences from large, sparse user-item logs, and real-time per-user ranking over the full catalog is too time-consuming to be practical. Moreover, many existing recommender systems focus solely on ranking items while overlooking explanations, which could help improve predictive accuracy and make recommendations more convincing to users. Inspired by recent works that achieve strong recommendation performance by forecasting near-term item popularity, we propose TRAIL (TRend and explAnation Integrated Learner). TRAIL is a fine-tuned LLM that jointly predicts short-term item popularity and generates faithful natural-language explanations. It employs contrastive learning with positive and negative pairs to align its scores and explanations with structured trend signals, yielding accurate and explainable popularity predictions. Extensive experiments show that TRAIL outperforms strong baselines and produces coherent, well-grounded explanations.",
        "entry_id": "http://arxiv.org/abs/2602.04225v1",
        "pub_date": "2026-02-04",
        "translated_summary": "大型语言模型（LLM）凭借其广博的知识和强大的推理能力，已在众多领域获得广泛应用。然而，将 LLM 引入推荐系统面临两大挑战：其一，LLM 难以从大规模稀疏的用户-物品交互日志中准确提取用户偏好；其二，在完整物品目录上对每位用户进行实时排序耗时过长，难以落地。此外，现有推荐系统大多仅聚焦物品排序，忽视了解释的重要性——而解释既能提升预测精度，也能让用户更易接受推荐结果。受近期“以预测短期物品热度实现高性能推荐”工作的启发，我们提出 TRAIL（趋势与解释融合学习器）。TRAIL 是一个经微调的 LLM，可同时预测短期物品热度，并生成忠实可靠的自然语言解释。它采用正负样本的对比学习，将其打分与解释与结构化趋势信号对齐，从而实现高精度的可解释热度预测。大量实验表明，TRAIL 不仅超越多组强基线，还能产生连贯且有理有据的解释。"
    },
    {
        "title": "GenMRP: A Generative Multi-Route Planning Framework for Efficient and Personalized Real-Time Industrial Navigation",
        "summary": "Existing industrial-scale navigation applications contend with massive road networks, typically employing two main categories of approaches for route planning. The first relies on precomputed road costs for optimal routing and heuristic algorithms for generating alternatives, while the second, generative methods, has recently gained significant attention. However, the former struggles with personalization and route diversity, while the latter fails to meet the efficiency requirements of large-scale real-time scenarios. To address these limitations, we propose GenMRP, a generative framework for multi-route planning. To ensure generation efficiency, GenMRP first introduces a skeleton-to-capillary approach that dynamically constructs a relevant sub-network significantly smaller than the full road network. Within this sub-network, routes are generated iteratively. The first iteration identifies the optimal route, while the subsequent ones generate alternatives that balance quality and diversity using the newly proposed correctional boosting approach. Each iteration incorporates road features, user historical sequences, and previously generated routes into a Link Cost Model to update road costs, followed by route generation using the Dijkstra algorithm. Extensive experiments show that GenMRP achieves state-of-the-art performance with high efficiency in both offline and online environments. To facilitate further research, we have publicly released the training and evaluation dataset. GenMRP has been fully deployed in a real-world navigation app, demonstrating its effectiveness and benefits.",
        "entry_id": "http://arxiv.org/abs/2602.04174v1",
        "pub_date": "2026-02-04",
        "translated_summary": "现有的大规模路径导航应用需要面对巨型道路网络，通常采取两种主流路线规划方案：一类依赖预先计算的道路代价进行最优路径搜索，并辅以启发式算法产生备选路线；另一类则是最近迅速走红的生成式方法。前者难以实现个性化且路线单一，后者又无法满足大规模实时场景对效率的严苛要求。为克服上述缺陷，我们提出 GenMRP——一种面向多路线规划的生成式框架。为确保生成效率，GenMRP 创新提出“骨干–毛细血管”策略，根据需求动态构建一个远小于全路网的子网络；在此子网络中，框架迭代生成路线：首轮找出最优路径，后续轮次则利用我们新提出的修正提升机制，在质量与多样性之间取得平衡。每一轮迭代均将道路特征、用户历史轨迹以及已生成的路线融入一套链接代价模型来重新估计道路代价，再用 Dijkstra 算法生成路线。大量实验表明，GenMRP 在离线及在线环境中均能以高效率达到顶尖性能。为推动后续研究，我们已公开发布训练与评测数据集。目前 GenMRP 已完整部署在真实导航 App 中，实际应用证明了其有效性与实用价值。"
    },
    {
        "title": "Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval",
        "summary": "Retrieval-Augmented Generation (RAG) systems have been popular for generative applications, powering language models by injecting external knowledge. Companies have been trying to leverage their large catalog of documents (e.g. PDFs, presentation slides) in such RAG pipelines, whose first step is the retrieval component. Dense retrieval has been a popular approach, where embedding models are used to generate a dense representation of the user query that is closer to relevant content embeddings. More recently, VLM-based embedding models have become popular for visual document retrieval, as they preserve visual information and simplify the indexing pipeline compared to OCR text extraction.\n  Motivated by the growing demand for visual document retrieval, we introduce Nemotron ColEmbed V2, a family of models that achieve state-of-the-art performance on the ViDoRe benchmarks. We release three variants - with 3B, 4B, and 8B parameters - based on pre-trained VLMs: NVIDIA Eagle 2 with Llama 3.2 3B backbone, Qwen3-VL-4B-Instruct and Qwen3-VL-8B-Instruct, respectively. The 8B model ranks first on the ViDoRe V3 leaderboard as of February 03, 2026, achieving an average NDCG@10 of 63.42.\n  We describe the main techniques used across data processing, training, and post-training - such as cluster-based sampling, hard-negative mining, bidirectional attention, late interaction, and model merging - that helped us build our top-performing models. We also discuss compute and storage engineering challenges posed by the late interaction mechanism and present experiments on how to balance accuracy and storage with lower dimension embeddings.",
        "entry_id": "http://arxiv.org/abs/2602.03992v1",
        "pub_date": "2026-02-03",
        "translated_summary": "检索增强生成（RAG）系统在现今的生成式应用中占据主流，它们通过注入外部知识来增强语言模型。企业正尝试将海量文档（如 PDF、演示文稿等）纳入此类 RAG 流程，而流程的首要环节即是检索模块。稠密检索是目前最流行的方式：用嵌入模型把用户查询编码成稠密向量，使其在向量空间中靠近相关内容。近期，基于视觉–语言模型（VLM）的嵌入方法在视觉文档检索中受到青睐，既能保留视觉信息，又比 OCR 文本提取更简化了索引链路。\n\n受视觉文档检索需求激增的驱动，我们推出了 Nemotron ColEmbed V2 模型家族，在 ViDoRe 基准测试中刷新性能纪录。我们发布了 3B、4B 与 8B 三种参数规模的版本，分别基于以下预训练 VLM：NVIDIA Eagle 2（以 Llama 3.2-3B 为骨干）、Qwen3-VL-4B-Instruct 和 Qwen3-VL-8B-Instruct。截至 2026 年 2 月 3 日，8B 模型在 ViDoRe V3 排行榜排名第一，平均 NDCG@10 达到 63.42。\n\n文中系统论述了我们在数据处理、训练及后训练阶段采用的关键技术：基于聚类的采样、难负例挖掘、双向注意力、延迟交互（late interaction）和模型融合。这些策略共同构建了我们的顶尖模型。文中还探讨了延迟交互机制在计算与存储上面临的工程挑战，并通过实验说明如何在保证准确率的同时，使用更低维度的嵌入向量来平衡性能与存储开销。"
    },
    {
        "title": "Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals",
        "summary": "Sequential recommender systems rank relevant items by modeling a user's interaction history and computing the inner product between the resulting user representation and stored item embeddings. To avoid the significant memory overhead of storing large item sets, the generative recommendation paradigm instead models each item as a series of discrete semantic codes. Here, the next item is predicted by an autoregressive model that generates the code sequence corresponding to the predicted item. However, despite promising ranking capabilities on small datasets, these methods have yet to surpass traditional sequential recommenders on large item sets, limiting their adoption in the very scenarios they were designed to address. To resolve this, we propose MSCGRec, a Multimodal Semantic and Collaborative Generative Recommender. MSCGRec incorporates multiple semantic modalities and introduces a novel self-supervised quantization learning approach for images based on the DINO framework. Additionally, MSCGRec fuses collaborative and semantic signals by extracting collaborative features from sequential recommenders and treating them as a separate modality. Finally, we propose constrained sequence learning that restricts the large output space during training to the set of permissible tokens. We empirically demonstrate on three large real-world datasets that MSCGRec outperforms both sequential and generative recommendation baselines and provide an extensive ablation study to validate the impact of each component.",
        "entry_id": "http://arxiv.org/abs/2602.03713v1",
        "pub_date": "2026-02-03",
        "translated_summary": "序列化推荐系统通过建模用户交互历史，以用户表示与物品嵌入向量的内积对物品进行排名。为避开存储大规模物品集带来的巨大内存开销，生成式推荐范式将每件物品建模为一系列离散语义码；此时，自动回归模型依次生成下一物品的对应码序列来完成推荐。然而，尽管在小型数据集上展现出有前景的排序能力，这些方法在真正面向的大规模物品集合上仍落后于传统序列化推荐，限制了其初衷场景中的实际应用。\n\n为此，我们提出 MSCGRec——一种多模态语义与协同生成式推荐器。MSCGRec 融合多种语义模态，并提出基于 DINO 的新型图像自监督量化学习方法。此外，MSCGRec 将协同信号与语义信息深度融合，把从序列化推荐器中提取的协同特征视作独立模态。最后，我们设计了受限序列学习机制，通过将训练时的巨大输出空间限缩至允许标记集合，显著降低训练复杂度。在三个大规模真实数据集上的实验结果表明，MSCGRec 同时超越序列化与生成式基线，并辅以大量消融实验验证了各组件的有效性。"
    },
    {
        "title": "Bringing Reasoning to Generative Recommendation Through the Lens of Cascaded Ranking",
        "summary": "Generative Recommendation (GR) has become a promising end-to-end approach with high FLOPS utilization for resource-efficient recommendation. Despite the effectiveness, we show that current GR models suffer from a critical \\textbf{bias amplification} issue, where token-level bias escalates as token generation progresses, ultimately limiting the recommendation diversity and hurting the user experience. By comparing against the key factor behind the success of traditional multi-stage pipelines, we reveal two limitations in GR that can amplify the bias: homogeneous reliance on the encoded history, and fixed computational budgets that prevent deeper user preference understanding.\n  To combat the bias amplification issue, it is crucial for GR to 1) incorporate more heterogeneous information, and 2) allocate greater computational resources at each token generation step. To this end, we propose CARE, a simple yet effective cascaded reasoning framework for debiased GR. To incorporate heterogeneous information, we introduce a progressive history encoding mechanism, which progressively incorporates increasingly fine-grained history information as the generation process advances. To allocate more computations, we propose a query-anchored reasoning mechanism, which seeks to perform a deeper understanding of historical information through parallel reasoning steps. We instantiate CARE on three GR backbones. Empirical results on four datasets show the superiority of CARE in recommendation accuracy, diversity, efficiency, and promising scalability. The codes and datasets are available at https://github.com/Linxyhaha/CARE.",
        "entry_id": "http://arxiv.org/abs/2602.03692v2",
        "pub_date": "2026-02-03",
        "translated_summary": "生成式推荐（GR）已成为一种端到端的解决方案，凭借其高效 FLOPS 利用率，在资源受限的场景中展现出巨大潜力。然而，本文发现当前 GR 模型存在一个关键的 **偏见放大** 问题：随着 token 生成的持续，token 层面的偏差不断被放大，最终降低推荐多样性并损害用户体验。通过与传统多阶段流水线的成功要素进行对比，我们指出了导致 GR 放偏的两个根本局限：  \n1）对编码后的历史信息过度同质化依赖；  \n2）固定的计算预算阻碍了更深层的用户偏好理解。  \n\n为解决这一问题，GR 亟需：  \n1）引入异构信息；  \n2）在每个 token 生成步骤中分配更多算力。  \n为此，我们提出了 CARE——一套简洁而高效的级联推理框架以实现去偏 GR：  \n• 引入**渐进历史编码机制**，在生成过程中逐步融合越来越细粒度的历史信息，从而注入异构信号；  \n• 设计**查询锚定推理机制**，通过并行推理步骤深入挖掘历史信息，动态增加计算投入。  \n\n我们在三种主流 GR 骨干上实例化 CARE，并在四个大规模数据集上进行验证。实验表明，CARE 在推荐准确度、多样性、效率和可扩展性方面均显著优于现有方法。  \n代码与数据集已开源：https://github.com/Linxyhaha/CARE"
    },
    {
        "title": "RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish",
        "summary": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.",
        "entry_id": "http://arxiv.org/abs/2602.03652v1",
        "pub_date": "2026-02-03",
        "translated_summary": "尽管检索增强生成（RAG）能够提升大语言模型的事实性，但现有设计指导仍以英语为中心，制约了对土耳其语等形态丰富语言的深入理解。为此，我们从土耳其语维基百科和 CulturaX 构建了覆盖全面的土耳其语 RAG 数据集，内容包含问答对与对应相关段落。我们对 RAG 流程的七个阶段，从查询改写、重排序到回答精修，在没有进行任务专用微调的前提下展开了基准测试。实验表明，HyDE 等复杂方法在准确率上达到 85%，显著高于 78.70% 的基线；采用交叉编码器重排序与上下文增强的帕累托最优配置以低得多的成本取得 84.60% 的相近表现。我们进一步发现，过度叠加生成模块会通过破坏形态线索降低性能，而简单的查询澄清配合强健的重排序便足以达到理想效果。"
    },
    {
        "title": "Tutorial on Reasoning for IR & IR for Reasoning",
        "summary": "Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies.",
        "entry_id": "http://arxiv.org/abs/2602.03640v1",
        "pub_date": "2026-02-03",
        "translated_summary": "信息检索长期聚焦于依据语义相关性对文档进行排序。然而，许多现实世界的信息需求远不止于此：它们要求强制满足逻辑约束、执行多步推理，并整合多方证据。归根到底，满足这些需求的核心是“推理”这一课题。在人工智能的各个子领域，研究者们正采用多样的解决方案，如推理时刻策略、大模型的后期微调训练、神经-符号系统、贝叶斯及概率框架、几何表示与能量模型等，来攻克推理问题。所有这些努力共同指向同一目标：让系统超越纯粹的“模式匹配”，实现结构化、可验证的推断。然而，这些成果散布于不同学科，亟需梳理，以便信息检索研究者发现最具价值的新思路与机遇。\n\n为了帮助参会者在错综复杂的推理研究图景中“导航”，本教程首先在信息检索语境下给出“推理”的工作定义，并由此提炼出一个统一的分析框架。该框架用一组反映定义核心要素的坐标轴，将现有方法逐一映射。通过对近期代表性方法的全面审阅与其方法轴线的对应映射，我们揭示其权衡与互补关系，指出跨学科成果可如何惠及信息检索，并演示检索过程如何在更宏大的推理体系里占据中心位置。\n\n本教程不仅为与会者提供一套概念框架，还给出了增强具有推理能力的信息检索系统的实践指南，从而将信息检索定位为既能受惠于、也可反哺当前广泛推理方法学发展的领域。"
    },
    {
        "title": "Controlling Output Rankings in Generative Engines for LLM-based Search",
        "summary": "The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.\n  In this work, we propose CORE, an optimization method that \\textbf{C}ontrols \\textbf{O}utput \\textbf{R}ankings in g\\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.\n  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \\textbf{91.4\\% @Top-5}, \\textbf{86.6\\% @Top-3}, and \\textbf{80.3\\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.",
        "entry_id": "http://arxiv.org/abs/2602.03608v1",
        "pub_date": "2026-02-03",
        "translated_summary": "随着大型语言模型（LLM）的兴起，顾客搜索和挑选商品的方式正在发生根本变化。基于 LLM 的“生成式搜索”（generative engines）直接向用户推荐商品，而不再像传统在线搜索那样让用户逐一浏览结果。然而，这些推荐结果高度受限于 LLM 初期的检索排序，导致小商家和独立创作者因排名靠后而缺乏曝光。\n\n本研究提出 CORE，一种专为生成式搜索设计的输出排名优化方法（Controlling Output Rankings in gEnerative Engines）。由于 LLM 本身与搜索引擎的交互过程是黑箱，CORE 把搜索引擎返回的内容作为优化抓手：在原有内容后附加精心设计的“优化内容”，从而引导最终推荐顺序。CORE 构建了三种互补的优化内容：字符型、推理型和评论型，并验证它们在调节排名方面的有效性。\n\n为了贴近真实场景，我们建立 ProductBench 基准：涵盖 15 个商品类别、每类 200 件商品，每件商品都采集了 Amazon 搜索界面前 10 推荐作为真实参考。我们在具备搜索能力的 4 个 LLM（GPT-4o、Gemini-2.5、Claude-4 和 Grok-3）上进行了大规模实验，结果表明：CORE 的“推广成功率”平均达到 **Top-5：91.4%**、**Top-3：86.6%**、**Top-1：80.3%**，全面优于现有排名操控方法，同时保证了优化内容的语言流畅度。"
    },
    {
        "title": "Bagging-Based Model Merging for Robust General Text Embeddings",
        "summary": "General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \\modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \\modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings.",
        "entry_id": "http://arxiv.org/abs/2602.05787v1",
        "pub_date": "2026-02-05",
        "translated_summary": "通用文本嵌入模型支撑了自然语言处理与信息检索的众多应用，通常在大规模多任务语料上训练，以实现广泛泛化。然而，现有实践对不同多任务训练策略的优劣尚不明确，并缺乏在领域与数据类型不断涌现时高效调整嵌入模型的方法。本文从数据调度和模型合并两个角度，系统研究了文本嵌入的多任务训练。我们对比了批次级混洗、顺序训练变体、两阶段训练及多级粒度合并，发现简单的批次级混洗始终表现最佳，表明任务冲突有限且数据集互补。尽管有效，该方法存在两大现实局限：域外泛化次优，以及对增量学习不友好，因需代价高昂的全量重训。为此，我们提出Bagging-based rObust mOdel Merging（\\modelname），通过在采样子集上训练多个嵌入模型后合并为单一模型，既提升了鲁棒性，又保持了单模型推理效率。此外，\\modelname天然支持高效增量更新：针对新数据，仅在少量历史子集上训练轻量级更新模型后并入已有模型。在多样化嵌入基准的实验表明，\\modelname在域内与域外性能均优于全量批次级混洗，并将增量学习场景的训练开销显著降低。"
    },
    {
        "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
        "summary": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
        "entry_id": "http://arxiv.org/abs/2602.05735v1",
        "pub_date": "2026-02-05",
        "translated_summary": "在大基础模型时代，嵌入质量已成为下游任务性能及整体系统能力的核心决定因素。然而，广泛使用的密集嵌入往往维度极高，带来显著的存储、内存和推理延迟开销。为此，近期提出的对比稀疏表示（CSR）被认为是一条有潜力的新路径：它将密集嵌入映射到“高维度却仅保留 k 个非零”的稀疏向量，与 Matryoshka 表征学习（MRL）等仅压缩维度的密集嵌入不同。尽管前景可期，CSR 在极稀疏场景下性能急剧退化——当超过 80% 的神经元永久静默时，其效率潜能远未发挥。\n\n本文提出 CSRv2，一套有原则的稀疏训练框架，旨在使“极限稀疏嵌入”真正可用。CSRv2 通过渐进 k-退火算法稳定稀疏学习，以监督对比目标提升表征质量，并以全骨干端到端微调确保适应性。实验显示，CSRv2 将静默神经元比例从 80% 降至 20%；当 k=2 时，准确率提升 14%，令仅激活 2 个特征的稀疏嵌入即可匹敌 k=8 的 CSR 和 32 维的 MRL。在保持相近精度下，CSRv2 相比 MRL 提供 7× 推理加速，并在文本表征任务中对密集嵌入实现最高 300× 的算力与内存效率提升。\n\n我们对文本和视觉领域的广泛实验表明，CSRv2 让极稀疏嵌入真正落地且不减性能：当 k=4 时，CSRv2 在文本/视觉任务上相对 CSR 提升 7%/4%；在更严苛的 k=2 条件下，该优势扩大至 14%/6%。借助 CSRv2，极限稀疏化被激活，从而拓宽了实时与边缘 AI 系统的设计空间——其中嵌入质量与效率同等关键。"
    },
    {
        "title": "Evaluating the impact of word embeddings on similarity scoring in practical information retrieval",
        "summary": "Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.\n  This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.",
        "entry_id": "http://arxiv.org/abs/2602.05734v1",
        "pub_date": "2026-02-05",
        "translated_summary": "搜索行为常以同义词和多义词为特征，因为用户往往希望基于“意义”来检索信息。为了能充分捕捉语言的这种复杂用法，语义表示策略正朝着更丰富的联想连接演进。向量空间模型（VSM）与神经词嵌入技术在现代机器学习与自然语言处理（NLP）管线中扮演关键角色。嵌入技术通过分布语义，将词、句、段落甚至整篇文档表示为高维空间中的向量，信息检索（IR）系统由此得以利用查询与答案之间的语义相关性。\n\n本文评估了一种可替代现有常见做法的查询语句相似度度量方法，摒弃了以往对神经词嵌入中心点之间距离的比较。受 Word Mover’s Distance（WMD）模型启发，新方法通过计算查询与待检语句中各单独词的“词移”距离来评估相似度。在对查询及相关响应语句的排序实验中，将 WMD 与词嵌入技术联合使用的做法显著提升了准确率。表现最优的“WMD + GloVe” 组合优于所有现有前沿检索模型，包括 Doc2Vec 与基线 LSA。除准确率大幅提升外，我们还得出结论：利用在庞大数据集上预训练的词嵌入，可获得与领域无关、可迁移的通用语言处理方案，适用于多样化的商业场景。"
    },
    {
        "title": "GLASS: A Generative Recommender for Long-sequence Modeling via SID-Tier and Semantic Search",
        "summary": "Leveraging long-term user behavioral patterns is a key trajectory for enhancing the accuracy of modern recommender systems. While generative recommender systems have emerged as a transformative paradigm, they face hurdles in effectively modeling extensive historical sequences. To address this challenge, we propose GLASS, a novel framework that integrates long-term user interests into the generative process via SID-Tier and Semantic Search. We first introduce SID-Tier, a module that maps long-term interactions into a unified interest vector to enhance the prediction of the initial SID token. Unlike traditional retrieval models that struggle with massive item spaces, SID-Tier leverages the compact nature of the semantic codebook to incorporate cross features between the user's long-term history and candidate semantic codes. Furthermore, we present semantic hard search, which utilizes generated coarse-grained semantic ID as dynamic keys to extract relevant historical behaviors, which are then fused via an adaptive gated fusion module to recalibrate the trajectory of subsequent fine-grained tokens. To address the inherent data sparsity in semantic hard search, we propose two strategies: semantic neighbor augmentation and codebook resizing. Extensive experiments on two large-scale real-world datasets, TAOBAO-MM and KuaiRec, demonstrate that GLASS outperforms state-of-the-art baselines, achieving significant gains in recommendation quality. Our codes are made publicly available to facilitate further research in generative recommendation.",
        "entry_id": "http://arxiv.org/abs/2602.05663v1",
        "pub_date": "2026-02-05",
        "translated_summary": "利用长期用户行为模式是现代推荐系统提升精度的关键路径。尽管生成式推荐系统已成为变革范式，但它们在有效建模超长历史序列时仍面临诸多障碍。为此，我们提出 GLASS 框架，通过 SID-Tier 与语义搜索将长期兴趣嵌入生成过程。首先，我们引入 SID-Tier 模块，将用户的长期交互压缩为统一兴趣向量，以更准确预测初始 SID（Semantic ID）令牌。相比传统检索模型在大规模商品空间中的不足，SID-Tier 借助紧凑的语义码本，引入用户长期历史与候选语义码之间的跨特征。接着，我们设计语义硬搜索：使用粗粒度生成的语义 ID 作为动态键，召回相关历史行为，并通过自适应门控融合模块重新标定后续细粒度令牌的生成轨迹。为缓解语义硬搜索固有的数据稀疏问题，我们提出两种策略：语义邻居增强与码本伸缩。在 TAOBAO-MM 与 KuaiRec 两大真实世界大规模数据集上的广泛实验表明，GLASS 显著超越最新基线，推荐质量获得大幅跃升。相关代码已开源，以促进生成式推荐领域的进一步研究。"
    },
    {
        "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
        "summary": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
        "entry_id": "http://arxiv.org/abs/2602.05512v1",
        "pub_date": "2026-02-05",
        "translated_summary": "大型语言模型（LLM）在语言理解方面表现出色，但在知识密集型领域仍受限：存在幻觉、信息过时及解释性不足。基于文本的检索增强生成（RAG）可让模型输出依托外部来源，却难承担多跳推理任务。相对而言，知识图谱（KG）支持精确且可解释的多跳查询，却要求用户掌握形式化查询语言。为此，我们提出一种交互式框架：由 LLM 利用自然语言生成并解释 Cypher 图查询，用户以自然语言逐轮精炼。在真实 KG 上的实验表明，该框架在保持事实准确和语义严谨的同时，降低了复杂数据集的访问门槛，并揭示了模型在不同领域的表现差异。核心量化评估为基于合成电影 KG 的 90 个查询基准，衡量多条 LLM 的查询解释质量与故障检测能力；辅以 Hyena KG 和 MaRDI（数学研究数据倡议）KG 上的两个小规模现实查询生成实验。"
    },
    {
        "title": "LMMRec: LLM-driven Motivation-aware Multimodal Recommendation",
        "summary": "Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information like review text. In multimodal motivation fusion, two challenges arise: 1) achieving stable cross-modal alignment amid noise, and 2) identifying features reflecting the same underlying motivation across modalities. To address these, we propose LLM-driven Motivation-aware Multimodal Recommendation (LMMRec), a model-agnostic framework leveraging large language models for deep semantic priors and motivation understanding. LMMRec uses chain-of-thought prompting to extract fine-grained user and item motivations from text. A dual-encoder architecture models textual and interaction-based motivations for cross-modal alignment, while Motivation Coordination Strategy and Interaction-Text Correspondence Method mitigate noise and semantic drift through contrastive learning and momentum updates. Experiments on three datasets show LMMRec achieves up to a 4.98\\% performance improvement.",
        "entry_id": "http://arxiv.org/abs/2602.05474v1",
        "pub_date": "2026-02-05",
        "translated_summary": "基于动机的推荐系统能够揭示驱动用户行为的核心因素。动机建模对决策过程和用户内容偏好至关重要，也是解释推荐结果的关键。现有方法通常仅从交互数据中把动机视为隐变量，忽视了评论文本等异质信息。在多模态动机融合过程中，系统面临两大挑战：一是如何在噪声影响下实现跨模态的稳定对齐；二是如何甄别不同模态中真正映射同一动机的特征。\n\n为此，我们提出 LLM 驱动的动机感知多模态推荐框架 LMMRec。该框架与具体模型无关，利用大语言模型提供深层语义先验和动机理解。LMMRec 采用链式思维提示，从文本中抽取细粒度的用户动机与商品动机；通过双编码器架构分别建模文本动机与交互动机，实现跨模态对齐。Motivation Coordination 策略和 Interaction-Text Correspondence 方法借助对比学习与动量更新机制，抑制噪声并防止语义漂移。在三个数据集上的实验证明，LMMRec 性能最高提升 4.98%。"
    },
    {
        "title": "Forward Index Compression for Learned Sparse Retrieval",
        "summary": "Text retrieval using learned sparse representations of queries and documents has, over the years, evolved into a highly effective approach to search. It is thanks to recent advances in approximate nearest neighbor search-with the emergence of highly efficient algorithms such as the inverted index-based Seismic and the graph-based Hnsw-that retrieval with sparse representations became viable in practice. In this work, we scrutinize the efficiency of sparse retrieval algorithms and focus particularly on the size of a data structure that is common to all algorithmic flavors and that constitutes a substantial fraction of the overall index size: the forward index. In particular, we seek compression techniques to reduce the storage footprint of the forward index without compromising search quality or inner product computation latency. In our examination with various integer compression techniques, we report that StreamVByte achieves the best trade-off between memory footprint, retrieval accuracy, and latency. We then improve StreamVByte by introducing DotVByte, a new algorithm tailored to inner product computation. Experiments on MsMarco show that our improvements lead to significant space savings while maintaining retrieval efficiency.",
        "entry_id": "http://arxiv.org/abs/2602.05445v1",
        "pub_date": "2026-02-05",
        "translated_summary": "利用查询与文档的稀疏学习到表示进行文本检索，近年来已发展为一种极其高效的搜索范式。得益于近似最近邻搜索（ANN）的最新突破——高效的倒排索引算法Seismic与基于图的HNSW等——稀疏表示检索才得以在实际中大规模落地。本文系统评估了各类稀疏检索算法的效率，聚焦于所有算法共用且占整体索引尺寸比例可观的关键数据结构：前向索引（forward index）。我们着力研究如何在不影响搜索质量或内积计算延迟的前提下，对其进行压缩以缩减存储。通过广泛实验多种整数压缩技术，我们发现 StreamVByte 能在内存占用、检索精度与延迟之间取得最佳权衡。于是，我们进一步提出 DotVByte：一种专为高效内积计算改造的新算法，在 StreamVByte 基础上加以改进。在 MS-MARCO 数据集上的实验证明，我们的方法在保证检索效率的同时显著减小了索引体积。"
    },
    {
        "title": "SciDef: Automating Definition Extraction from Academic Literature with Large Language Models",
        "summary": "Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them.\n  Code & datasets are available at https://github.com/Media-Bias-Group/SciDef.",
        "entry_id": "http://arxiv.org/abs/2602.05413v1",
        "pub_date": "2026-02-05",
        "translated_summary": "定义是一切科学研究的基石；然而，在发表文献数量激增的今天，针对任意关键词快速收集相关定义已愈加困难。为解决这一问题，我们推出了 SciDef——一个基于大语言模型（LLM）的自动化定义提取管线。我们在两个新构建的数据集上测试 SciDef：DefExtra 提供人工标注的定义语料，DefSim 则提供定义之间的成对相似度标注。我们对 16 种语言模型和多种提示策略进行了评估，结果证明多步提示与经 DSPy 优化的提示都能显著提升提取效果。为了准确衡量提取质量，我们比较了多种评估指标，发现基于 NLI（自然语言推理）的方法最为可靠。实验表明，大模型大体能够成功从科学文献中提取定义（在测试集中检出率达 86.4%），但仍存在过生成问题。因此，未来研究不应止步于“发现”定义，而应进一步聚焦于“甄别”定义的相关性。\n\n代码与数据集已开源：https://github.com/Media-Bias-Group/SciDef"
    },
    {
        "title": "Rich-Media Re-Ranker: A User Satisfaction-Driven LLM Re-ranking Framework for Rich-Media Search",
        "summary": "Re-ranking plays a crucial role in modern information search systems by refining the ranking of initial search results to better satisfy user information needs. However, existing methods show two notable limitations in improving user search satisfaction: inadequate modeling of multifaceted user intents and neglect of rich side information such as visual perception signals. To address these challenges, we propose the Rich-Media Re-Ranker framework, which aims to enhance user search satisfaction through multi-dimensional and fine-grained modeling. Our approach begins with a Query Planner that analyzes the sequence of query refinements within a session to capture genuine search intents, decomposing the query into clear and complementary sub-queries to enable broader coverage of users' potential intents. Subsequently, moving beyond primary text content, we integrate richer side information of candidate results, including signals modeling visual content generated by the VLM-based evaluator. These comprehensive signals are then processed alongside carefully designed re-ranking principle that considers multiple facets, including content relevance and quality, information gain, information novelty, and the visual presentation of cover images. Then, the LLM-based re-ranker performs the holistic evaluation based on these principles and integrated signals. To enhance the scenario adaptability of the VLM-based evaluator and the LLM-based re-ranker, we further enhance their capabilities through multi-task reinforcement learning. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines. Notably, the proposed framework has been deployed in a large-scale industrial search system, yielding substantial improvements in online user engagement rates and satisfaction metrics.",
        "entry_id": "http://arxiv.org/abs/2602.05408v1",
        "pub_date": "2026-02-05",
        "translated_summary": "重排序在现代信息检索系统中起着关键作用，通过对初步检索结果的再次排序，更好地满足用户的信息需求。然而，现有方法在提高用户搜索满意度方面存在两大明显局限：其一，对用户多维度意图的建模不足；其二，忽视了如视觉感知信号等丰富的辅助信息。为解决这些挑战，我们提出 Rich-Media Re-Ranker 框架，旨在通过多维度且细粒度的建模显著提升用户搜索满意度。\n\n我们的方法首先从 Query Planner 入手，分析会话内一系列查询修订的序列，捕捉用户的真实搜索意图，将主查询拆解为清晰且互补的子查询，从而更全面地覆盖潜在的用户意图。随后，除了主要文本内容外，我们还引入候选结果的丰富辅助信息，包括由基于视觉语言模型（VLM）的评估器生成的视觉内容感知信号。这些综合信号在重排序阶段与精心设计的多面原则共同处理，原则涵盖内容相关性与质量、信息增益、信息新颖度，以及封面图的视觉呈现效果。接着，基于大语言模型（LLM）的重排序器利用上述原则与整合信号进行整体评估。\n\n为了提升基于 VLM 的评估器和基于 LLM 的重排序器在不同场景下的适应能力，我们通过多任务强化学习进一步增强两者能力。大量实验表明，我们的方法显著优于现有的最强基线。更重要的是，该框架已成功部署于大规模工业搜索系统，在线用户参与度与满意度指标均获得大幅提升。"
    },
    {
        "title": "Multi-Field Tool Retrieval",
        "summary": "Integrating external tools enables Large Language Models (LLMs) to interact with real-world environments and solve complex tasks. Given the growing scale of available tools, effective tool retrieval is essential to mitigate constraints of LLMs' context windows and ensure computational efficiency. Existing approaches typically treat tool retrieval as a traditional ad-hoc retrieval task, matching user queries against the entire raw tool documentation. In this paper, we identify three fundamental challenges that limit the effectiveness of this paradigm: (i) the incompleteness and structural inconsistency of tool documentation; (ii) the significant semantic and granular mismatch between user queries and technical tool documents; and, most importantly, (iii) the multi-aspect nature of tool utility, that involves distinct dimensions, such as functionality, input constraints, and output formats, varying in format and importance. To address these challenges, we introduce Multi-Field Tool Retrieval, a framework designed to align user intent with tool representations through fine-grained, multi-field modeling. Experimental results show that our framework achieves SOTA performance on five datasets and a mixed benchmark, exhibiting superior generalizability and robustness.",
        "entry_id": "http://arxiv.org/abs/2602.05366v1",
        "pub_date": "2026-02-05",
        "translated_summary": "为了让大语言模型（LLM）与现实世界环境交互并完成复杂任务，需要借助外部工具。随着可用工具数量的快速增长，如何高效地检索合适工具变得至关重要，这既可缓解 LLM 上下文窗口的限制，也能显著提升计算效率。现有方法通常将工具检索视作传统的 ad-hoc 检索任务，直接将用户查询与原始工具描述文档进行匹配。本文指出该范式存在三大根本性挑战：（i）工具文档信息不完整且结构不一致；（ii）用户的自然语言查询与技术性工具描述在语义和粒度层面严重不匹配；（iii）更为重要的是，工具的实用价值具有多维度特性，包括功能、输入约束、输出格式等不同侧面，而这些侧面的形式和重要性各异。\n\n为应对上述挑战，我们提出 Multi-Field Tool Retrieval（多字段工具检索）框架，通过细粒、多字段建模将用户意图与工具表征精准对齐。实验结果表明，该框架在五个数据集及一个综合基准测试上均达到了 SOTA 性能，展现出卓越的泛化能力与鲁棒性。"
    },
    {
        "title": "NeuCLIRTech: Chinese Monolingual and Cross-Language Information Retrieval Evaluation in a Challenging Domain",
        "summary": "Measuring advances in retrieval requires test collections with relevance judgments that can faithfully distinguish systems. This paper presents NeuCLIRTech, an evaluation collection for cross-language retrieval over technical information. The collection consists of technical documents written natively in Chinese and those same documents machine translated into English. It includes 110 queries with relevance judgments. The collection supports two retrieval scenarios: monolingual retrieval in Chinese, and cross-language retrieval with English as the query language. NeuCLIRTech combines the TREC NeuCLIR track topics of 2023 and 2024. The 110 queries with 35,962 document judgments provide strong statistical discriminatory power when trying to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included so that developers of reranking algorithms are not reliant on BM25 as their first stage retriever. The dataset and artifacts are released on Huggingface Datasets",
        "entry_id": "http://arxiv.org/abs/2602.05334v1",
        "pub_date": "2026-02-05",
        "translated_summary": "衡量检索技术的进展，离不开能够通过相关判定准确区分系统性能水准的测试集合。本研究提出 NeuCLIRTech——一个面向技术领域的跨语言检索评测集合。该集合包含由中文原文撰写且已机器翻译为英文的技术文档，涵盖 110 条查询与相关判定，可同时支持两种检索情境：以中文为查询语言的单语检索，以及以英文为查询语言的跨语言检索。NeuCLIRTech 整合了 TREC NeuCLIR 2023 与 2024 两届评测的查询主题，共产生 110 条查询与 35,962 篇文档的相关判定，具备充足的统计区分能力，可用于可靠地区分不同检索方法。我们还提供了一组由前沿神经检索系统融合得到的基础排序结果，使后续的重新排序（reranking）算法开发者无需再局限以 BM25 作首轮检索。完整的数据集及配套资源已发布在 Huggingface Datasets 平台。"
    },
    {
        "title": "Semantic Search over 9 Million Mathematical Theorems",
        "summary": "Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of $9.2$ million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at \\href{https://huggingface.co/spaces/uw-math-ai/theorem-search}{this link}, and the dataset is available at \\href{https://huggingface.co/datasets/uw-math-ai/TheoremSearch}{this link}.",
        "entry_id": "http://arxiv.org/abs/2602.05216v1",
        "pub_date": "2026-02-05",
        "translated_summary": "数学结果的检索仍然困难重重：现有工具大多只能返回整篇论文，然而数学家和定理自动证明系统往往只需要找到恰好能够回答提问的那条定理、引理或命题。虽然语义检索技术近年来进展迅猛，其在大型、高度专业的语料（例如研究级别的数学定理）上表现如何仍缺乏深入理解。\n\n本工作首次提出并研究了**语义定理检索**的大规模实现，使用统一语料库共 920 万条定理陈述，数据源自 arXiv 以及其他七个来源，是目前公开发布且最大的、由人类撰写的研究级定理集合。我们为每条定理生成简短的自然语言描述作为检索向量，系统探究了**定理上下文范围、语言模型选择、嵌入模型**以及**提示策略**等多重因素对检索效果的影响。\n\n在由职业数学家编写的定理检索基准测试上，与现有基线相比，我们的方法在“定理级”和“论文级”检索任务上均取得显著提升，证明语义定理搜索已可在网络规模下实现并行之有效。\n\n定理检索工具现已上线：  \n🔗 https://huggingface.co/spaces/uw-math-ai/theorem-search  \n相关数据集公开地址：  \n🔗 https://huggingface.co/datasets/uw-math-ai/TheoremSearch"
    },
    {
        "title": "RAG without Forgetting: Continual Query-Infused Key Memory",
        "summary": "Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed.",
        "entry_id": "http://arxiv.org/abs/2602.05152v1",
        "pub_date": "2026-02-05",
        "translated_summary": "检索增强生成（RAG）系统通常依赖即席查询适配来提升鲁棒性，如查询扩展和迭代检索。这些方法虽有效，却是无状态的：每次查询都必须重新计算适配，事后便被丢弃，既阻碍了累积学习，也反复产生推理成本。键扩展等索引侧做法引入了“持久性”，但依赖离线预处理或启发式更新，与下游任务效用对齐不充分，易导致语义漂移和噪声累积。为此，我们提出演化检索记忆（ERM），这是一个无需训练的框架，可把瞬时的查询增益转译为持久的检索改进。ERM通过“正确性门控”反馈实时更新索引，精确定位并归因每一点原子级扩展信号到其真正受益的文档键，以稳定且范数有界的方式逐步演化这些键。理论表明，在常用相似度下，查询扩展与键扩展完全等价；我们进一步证明了ERM选择性更新的收敛特性，从而将最优查询扩展开销永久摊销至稳定索引，推理阶段零额外成本。在 BEIR 与 BRIGHT 跨越 13 个领域的实验表明：检索与生成就算全部以原生速度运行，也能稳步提升，尤其对重推理任务效果显著。"
    },
    {
        "title": "HugRAG: Hierarchical Causal Knowledge Graph Design for RAG",
        "summary": "Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.",
        "entry_id": "http://arxiv.org/abs/2602.05143v1",
        "pub_date": "2026-02-04",
        "translated_summary": "检索增强生成（RAG）通过为大型语言模型提供外部知识接入能力而显著提升了其性能，其中基于图的 RAG 更是成为结构化检索与推理的有力主流范式。然而，现有图基方法往往过度依赖表层节点匹配，且缺乏显式因果建模，导致生成结果出现不可信或伪相关问题。此前在因果建模方面的工作通常局限于局部场景或单文档情境，并受碎片化图结构的“信息孤岛”困扰，制约了可扩展性与跨模块因果推理。\n\n为此，我们提出 HugRAG：一种通过跨层级模块实施因果门控、重新思考知识组织的图基 RAG 框架。HugRAG 显式建模因果关系以抑制伪相关，同时支持对大规模知识图的可扩展推理。大量实验表明，在多个数据集和评价指标上，HugRAG 均显著优于当前领先的图基 RAG 基线。本研究为构建结构化、可扩展且因果明确的 RAG 系统奠定了规范化基础。"
    },
    {
        "title": "Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling",
        "summary": "Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent's decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.",
        "entry_id": "http://arxiv.org/abs/2602.05087v1",
        "pub_date": "2026-02-04",
        "translated_summary": "系统性文献综述（SLR）是循证研究的核心，但随着科学产出爆发式增长，人工筛查日益成为瓶颈。筛查任务的特点是相关研究极少且专家决策稀缺、代价高昂。传统主动学习（AL）系统虽有帮助，但通常采用固定的查询策略来挑选下一批未标记文献，这些静态策略既无法随时间演进，也无视科学引文网络中天然存在的关联结构。本论文提出 AutoDiscover 框架，将 AL 视为一个由自适应智能体驱动的在线决策问题。该框架将文献建模为异构图，表征文献、作者及元数据之间的多元关系。Heterogeneous Graph Attention Network（HAN）学习节点表示，Discounted Thompson Sampling（DTS）智能体则据此动态维护一组查询策略的投资组合。凭借人机协同的实时标签，智能体在非平稳的综述环境下（策略效用随时间变化）实现探索与利用的动态平衡。在 26 个数据集构成的 SYNERGY 基准上，AutoDiscover 相比静态 AL 基线显著提升筛查效率。尤为关键的是，仅需极少初始标签即可启动发现过程，突破冷启动瓶颈，而静态方法在此情境下常告失败。此外，我们公开了 TS-Insight，一款可视化分析仪表盘，便于解释、验证并诊断智能体的每一次决策。综上，这些成果在专家标签稀缺、相关研究罕见的情形下，为 SLR 筛查提供了新的加速范式。"
    },
    {
        "title": "Scaling Laws for Embedding Dimension in Information Retrieval",
        "summary": "Dense retrieval, which encodes queries and documents into a single dense vector, has become the dominant neural retrieval approach due to its simplicity and compatibility with fast approximate nearest neighbor algorithms. As the tasks dense retrieval performs grow in complexity, the fundamental limitations of the underlying data structure and similarity metric -- namely vectors and inner-products -- become more apparent. Prior recent work has shown theoretical limitations inherent to single vectors and inner-products that are generally tied to the embedding dimension. Given the importance of embedding dimension for retrieval capacity, understanding how dense retrieval performance changes as embedding dimension is scaled is fundamental to building next generation retrieval models that balance effectiveness and efficiency. In this work, we conduct a comprehensive analysis of the relationship between embedding dimension and retrieval performance. Our experiments include two model families and a range of model sizes from each to construct a detailed picture of embedding scaling behavior. We find that the scaling behavior fits a power law, allowing us to derive scaling laws for performance given only embedding dimension, as well as a joint law accounting for embedding dimension and model size. Our analysis shows that for evaluation tasks aligned with the training task, performance continues to improve as embedding size increases, though with diminishing returns. For evaluation data that is less aligned with the training task, we find that performance is less predictable, with performance degrading with larger embedding dimensions for certain tasks. We hope our work provides additional insight into the limitations of embeddings and their behavior as well as offers a practical guide for selecting model and embedding dimension to achieve optimal performance with reduced storage and compute costs.",
        "entry_id": "http://arxiv.org/abs/2602.05062v1",
        "pub_date": "2026-02-04",
        "translated_summary": "稠密检索把查询与文档编码成单个稠密向量，凭借简洁性以及与快速近似最近邻算法的天然兼容，已成为主流神经检索范式。随着任务复杂度不断攀升，“向量+内积”这一底层数据结构及相似度度量的固有局限愈发凸显：先前研究已在理论上指出，单向量与内积存在受限于嵌入维度的容量瓶颈。鉴于嵌入维度对检索能力的决定性作用，研究其扩展规律，是实现“权衡效果与效率”的下一代检索模型的关键。\n\n本文系统分析了嵌入维度与检索性能之间的关系。实验涵盖两个模型家族，每族各用多种规模，以刻画嵌入的扩展行为。我们发现该行为遵循幂律，据此推导出仅以嵌入维度为变量的性能缩放律，以及同时考虑嵌入维度与模型规模的联合律。分析表明：当评测任务高度匹配训练任务时，增大嵌入维度仍能提升性能，但边际收益递减；而在任务间差异较大时，性能表现出不可预测性，某些任务甚至在嵌入维度上升时出现性能下降。本文旨在深化对嵌入局限与行为机制的理解，并为选择模型及其嵌入维度、降低存储与计算成本提供实践指南。"
    },
    {
        "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
        "summary": "With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.",
        "entry_id": "http://arxiv.org/abs/2602.05014v1",
        "pub_date": "2026-02-04",
        "translated_summary": "随着能使用工具、具备规划能力的大型语言模型（LLM）日新月异，检索增强生成（RAG）正从一次性、被动式检索转变为多轮、由决策驱动的证据获取。尽管在开放域场景中表现出色，现有的可行动搜索框架仍普遍把长文档视作平面切块集合，未能充分挖掘文档本身的层级结构和顺序话语结构等先验信息。本文提出的 DeepRead 是一种具备结构感知能力的多轮文档推理 agent，将这些先验信息显式整合到长文档问答任务中。DeepRead 首先借助基于 LLM 的 OCR 模型把 PDF 转化为保留标题与段落边界的结构化 Markdown，随后在段落粒度建立索引，并用“坐标式”元数据键为每个段落编码其所属章节及章节内顺序。\n\n在此基础上，DeepRead 为 LLM 配备两种互补工具：  \n• Retrieve 工具——在定位相关段落时，同时暴露其结构坐标并附带轻量级上下文以供快速扫读；  \n• ReadSection 工具——支持在指定章节与段落区间进行按序、连贯的深度阅读。\n\n实验表明，DeepRead 在长文档问答任务上相比 Search-o1 式 agentic 搜索取得显著提升，且验证了检索与阅读工具之间的协同效应。细粒度行为分析进一步揭示，DeepRead 的阅读与推理模式呈现出近似人类的“先定位、后精读”范式。"
    },
    {
        "title": "Robust Generalizable Heterogeneous Legal Link Prediction",
        "summary": "Recent work has applied link prediction to large heterogeneous legal citation networks \\new{with rich meta-features}. We find that this approach can be improved by including edge dropout and feature concatenation for the learning of more robust representations, which reduces error rates by up to 45%. We also propose an approach based on multilingual node features with an improved asymmetric decoder for compatibility, which allows us to generalize and extend the prediction to more, geographically and linguistically disjoint, data from New Zealand. Our adaptations also improve inductive transferability between these disjoint legal systems.",
        "entry_id": "http://arxiv.org/abs/2602.04812v1",
        "pub_date": "2026-02-04",
        "translated_summary": "最新研究将带丰富元特征的链路预测技术应用于大规模异构法律引文网络。我们发现，通过引入边随机失活和特征拼接策略可学习更为鲁棒的表征，使错误率降低多达 45%。此外，我们提出一种利用多语言节点特征的方法，并采用改进的非对称解码器以保证兼容性，从而将预测推广到新西兰地理与语言均不相交的新数据集。这些改进还增强了在两个互不相交的法律体系间的归纳可迁移性。"
    },
    {
        "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
        "summary": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.",
        "entry_id": "http://arxiv.org/abs/2602.04735v1",
        "pub_date": "2026-02-04",
        "translated_summary": "大型语言模型（LLM）即便在没有显式线索或恶意内容的情况下，仍可能从看似无害的训练数据中习得意想不到的偏见。现有方法难以在微调前检测此类风险，使得事后评估既昂贵又低效。为解决该难题，我们提出了“Data2Behavior”任务，用于在训练前预判模型的非预期行为。同时，我们设计了轻量级的“Manipulating Data Features（MDF）”方法：首先用数据的均值表征对候选数据进行摘要，再将其注入到基模型的前向过程中，使数据中潜在的统计信号影响模型激活，从而无需更新任何参数即可揭示潜在偏见与安全风险。该方法仅需微调所需约 20% 的 GPU 资源即可实现可靠预测。在 Qwen3-14B、Qwen2.5-32B-Instruct 及 Gemma-3-12b-it 上的实验表明，MDF 能够前瞻性地预测非预期行为，并为发现预训练阶段的漏洞提供洞见。"
    },
    {
        "title": "Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention",
        "summary": "Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to steer an LLM's output to an undesired response. We argue that the standard causal attention mechanism in LLMs enables harmful cross-document interactions, specifically in cases of attacks. Accordingly, we introduce a novel defense approach for RAG: Sparse Document Attention RAG (SDAG). This is a block-sparse attention mechanism that disallows cross-attention between retrieved documents. SDAG requires a minimal inference-time change to the attention mask; furthermore, no fine-tuning or additional architectural changes are needed. We present an empirical evaluation of LLM-based question answering (QA) with a variety of attack strategies on RAG. We show that our SDAG method substantially outperforms the standard causal attention mechanism in terms of attack success rate. We further demonstrate the clear merits of integrating SDAG with state-of-the-art RAG defense methods. Specifically, the integration results in performance that is statistically significantly better than the state-of-the-art.",
        "entry_id": "http://arxiv.org/abs/2602.04711v2",
        "pub_date": "2026-02-04",
        "translated_summary": "检索增强生成（RAG）是让基于 LLM 的响应保持最新、并降低幻觉概率的高效范式。然而，最新研究表明，RAG 对语料知识中毒极为脆弱：攻击者向语料注入误导性文档，从而将 LLM 的输出引向错误答案。我们指出，LLM 中标准的因果注意力机制在面对攻击时会促成有害的跨文档交互。为此，我们提出一种新颖的 RAG 防御方法——稀疏文档注意力 RAG（SDAG）。该方法采用块稀疏注意力机制，禁止检索到的文档之间进行跨注意力计算。 SDAG 仅需在推理时对注意力掩码做最小改动，无需微调或额外架构变更。\n\n我们基于多种攻击策略，对 LLM 问答（QA）任务中的 RAG 进行了实证评估。实验表明，在攻击成功率指标上，SDAG 大幅优于传统因果注意力机制。进一步地，将 SDAG 集成到当前最先进的 RAG 防御方法后，整体表现显著超越现有最优方案，差异具有统计学意义。"
    },
    {
        "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
        "summary": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.",
        "entry_id": "http://arxiv.org/abs/2602.05975v1",
        "pub_date": "2026-02-05",
        "translated_summary": "深度研究智能体已成为应对复杂查询的强大系统，与此同时，基于大模型的检索器在遵从指令和进行推理方面也展现出突出能力。然而，一个关键问题随之浮现：基于大模型的检索器能否有效融入深度研究智能体的工作流程？为探究这一问题，我们推出 SAGE 基准——一个专为科学文献检索设计的评测集，涵盖四个科学领域的 1 200 个查询，以及一个包含 20 万篇论文的可检索语料库。\n\n我们评估了六种深度研究智能体，发现所有系统在执行需要大量推理的检索任务时均表现欠佳。以 DR Tulu 为骨干模型，我们将 BM25 与两类基于大模型的检索器（ReasonIR 及 gte-Qwen2-7B-instruct）作为候选搜索工具进行比较。出乎意料的是，BM25 比大模型检索器高出约 30%。原因在于，现有智能体倾向于生成面向关键词的子查询。为提升检索效果，我们提出一套在语料级测试时扩展的框架：通过大模型为文档补充元数据和关键词，降低现有检索器的工作难度。实验表明，该方法在简短题型上提升 8%，在开放题型上提升 2%。"
    },
    {
        "title": "AgenticTagger: Structured Item Representation for Recommendation with LLM Agents",
        "summary": "High-quality representations are a core requirement for effective recommendation. In this work, we study the problem of LLM-based descriptor generation, i.e., keyphrase-like natural language item representation generation frameworks with minimal constraints on downstream applications. We propose AgenticTagger, a framework that queries LLMs for representing items with sequences of text descriptors. However, open-ended generation provides little control over the generation space, leading to high cardinality, low-performance descriptors that renders downstream modeling challenging. To this end, AgenticTagger features two core stages: (1) a vocabulary building stage where a set of hierarchical, low-cardinality, and high-quality descriptors is identified, and (2) a vocabulary assignment stage where LLMs assign in-vocabulary descriptors to items. To effectively and efficiently ground vocabulary in the item corpus of interest, we design a multi-agent reflection mechanism where an architect LLM iteratively refines the vocabulary guided by parallelized feedback from annotator LLMs that validates the vocabulary against item data. Experiments on public and private data show AgenticTagger brings consistent improvements across diverse recommendation scenarios, including generative and term-based retrieval, ranking, and controllability-oriented, critique-based recommendation.",
        "entry_id": "http://arxiv.org/abs/2602.05945v1",
        "pub_date": "2026-02-05",
        "translated_summary": "高质量的表征是有效推荐系统的核心需求。本文聚焦于“基于大型语言模型的描述符生成”问题——即在极少依赖下游应用的约束下，生成短语式自然语言物品表示。我们提出 AgenticTagger：通过向大模型提出查询，以文本描述符序列来刻画物品。然而，开放式生成几乎无法控制生成空间，导致描述符基数极高、质量参差，给后续建模带来显著挑战。\n\n为此， AgenticTagger 设计了两个核心阶段：\n1. 词汇构建阶段：构建一套层次化、低基数且高质量的描述符全集；\n2. 词汇分配阶段：让大模型从该全集内为每件物品分配恰当描述符。\n\n为高效、准确地将词汇锚定在目标物品语料中，我们设计了一种多智能体反思机制：由一个“架构师”大模型在多轮迭代中精炼词汇，同时并行地从多个“标注员”大模型获取反馈，后者负责在真实物品数据上验证当前词汇的适用性与质量。\n\n在公开和私有数据集上的实验表明，AgenticTagger 在多种推荐场景——包括生成式检索、关键词检索、排序，以及面向可控性或基于评论的推荐——均带来了持续提升。"
    },
    {
        "title": "On the Efficiency of Sequentially Aware Recommender Systems: Cotten4Rec",
        "summary": "Sequential recommendation (SR) models predict a user's next interaction by modeling their historical behaviors. Transformer-based SR methods, notably BERT4Rec, effectively capture these patterns but incur significant computational overhead due to extensive intermediate computations associated with Softmax-based attention. We propose Cotten4Rec, a novel SR model utilizing linear-time cosine similarity attention, implemented through a single optimized compute unified device architecture (CUDA) kernel. By minimizing intermediate buffers and kernel-launch overhead, Cotten4Rec substantially reduces resource usage compared to BERT4Rec and the linear-attention baseline, LinRec, especially for datasets with moderate sequence lengths and vocabulary sizes. Evaluations across three benchmark datasets confirm that Cotten4Rec achieves considerable reductions in memory and runtime with minimal compromise in recommendation accuracy, demonstrating Cotten4Rec's viability as an efficient alternative for practical, large-scale sequential recommendation scenarios where computational resources are critical.",
        "entry_id": "http://arxiv.org/abs/2602.06935v1",
        "pub_date": "2026-02-06",
        "translated_summary": "顺序推荐（SR）模型利用用户历史行为建模，预测其下一个交互。以 BERT4Rec 为代表的 Transformer 基方法能捕获复杂的序列模式，但 Softmax 注意力带来的大量中间计算使其计算开销巨大。我们提出 Cotten4Rec：一种采用线性时间复杂度的余弦相似度注意力、并通过单一、优化的 CUDA 核实现的新型顺序推荐模型。通过最大限度压缩中间缓存与核启动开销，Cotten4Rec 在中等序列长度和中等词典规模的场景下，相较 BERT4Rec 及线性注意力基线 LinRec，显著降低了资源消耗。在三个基准数据集上的评估表明，Cotten4Rec 能在几乎不损失推荐准确度的情况下，大幅减少内存占用与运行时间，证明了其在计算资源受限的大型线上顺序推荐系统中的高效替代价值。"
    },
    {
        "title": "Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan",
        "summary": "Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint optimization of intra-tower and inter-tower tasks. However, we observe that joint optimization often leads to certain modalities dominating the training process, while other modalities are neglected. In addition, inconsistent training speeds across modalities can easily result in the one-epoch problem. To address these challenges, we propose a staged pretraining strategy, which guides the model to focus on specialized tasks at each stage, enabling it to effectively attend to and utilize multimodal features, and allowing flexible control over the training process at each stage to avoid the one-epoch problem. Furthermore, to better utilize the semantic IDs that compress high-dimensional multimodal embeddings, we design both generative and discriminative tasks to help the model understand the associations between SIDs, queries, and item features, thereby improving overall performance. Extensive experiments on large-scale real-world Meituan data demonstrate that our method achieves improvements of 3.80%, 2.64%, and 2.17% on R@5, R@10, and R@20, and 5.10%, 4.22%, and 2.09% on N@5, N@10, and N@20 compared to mainstream baselines. Online A/B testing on the Meituan platform shows that our approach achieves a 1.12% increase in revenue and a 1.02% increase in click-through rate, validating the effectiveness and superiority of our method in practical applications.",
        "entry_id": "http://arxiv.org/abs/2602.06654v1",
        "pub_date": "2026-02-06",
        "translated_summary": "多模态检索模型在外卖点餐等场景中日益关键，因其能够综合利用丰富的多模态特征，满足多样的用户需求，实现精准召回。主流方法通常采用“查询-商品”双塔结构，并在塔内和塔间同时优化多项任务。然而我们发现，联合优化易导致某些模态在训练中占据主导，其它模态则被忽视；此外，不同模态的训练速度不一致，很容易引发“单轮未学完”问题。为此，我们提出一种阶段性预训练策略：在每个阶段聚焦专门的子任务，引导模型充分关注并利用多模态特征；同时可在各阶段灵活控制训练进度，避免单轮完结。进一步地，为充分利用将高维多模态嵌入压缩得到的语义 ID（SID），我们设计了生成型与判别型两类任务，帮助模型理解 SID 与查询、商品特征之间的关联，进而提升整体性能。大规模真实美团数据实验表明，我们的方法在 R@5、R@10、R@20 指标上分别较主流基线提升 3.80%、2.64%、2.17%，在 N@5、N@10、N@20 指标上分别提升 5.10%、4.22%、2.09%。在线 A/B 测试显示，模型在美团平台使收入提升 1.12%，点击率提升 1.02%，充分证明了本方法在实际业务中的有效性与优越性。"
    },
    {
        "title": "R2LED: Equipping Retrieval and Refinement in Lifelong User Modeling with Semantic IDs for CTR Prediction",
        "summary": "Lifelong user modeling, which leverages users' long-term behavior sequences for CTR prediction, has been widely applied in personalized services. Existing methods generally adopted a two-stage \"retrieval-refinement\" strategy to balance effectiveness and efficiency. However, they still suffer from (i) noisy retrieval due to skewed data distribution and (ii) lack of semantic understanding in refinement. While semantic enhancement, e.g., LLMs modeling or semantic embeddings, offers potential solutions to these two challenges, these approaches face impractical inference costs or insufficient representation granularity. Obsorbing multi-granularity and lightness merits of semantic identity (SID), we propose a novel paradigm that equips retrieval and refinement in Lifelong User Modeling with SEmantic IDs (R2LED) to address these issues. First, we introduce a Multi-route Mixed Retrieval for the retrieval stage. On the one hand, it captures users' interests from various granularities by several parallel recall routes. On the other hand, a mixed retrieval mechanism is proposed to efficiently retrieve candidates from both collaborative and semantic views, reducing noise. Then, for refinement, we design a Bi-level Fusion Refinement, including a target-aware cross-attention for route-level fusion and a gate mechanism for SID-level fusion. It can bridge the gap between semantic and collaborative spaces, exerting the merits of SID. The comprehensive experimental results on two public datasets demonstrate the superiority of our method in both performance and efficiency. To facilitate the reproduction, we have released the code online https://github.com/abananbao/R2LED.",
        "entry_id": "http://arxiv.org/abs/2602.06622v1",
        "pub_date": "2026-02-06",
        "translated_summary": "终身用户建模通过利用用户长期行为序列进行点击率预估，已被广泛应用于个性化服务。现有方法普遍采用两阶段“检索—精修”策略以兼顾效果与效率。然而，这些方法仍面临两大挑战：（i）由于数据分布偏斜带来的检索噪声；（ii）在精修阶段缺乏语义理解。虽然诸如大模型建模或语义嵌入之类的语义增强方案有望同时解决这两个难题，但由于推理成本过高或表征粒度不足而难以实践。\n\n受“语义身份（Semantic ID, SID）”多粒度与轻量化的优势启发，我们提出一种全新的终身用户建模范式：在检索与精修阶段均配备语义身份的 R²LED（Retrieve-and-Refine Lifelong user modeling with Semantic IDs）。具体而言：  \n1) 在检索阶段，提出多路由混合检索（Multi-route Mixed Retrieval）。一方面，通过若干并行的召回路由，从多种粒度捕捉用户兴趣；另一方面，设计混合检索机制高效地从协同和语义两个视角召回候选，有效降低噪声。  \n2) 在精修阶段，设计双层融合精修（Bi-level Fusion Refinement）：包含面向目标的路由级交叉注意力以实现路由级融合，以及门控机制实现SID级融合，桥接语义空间与协同空间，充分发挥SID的优势。  \n\n在两个公开数据集上的综合实验表明，R²LED在效果和效率两方面均显著优于现有方法。为便于复现，代码已开源：https://github.com/abananbao/R2LED"
    },
    {
        "title": "TokenMixer-Large: Scaling Up Large Ranking Models in Industrial Recommenders",
        "summary": "In recent years, the study of scaling laws for large recommendation models has gradually gained attention. Works such as Wukong, HiFormer, and DHEN have attempted to increase the complexity of interaction structures in ranking models and validate scaling laws between performance and parameters/FLOPs by stacking multiple layers. However, their experimental scale remains relatively limited. Our previous work introduced the TokenMixer architecture, an efficient variant of the standard Transformer where the self-attention mechanism is replaced by a simple reshape operation, and the feed-forward network is adapted to a pertoken FFN. The effectiveness of this architecture was demonstrated in the ranking stage by the model presented in the RankMixer paper. However, this foundational TokenMixer architecture itself has several design limitations. In this paper, we propose TokenMixer-Large, which systematically addresses these core issues: sub-optimal residual design, insufficient gradient updates in deep models, incomplete MoE sparsification, and limited exploration of scalability. By leveraging a mixing-and-reverting operation, inter-layer residuals, the auxiliary loss and a novel Sparse-Pertoken MoE architecture, TokenMixer-Large successfully scales its parameters to 7-billion and 15-billion on online traffic and offline experiments, respectively. Currently deployed in multiple scenarios at ByteDance, TokenMixer -Large has achieved significant offline and online performance gains.",
        "entry_id": "http://arxiv.org/abs/2602.06563v1",
        "pub_date": "2026-02-06",
        "translated_summary": "近年来，针对大规模推荐模型 scaling law 的研究逐渐受到关注。诸如 Wukong、HiFormer 和 DHEN 等工作试图通过堆叠多层来提升排序模型的交互结构复杂度，并在性能与参数/FLOPs 之间验证 scaling law，但其实验规模仍相对有限。我们在先前工作中提出了 TokenMixer 架构——标准 Transformer 的一种高效变体，其中自注意力机制被简单的 reshape 操作替代，前馈网络则被改造为 per-token FFN。RankMixer 论文中的模型已在排序阶段验证了该架构的有效性。然而，该基础 TokenMixer 架构本身存在若干设计缺陷。本文提出 TokenMixer-Large，针对以下核心问题进行了系统性优化：残差设计欠佳、深层模型梯度更新不足、MoE 稀疏化不彻底，以及可扩展性探索有限。通过引入 mix-and-revert 操作、跨层残差、辅助损失以及全新的 Sparse-Pertoken MoE 架构，TokenMixer-Large 在在线和离线实验中分别将参数量扩展至 70 亿和 150 亿。目前，该模型已在抖音多项业务场景上线，带来了显著的离线及在线性能提升。"
    },
    {
        "title": "A methodology for analyzing financial needs hierarchy from social discussions using LLM",
        "summary": "This study examines the hierarchical structure of financial needs as articulated in social media discourse, employing generative AI techniques to analyze large-scale textual data. While human needs encompass a broad spectrum from fundamental survival to psychological fulfillment financial needs are particularly critical, influencing both individual well-being and day-to-day decision-making. Our research advances the understanding of financial behavior by utilizing large language models (LLMs) to extract and analyze expressions of financial needs from social media posts. We hypothesize that financial needs are organized hierarchically, progressing from short-term essentials to long-term aspirations, consistent with theoretical frameworks established in the behavioral sciences. Through computational analysis, we demonstrate the feasibility of identifying these needs and validate the presence of a hierarchical structure within them. In addition to confirming this structure, our findings provide novel insights into the content and themes of financial discussions online. By inferring underlying needs from naturally occurring language, this approach offers a scalable and data-driven alternative to conventional survey methodologies, enabling a more dynamic and nuanced understanding of financial behavior in real-world contexts.",
        "entry_id": "http://arxiv.org/abs/2602.06431v1",
        "pub_date": "2026-02-06",
        "translated_summary": "本研究借助生成式人工智能技术，通过大规模文本数据，考察社交媒体话语中所呈现的金融需求层级结构。人类需求从基本生存到心理满足涵盖甚广，而金融需求尤为关键，既影响个体福祉，也左右日常决策。本研究运用大型语言模型（LLM），从社交媒体帖子中提取并分析金融需求的表述，推进了对金融行为的理解。我们假设：金融需求呈层级排列，自短期必需品演进至长期愿景，并符合行为科学确立的理论框架。经计算分析，不仅验证了层级结构的可辨识性，还首次揭示了在线金融讨论的内容与主题。通过从自然语言中推断隐含需求，该方法为传统调查提供了可扩展、数据驱动的替代方案，使现实场景下的金融行为获得更动态而细致的刻画。"
    },
    {
        "title": "MuCo: Multi-turn Contrastive Learning for Multimodal Embedding Model",
        "summary": "Universal Multimodal embedding models built on Multimodal Large Language Models (MLLMs) have traditionally employed contrastive learning, which aligns representations of query-target pairs across different modalities. Yet, despite its empirical success, they are primarily built on a \"single-turn\" formulation where each query-target pair is treated as an independent data point. This paradigm leads to computational inefficiency when scaling, as it requires a separate forward pass for each pair and overlooks potential contextual relationships between multiple queries that can relate to the same context. In this work, we introduce Multi-Turn Contrastive Learning (MuCo), a dialogue-inspired framework that revisits this process. MuCo leverages the conversational nature of MLLMs to process multiple, related query-target pairs associated with a single image within a single forward pass. This allows us to extract a set of multiple query and target embeddings simultaneously, conditioned on a shared context representation, amplifying the effective batch size and overall training efficiency. Experiments exhibit MuCo with a newly curated 5M multimodal multi-turn dataset (M3T), which yields state-of-the-art retrieval performance on MMEB and M-BEIR benchmarks, while markedly enhancing both training efficiency and representation coherence across modalities. Code and M3T are available at https://github.com/naver-ai/muco",
        "entry_id": "http://arxiv.org/abs/2602.06393v1",
        "pub_date": "2026-02-06",
        "translated_summary": "传统上，建立在多模态大语言模型（MLLM）之上的通用多模态嵌入模型都采用对比学习，将不同模态的查询–目标对进行表征对齐。尽管该方法在经验上获得成功，但它们基本遵循“单轮”范式，把每对查询–目标当作独立样本处理。这种范式在大规模训练时计算效率低下：每对配对都需一次独立前向传播，且忽视了多个查询可能共享同一上下文所能带来的潜在关联。\n\n本文提出多轮对比学习（Multi-Turn Contrastive Learning，MuCo），一个受对话启发的重新设计框架。MuCo利用 MLLM 的对话特性，将一张图片对应的多个关联查询–目标对在一次前向传播内并行处理。我们因此能在共享上下文表征的条件下，同时抽取成组查询与目标嵌入，显著放大有效 batch size 并提升训练效率。\n\n我们在新整理的 500 万条多模态多轮数据（M3T）上进行的实验表明，MuCo 在 MMEB 和 M-BEIR 基准测试中均取得当前最佳检索性能，同时在训练效率和跨模态表征一致性方面均有显著提升。代码与 M3T 已开源：https://github.com/naver-ai/muco"
    },
    {
        "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
        "summary": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus. We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.",
        "entry_id": "http://arxiv.org/abs/2602.05975v2",
        "pub_date": "2026-02-05",
        "translated_summary": "深度研究智能体已成为应对复杂查询的有力系统；与此同时，基于大语言模型的检索器在遵循指令与推理方面也展现出强大能力。由此引发一个关键问题：这类 LLM 检索器能否有效服务于深度研究智能体流程？为探究此问题，我们推出 SAGE 基准，它包含来自四个科学领域、总计 1,200 个查询的科学文献检索任务，并配套 20 万篇论文的检索库。我们评估了六种深度研究智能体后，发现它们在面对需要深度推理的检索场景时普遍捉襟见肘。以 DR Tulu 为骨干，我们进一步比较了 BM25 与两类 LLM 检索器（ReasonIR 和 gte-Qwen2-7B-instruct）作为替代检索工具的效果。令人意外的是，现有智能体生成的子查询多为关键词导向，导致 BM25 反而比 LLM 检索器高出约 30% 的绝对性能。为此，我们提出了一种语料级的测试阶段缩放框架：利用大模型为已有文献补充元数据和关键词，降低现成检索器的负担；最终在短答案与开放式问题中分别带来 8% 和 2% 的性能提升。"
    },
    {
        "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
        "summary": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
        "entry_id": "http://arxiv.org/abs/2602.05735v2",
        "pub_date": "2026-02-05",
        "translated_summary": "在大型基础模型时代，表征向量（embedding）的质量已成为决定下游任务性能与整体系统能力的核心因素。然而，目前广泛使用的稠密向量往往维度极高，带来沉重的存储、内存和推理延迟代价。为应对这一问题，近期提出的对比稀疏表征（CSR）方法将稠密向量映射为高维但仅保留 k 个非零值的稀疏向量，与“套娃表征学习”（MRL）之类仍保持稠密压缩的做法形成鲜明对比。遗憾的是，CSR 在“极度稀疏”场景下性能严重退化：80% 以上的神经元长期失活，使其效率优势远未兑现。\n\n本文提出 CSRv2——一套系统化的训练框架，旨在使“极度稀疏”的表征向量成为可用选项。具体而言，CSRv2 通过渐进式 k-退火策略稳定稀疏学习，以监督对比损失提升表示质量，并利用全主干预训练保持端到端可适配性。实验表明，CSRv2 将失活神经元比例从 80% 降至 20%，在 k=2 时带来 14% 的准确率提升，仅用 2 个有效维度即可持平 CSR@k=8 以及 32 维的 MRL。在性能相当的前提下，CSRv2 推理速度比 MRL 快 7 倍；与稠密表征相比，文本任务的计算与内存效率最多提升 300 倍。\n\n跨文本与视觉的大规模实验结果显示，CSRv2 在不牺牲性能的前提下让极度稀疏表征变得实用：当 k=4 时，文本/视觉任务相较 CSR 提升 7%/4%；k=2 时进一步提升至 14%/6%。通过让“极端稀疏”真正落地，CSRv2 拓展了实时推理及边缘部署 AI 系统的设计空间，使得模型在保持高表征质量的同时，兼具前所未有的计算与存储效率。"
    },
    {
        "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
        "summary": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
        "entry_id": "http://arxiv.org/abs/2602.05512v2",
        "pub_date": "2026-02-05",
        "translated_summary": "大型语言模型 (LLMs) 在语言理解方面表现出色，但在知识密集型领域仍受幻觉、信息陈旧和缺乏可解释性等问题的限制。基于文本的检索增强生成 (RAG) 可将模型输出锚定于外部资料，却难以胜任多跳推理任务。相比之下，知识图谱 (KG) 支持精确、可追溯的查询，却对用户掌握查询语言提出了要求。本研究提出互动式框架：LLM 借助自然语言生成并解释 Cypher 图查询，用户以自然语言循环迭代地完善查询。在真实 KG 上的应用表明，该框架降低了复杂数据集的访问门槛，同时保持了事实准确性与语义严谨性，并揭示了模型性能在不同领域的差异。核心量化评估由包含 90 条查询的合成电影 KG 基准组成，面向多种 LLM 测量查询解释质量与错误检测能力；此外，还辅以两项小规模的真实场景实验，分别在 Hyena KG 与 MaRDI（数学研究数据倡议）KG 上生成查询。"
    },
    {
        "title": "SimGR: Escaping the Pitfalls of Generative Decoding in LLM-based Recommendation",
        "summary": "A core objective in recommender systems is to accurately model the distribution of user preferences over items to enable personalized recommendations. Recently, driven by the strong generative capabilities of large language models (LLMs), LLM-based generative recommendation has become increasingly popular. However, we observe that existing methods inevitably introduce systematic bias when estimating item-level preference distributions. Specifically, autoregressive generation suffers from incomplete coverage due to beam search pruning, while parallel generation distorts probabilities by assuming token independence. We attribute this issue to a fundamental modeling mismatch: these methods approximate item-level distributions via token-level generation, which inherently induces approximation errors. Through both theoretical analysis and empirical validation, we demonstrate that token-level generation cannot faithfully substitute item-level generation, leading to biased item distributions. To address this, we propose \\textbf{Sim}ply \\textbf{G}enerative \\textbf{R}ecommendation (\\textbf{SimGR}), a framework that directly models item-level preference distributions in a shared latent space and ranks items by similarity, thereby aligning the modeling objective with recommendation and mitigating distributional distortion. Extensive experiments across multiple datasets and LLM backbones show that SimGR consistently outperforms existing generative recommenders. Our code is available at https://anonymous.4open.science/r/SimGR-C408/",
        "entry_id": "http://arxiv.org/abs/2602.07847v1",
        "pub_date": "2026-02-08",
        "translated_summary": "推荐系统的核心目标之一，是准确建模用户对物品的偏好分布，从而实现个性化推荐。近年来，随着大型语言模型（LLM）强大的生成能力被广泛认可，基于LLM的生成式推荐方法日益流行。然而我们观察到，现有方法在估计物品级偏好分布时不可避免地引入了系统性偏差：自回归生成受限于束搜索剪枝而导致覆盖不完整；并行生成则因假设词元独立而扭曲真实概率。问题的根源在于一个根本性建模错位——这些方法用“词元级”生成去近似“物品级”分布，从而天生带来近似误差。通过理论分析与实验验证我们证明，仅用词元级生成无法忠实地替代物品级生成，终使物品分布产生偏差。为破解这一困局，我们提出 \\textbf{Simply Generative Recommendation}（\\textbf{SimGR}）：在共享潜空间中直接建模物品级的偏好分布，并通过相似度对物品进行排序，把建模目标与推荐任务精准对齐，从根源上缓解分布失真。在多数据集、多LLM骨干上的大量实验表明，SimGR一致显著优于现有生成式推荐方法。代码已开源：https://anonymous.4open.science/r/SimGR-C408/"
    },
    {
        "title": "SAGE: Scalable AI Governance & Evaluation",
        "summary": "Evaluating relevance in large-scale search systems is fundamentally constrained by the governance gap between nuanced, resource-constrained human oversight and the high-throughput requirements of production systems. While traditional approaches rely on engagement proxies or sparse manual review, these methods often fail to capture the full scope of high-impact relevance failures. We present \\textbf{SAGE} (Scalable AI Governance \\& Evaluation), a framework that operationalizes high-quality human product judgment as a scalable evaluation signal. At the core of SAGE is a bidirectional calibration loop where natural-language \\emph{Policy}, curated \\emph{Precedent}, and an \\emph{LLM Surrogate Judge} co-evolve. SAGE systematically resolves semantic ambiguities and misalignments, transforming subjective relevance judgment into an executable, multi-dimensional rubric with near human-level agreement. To bridge the gap between frontier model reasoning and industrial-scale inference, we apply teacher-student distillation to transfer high-fidelity judgments into compact student surrogates at \\textbf{92$\\times$} lower cost. Deployed within LinkedIn Search ecosystems, SAGE guided model iteration through simulation-driven development, distilling policy-aligned models for online serving and enabling rapid offline evaluation. In production, it powered policy oversight that measured ramped model variants and detected regressions invisible to engagement metrics. Collectively, these drove a \\textbf{0.25\\%} lift in LinkedIn daily active users.",
        "entry_id": "http://arxiv.org/abs/2602.07840v1",
        "pub_date": "2026-02-08",
        "translated_summary": "在超大规模搜索系统中，相关性评估的瓶颈在于“治理缺口”：一边是精细却受资源限制的人工监督，一边是产线系统的高吞吐需求。传统做法要么仰赖点击率代理，要么只做稀疏人工抽检，往往无法捕捉那些对体验影响最大但发生频率极低的失效模式。我们提出 **SAGE**（Scalable AI Governance & Evaluation），将高质量的产品判断（human product judgment）转化为可扩张的评估信号。其核心是一条双向校准循环，自然语言 **Policy**、精选的 **Precedent** 以及 **LLM Surrogate Judge** 三者互相演进：SAGE 系统化解所有语义歧义与价值错位，将主观相关性判断转化为可执行的多维度评分标准，并维持接近人类一致性。\n\n为了把前沿模型的推理能力部署到工业级推理规模，SAGE 采用教师—学生蒸馏，把高保真判决压缩进微型的学生代理上，**成本仅为原来的 1/92**。在 LinkedIn 搜索体系中，SAGE 以仿真驱动模型迭代：线下来回验证策略对齐，线上直接蒸馏出合规轻量模型用于实际流量。上线后，它承担了策略层面的监督任务——衡量灰度模型版本，检出对点击指标毫无信号的隐性回归。整体提升 **LinkedIn 日活用户 0.25%**。"
    },
    {
        "title": "Generative Reasoning Re-ranker",
        "summary": "Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2's effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.",
        "entry_id": "http://arxiv.org/abs/2602.07774v1",
        "pub_date": "2026-02-08",
        "translated_summary": "最新研究日益关注将大语言模型（LLM）作为推荐系统的新范式，原因是其具备可扩展性与全球知识。然而，已有工作存在三大关键局限：（1）现有努力集中在检索与排序阶段，而对重排序阶段关注不足，而重排序恰恰是精调最终推荐结果的关键；（2）LLM 通常仅在零样本或监督微调模式下使用，其推理能力——尤其是通过强化学习（RL）与高质量推理数据所强化的推理能力——仍被严重低估；（3）物品常用无语义 ID 表示，在拥有数十亿标识符的工业系统中带来巨大可扩展性挑战。\n\n为填补这些空白，我们提出生成式推理重排序器（GR2），一个端到端的框架，专为重排序设计的三阶段训练管道。首先，通过可达≥99%唯一性的 Tokenizer 将无语义 ID 编码成语义 ID，再对预训练 LLM 进行中期训练。接着，依托精心设计的提示与拒绝采样，让规模更大、能力更强的 LLM 产出高质量推理轨迹，用于监督微调，赋予模型基础推理技能。最后，我们采用解耦剪辑与动态采样策略优化（DAPO），利用专为重排序设计的可验证奖励，实现可扩展的强化学习监督。\n\n在两个真实世界数据集上的实验表明，GR2 效果显著：在 Recall@5 上较当前最优的 OneRec-Think 提升 2.4%，在 NDCG@5 上提升 1.3%。消融实验证实，高质量推理轨迹可在所有指标上带来显著增益。我们进一步发现，RL 奖励设计在重排序中至关重要：LLM 往往通过维持物品顺序来利用奖励漏洞；为此，我们提出条件可验证奖励，以缓解此行为并优化重排序性能。"
    },
    {
        "title": "SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents",
        "summary": "Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.",
        "entry_id": "http://arxiv.org/abs/2602.07773v1",
        "pub_date": "2026-02-08",
        "translated_summary": "近期，基于大型推理模型（LRM）的深度搜索智能体通过“计划-行动-收集证据”的迭代循环在复杂问答任务中表现优异，这一能力被统称为“融合搜索的推理”。然而，主流方法仅使用基于最终结果的监督进行训练，忽视了中间思维过程与行动的质量。为此，我们提出 SRR-Judge——一个用于可靠地逐步评估推理与搜索动作的框架。将该框架嵌入经过改进的 ReAct 风格“打分-再精炼”工作流后，SRR-Judge 能够为融合搜索的推理提供细粒度指导，并高效地生成用于后训练的高质量标注数据。借助 SRR-Judge 生成的标注轨迹，我们对基础智能体执行迭代拒绝采样微调，显著提升其深度搜索能力。实验表明，SRR-Judge 在步级评估可靠性上优于更大规模的模型（如 DeepSeek-V3.1），其打分与最终答案正确性高度相关。进一步地，将策略网络与 SRR-Judge 标注轨迹对齐后，我们在多项挑战性深度搜索基准上取得了超过 10% 的绝对 pass@1 提升。"
    },
    {
        "title": "HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation",
        "summary": "Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail to preserve, causing semantically distant documents to appear spuriously similar and increasing hallucination risk. To address these limitations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean embeddings into hyperbolic space. To prevent representational collapse during sequence aggregation, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Euclidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean baselines in context relevance and answer relevance using substantially smaller models than current state-of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separation, with over 20% radial increase from general to specific concepts, a property absent in Euclidean embeddings, underscoring the critical role of geometric inductive bias in faithful RAG systems.",
        "entry_id": "http://arxiv.org/abs/2602.07739v1",
        "pub_date": "2026-02-08",
        "translated_summary": "嵌入几何从根本上决定了检索质量，然而用于检索增强生成（RAG）的稠密检索器仍被局限于欧氏空间。自然语言从广泛主题到具体实体呈现明显的层级结构，但欧氏嵌入无法保持这种结构，导致语义上相距甚远的文档被错误地判定为相似，从而加剧幻觉风险。为此，我们提出“双曲稠密检索”，在洛伦兹模型的双曲空间中构建两种变体：HyTE-FH，完全在双曲空间内运算的 Transformer；HyTE-H，将预训练欧氏嵌入投影到双曲空间的混合架构。为防止序列聚合过程中的表征坍缩，我们设计了面向外部的爱因斯坦中点（Outward Einstein Midpoint）几何感知池化算子，理论证明其可严格保持层级结构。在 MTEB 基准上，HyTE-FH 显著超越对等欧氏基线；在 RAGBench 上，HyTE-H 使用远小于当前最佳检索器的模型体积，在上下文相关性和答案相关性指标上带来最高 29% 的提升。进一步分析发现，双曲表示通过范数区分文档粒度——从通用到具体概念的径向距离增幅超过 20%，而欧氏嵌入无此特征，从而凸显几何归纳偏置对忠实 RAG 系统的关键作用。"
    },
    {
        "title": "EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge",
        "summary": "Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.",
        "entry_id": "http://arxiv.org/abs/2602.07695v1",
        "pub_date": "2026-02-07",
        "translated_summary": "需求预测是电商运营的核心环节，直接影响库存规划与履约调度。然而，在闪购、大促或突发的政策调控等高影响力时段，需求模式会剧烈且不可预期地波动，现有预测系统在这些场景下往往失效。  \n本文提出 EventCast——一种将未来事件知识整合进时间序列预测的模块化预测框架。与以往忽略干预事件、或直接用大语言模型（LLM）做数值预测的路线不同，EventCast 只把 LLM 用于事件驱动的推理。来自运营数据库的非结构化业务数据（涵盖促销活动、节假日安排、商家激励等）首先由 LLM 处理，利用世界知识理解文化语义及未知事件组合，生成可解释的文本摘要；然后这些摘要与历史需求特征一起，输入到双塔架构中融合，实现既精准又可扩展、且具备可解释性的预测。  \n在横跨 4 个国家、160 个区域的 10 个月真实电商场景中部署测试，EventCast 相比无事件知识的版本在 MAE 与 MSE 上分别最高提升 86.9% 与 97.7%；在高影响事件期间，相比业界最佳基线进一步将 MAE 降低 57.0%，MSE 降低 83.3%。自 2025 年 3 月起，EventCast 已上线至实际工业管线，为动态电商环境中的运营决策提供实用可靠的解决方案。"
    },
    {
        "title": "Assessing the impact of Open Research Information Infrastructures using NLP driven full-text Scientometrics: A case study of the LXCat open-access platform",
        "summary": "Open research information (ORI) play a central role in shaping how scientific knowledge is produced, disseminated, validated, and reused across the research lifecycle. While the visibility of such ORI infrastructures is often assessed through citation-based metrics, in this study, we present a full-text, natural language processing (NLP) driven scientometric framework to systematically quantify the impact of ORI infrastructures beyond citation counts, using the LXCat platform for low temperature plasma (LTP) research as a representative case study. The modeling of LTPs and interpretation of LTP experiments rely heavily on accurate data, much of which is hosted on LXCat, a community-driven, open-access platform central to the LTP research ecosystem. To investigate the scholarly impact of the LXCat platform over the past decade, we analyzed a curated corpus of full-text research articles citing three foundational LXCat publications. We present a comprehensive pipeline that integrates chemical entity recognition, dataset and solver mention extraction, affiliation based geographic mapping and topic modeling to extract fine-grained patterns of data usage that reflect implicit research priorities, data practices, differential reliance on specific databases, evolving modes of data reuse and coupling within scientific workflows, and thematic evolution. Importantly, our proposed methodology is domain-agnostic and transferable to other ORI contexts, and highlights the utility of NLP in quantifying the role of scientific data infrastructures and offers a data-driven reflection on how open-access platforms like LXCat contribute to shaping research directions. This work presents a scalable scientometric framework that has the potential to support evidence based evaluation of ORI platforms and to inform infrastructure design, governance, sustainability, and policy for future development.",
        "entry_id": "http://arxiv.org/abs/2602.07664v1",
        "pub_date": "2026-02-07",
        "translated_summary": "开放研究信息（ORI）在整个科研生命周期中对科学知识的生产、传播、验证和复用发挥着核心作用。虽然目前往往借助引文指标来评价这类 ORI 基础设施的可见度，但本研究在传统的引文计数之外，提出了一套基于全文自然语言处理（NLP）驱动的科学计量框架，并选取低温等离子体（LTP）领域的 LXCat 平台作为典型案例，对其学术影响力进行系统量化。LTP 的建模及其实验数据的解读高度依赖精准数据，而 LXCat 正是一个由社区共建、开放获取、承载这些关键数据的核心平台。\n\n为了探析 LXCat 在近十年内的学术影响，我们构建了一个精选的全文学术文献语料库，涵盖引用三篇 LXCat 奠基性出版物的研究文章。在此基础上，我们提出一条完整且可复制的处理管线：整合化学实体识别、数据集与求解器提及抓取、基于附属机构的地理位置映射，以及主题建模等技术，用以抽取细粒度的数据使用模式。这些模式能够映射出隐含的研究优先级、数据惯例、对特定数据库的差异性依赖、数据复用方式的演进，以及科学工作流程中的耦合关系与主题演化。\n\n值得注意的是，该方法论完全不受学科限制，可迁移至其他 ORI 场景；再次凸显了 NLP 在量化科学数据基础设施作用中的实用价值，并为开放获取平台（如 LXCat）如何塑造科研方向提供了数据驱动的思考。本研究贡献了一套可扩展的科学计量框架，不仅有望为 ORI 平台的循证评估提供支撑，也为基础设施的设计、治理、可持续发展及未来政策制定提供参考。"
    },
    {
        "title": "MSN: A Memory-based Sparse Activation Scaling Framework for Large-scale Industrial Recommendation",
        "summary": "Scaling deep learning recommendation models is an effective way to improve model expressiveness. Existing approaches often incur substantial computational overhead, making them difficult to deploy in large-scale industrial systems under strict latency constraints. Recent sparse activation scaling methods, such as Sparse Mixture-of-Experts, reduce computation by activating only a subset of parameters, but still suffer from high memory access costs and limited personalization capacity due to the large size and small number of experts. To address these challenges, we propose MSN, a memory-based sparse activation scaling framework for recommendation models. MSN dynamically retrieves personalized representations from a large parameterized memory and integrates them into downstream feature interaction modules via a memory gating mechanism, enabling fine-grained personalization with low computational overhead. To enable further expansion of the memory capacity while keeping both computational and memory access costs under control, MSN adopts a Product-Key Memory (PKM) mechanism, which factorizes the memory retrieval complexity from linear time to sub-linear complexity. In addition, normalization and over-parameterization techniques are introduced to maintain balanced memory utilization and prevent memory retrieval collapse. We further design customized Sparse-Gather operator and adopt the AirTopK operator to improve training and inference efficiency in industrial settings. Extensive experiments demonstrate that MSN consistently improves recommendation performance while maintaining high efficiency. Moreover, MSN has been successfully deployed in the Douyin Search Ranking System, achieving significant gains over deployed state-of-the-art models in both offline evaluation metrics and large-scale online A/B test.",
        "entry_id": "http://arxiv.org/abs/2602.07526v1",
        "pub_date": "2026-02-07",
        "translated_summary": "扩展深度学习推荐模型的规模是提升模型表达能力的有效手段。然而，现有方法通常伴随巨大的计算开销，在严苛的延迟限制下难以在大规模工业系统中落地。近期的稀疏激活扩展技术（如 Sparse Mixture-of-Experts）通过仅激活部分参数来削减计算，但仍因专家体量庞大、数量稀少，导致高昂的内存访问成本与有限的个性化能力。为此，本文提出 MSN——一种面向推荐模型的基于存储器的稀疏激活扩展框架。MSN 通过存储器门控机制，动态地从大规模参数化存储中检索个性化表征，并无缝集成至下游特征交互模块，实现细粒度个性化且保持极低计算开销。为了在继续扩张存储容量而不增计耗和访存开销的前提下达到极致效果，MSN 引入 Product-Key Memory (PKM) 机制，将存储检索复杂度从线性降至亚线性。此外，通过归一化与过参数化技术共同保障存储利用均衡，避免检索崩溃。我们自研 Sparse-Gather 算子并采用 AirTopK 算子，进一步强化工业环境下的训练与推理效率。大量实验表明，MSN 在提高推荐性能的同时维持高效运行；目前已在抖音搜索排序系统全量上线，离线指标与大规模在线 A/B 测试均显著优于已服役的 SOTA 模型。"
    },
    {
        "title": "IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory",
        "summary": "Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.",
        "entry_id": "http://arxiv.org/abs/2602.07525v1",
        "pub_date": "2026-02-07",
        "translated_summary": "检索增强生成（RAG）为大语言模型（LLM）注入了可靠的知识记忆。为强化跨文本关联，近期研究将图与超图引入 RAG，以对双元和多元实体关系进行结构化链接。然而，其记忆组织与检索需求存在错位，导致检索过程代价高昂且割裂。为解决这一局限，我们提出 IGMiRAG——一种受人类直觉式推理启发的框架。该框架构建层级异构超图以对齐多粒度知识，并融入演绎路径，模拟真实的记忆结构。  \n在查询阶段，IGMiRAG 通过问题解析器提炼直觉式策略，动态控制挖掘深度与记忆窗口，并使用双焦点检索将瞬间记忆激活为锚点。借此，框架像人类直觉一样动态引导检索资源的分配。进一步地，我们设计了一种双向扩散算法，沿演绎路径导航、挖掘深层记忆，模拟人类推理过程。  \n大量实验表明，IGMiRAG 在整体指标上以 4.8% EM 和 5.0% F1 优于当前最佳基线；其 Token 开销随任务复杂度自适应（平均 6.3 k+，最低 3.0 k+）。本研究提出了一种兼顾效率与效果的 RAG 新范式。"
    },
    {
        "title": "MDL: A Unified Multi-Distribution Learner in Large-scale Industrial Recommendation through Tokenization",
        "summary": "Industrial recommender systems increasingly adopt multi-scenario learning (MSL) and multi-task learning (MTL) to handle diverse user interactions and contexts, but existing approaches suffer from two critical drawbacks: (1) underutilization of large-scale model parameters due to limited interaction with complex feature modules, and (2) difficulty in jointly modeling scenario and task information in a unified framework. To address these challenges, we propose a unified \\textbf{M}ulti-\\textbf{D}istribution \\textbf{L}earning (MDL) framework, inspired by the \"prompting\" paradigm in large language models (LLMs). MDL treats scenario and task information as specialized tokens rather than auxiliary inputs or gating signals. Specifically, we introduce a unified information tokenization module that transforms features, scenarios, and tasks into a unified tokenized format. To facilitate deep interaction, we design three synergistic mechanisms: (1) feature token self-attention for rich feature interactions, (2) domain-feature attention for scenario/task-adaptive feature activation, and (3) domain-fused aggregation for joint distribution prediction. By stacking these interactions, MDL enables scenario and task information to \"prompt\" and activate the model's vast parameter space in a bottom-up, layer-wise manner. Extensive experiments on real-world industrial datasets demonstrate that MDL significantly outperforms state-of-the-art MSL and MTL baselines. Online A/B testing on Douyin Search platform over one month yields +0.0626\\% improvement in LT30 and -0.3267\\% reduction in change query rate. MDL has been fully deployed in production, serving hundreds of millions of users daily.",
        "entry_id": "http://arxiv.org/abs/2602.07520v1",
        "pub_date": "2026-02-07",
        "translated_summary": "工业界推荐系统日益依赖多场景学习（MSL）与多任务学习（MTL）来应对多样化的用户交互与上下文环境，但现有方法存在两大缺陷：（1）由于与复杂特征模块交互有限，超大规模模型参数使用不足；（2）难以在统一框架内联合建模场景和任务信息。为此，我们受大语言模型（LLM）中“提示（prompting）”范式的启发，提出统一的多分布学习（Multi-Distribution Learning，MDL）框架。MDL将场景与任务信息视为专用令牌，而不再是辅助输入或门控信号。具体地，我们引入统一的信息分词模块，将特征、场景和任务转换为统一的令牌化格式。为促进深度交互，我们设计三类协同机制：（1）特征令牌自注意力，用于充分刻画特征交互；（2）域—特征注意力，用于依据场景/任务自适应激活特征；（3）域融合聚合，用于联合分布预测。通过将这些交互层叠堆叠，MDL使场景与任务信息能够“提示”并以自下而上、逐层的方式激活模型庞大的参数空间。在真实工业数据集上的大量实验表明，MDL显著优于现有 MSL 和 MTL 最强基线。抖音搜索平台为期一个月的在线 A/B 测试得到：LT30 提升 0.0626%，换 query 率降低 0.3267%。MDL 已全面上线，每日为数亿用户提供稳定服务。"
    },
    {
        "title": "Echoes in the Loop: Diagnosing Risks in LLM-Powered Recommender Systems under Feedback Loops",
        "summary": "Large language models (LLMs) are increasingly embedded into recommender systems, where they operate across multiple functional roles such as data augmentation, profiling, and decision making. While prior work emphasizes recommendation performance, the systemic risks of LLMs, such as bias and hallucination, and their propagation through feedback loops remain largely unexplored. In this paper, we propose a role-aware, phase-wise diagnostic framework that traces how these risks emerge, manifest in ranking outcomes, and accumulate over repeated recommendation cycles. We formalize a controlled feedback-loop pipeline that simulates long-term interaction dynamics and enables empirical measurement of risks at the LLM-generated content, ranking, and ecosystem levels. Experiments on widely used benchmarks demonstrate that LLM-based components can amplify popularity bias, introduce spurious signals through hallucination, and lead to polarized and self-reinforcing exposure patterns over time. We plan to release our framework as an open-source toolkit to facilitate systematic risk analysis across diverse LLM-powered recommender systems.",
        "entry_id": "http://arxiv.org/abs/2602.07442v1",
        "pub_date": "2026-02-07",
        "translated_summary": "大语言模型（LLM）越来越多地被嵌入推荐系统，并同时在多个职能角色中发挥作用，例如数据增强、用户画像构建与决策制定。已有研究主要关注推荐性能，而LLM存在的系统性风险（如偏见与幻觉）以及这些风险在反馈回路中的传播几乎未被探讨。本文提出一种面向角色、按阶段划分的诊断框架，可追踪风险的产生方式、在排序结果中的表现，以及如何在反复迭代的推荐循环中累积。我们构建了一条受控的反馈回路管道，用于模拟长期交互动态，并在LLM生成内容、排序结果以及整个生态系统三个层面，实证量化风险。在广泛使用的基准数据集上的实验表明，基于 LLM 的组件会放大流行度偏差，通过幻觉注入虚假信号，并随时间推移造成极化且自我强化的曝光模式。我们拟将该框架开源，促进对各种 LLM 驱动推荐系统的系统性风险分析。"
    },
    {
        "title": "ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations",
        "summary": "Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.",
        "entry_id": "http://arxiv.org/abs/2602.07361v1",
        "pub_date": "2026-02-07",
        "translated_summary": "摘要  \n在监管文档上开展问答（QA）极具挑战性，原因在于必须对具有法律联动关系的文本进行多跳推理；这种需求在医疗监管领域尤为突出，因为相关法规呈层级结构，且经常以修订或交叉引用的形式变动。尽管检索增强和基于图的 QA 模型近来取得进展，该场景下的系统性评估仍然稀缺，特别是对于越南语等低资源语言，原因在于缺乏明确支持医疗监管多跳推理的基准数据集。本文推出越南医疗法规多跳推理数据集（ViHERMES），这是首个针对越南语医疗监管文档的多跳 QA 基准。ViHERMES 包含高质量问答对，要求跨越多部法规进行推理，并覆盖修订追溯、跨文档对比、程序性综合等复杂依赖模式。为构建该数据集，我们设计了受控的多跳 QA 生成流程：先用语义聚类和受图启发的数据挖掘定位可推理素材，随后基于大语言模型生成问答，并为每条答案附加结构化证据及逐步推理注释。进一步，我们提出图感知检索框架，将法律单元内部的正式法理关系显式建模成图，并以合法且连贯的前提扩展上下文。实验表明，ViHERMES 为多跳监管 QA 系统提供了极具挑战的评估基准；所提出的图感知方法全面优于现有强检索基线。ViHERMES 数据集与系统代码已开源：https://github.com/ura-hcmut/ViHERMES"
    },
    {
        "title": "High Fidelity Textual User Representation over Heterogeneous Sources via Reinforcement Learning",
        "summary": "Effective personalization on large-scale job platforms requires modeling members based on heterogeneous textual sources, including profiles, professional data, and search activity logs. As recommender systems increasingly adopt Large Language Models (LLMs), creating unified, interpretable, and concise representations from heterogeneous sources becomes critical, especially for latency-sensitive online environments. In this work, we propose a novel Reinforcement Learning (RL) framework to synthesize a unified textual representation for each member. Our approach leverages implicit user engagement signals (e.g., clicks, applies) as the primary reward to distill salient information. Additionally, the framework is complemented by rule-based rewards that enforce formatting and length constraints. Extensive offline experiments across multiple LinkedIn products, one of the world's largest job platforms, demonstrate significant improvements in key downstream business metrics. This work provides a practical, labeling-free, and scalable solution for constructing interpretable user representations that are directly compatible with LLM-based systems.",
        "entry_id": "http://arxiv.org/abs/2602.07333v1",
        "pub_date": "2026-02-07",
        "translated_summary": "在大型求职平台上实现高效个性化，需要基于多种异构文本数据源（如个人档案、职业信息及搜索行为日志）对用户进行建模。随着推荐系统越来越多地采用大语言模型（LLMs），从异构数据中提取统一、可解释且简洁的文本表示变得尤为关键，尤其是在对延迟极其敏感的在线环境中。本文提出一种新颖的强化学习（RL）框架，将异构文本合成为针对每位用户的统一文本表示。该方法以隐式用户互动信号（点击、应聘等）为主要奖励，提炼关键信息；同时引入基于规则的奖励，强制满足格式与长度约束。在 LinkedIn 多个产品上的大规模离线实验表明，该方法在关键下游业务指标上取得了显著提升。本研究为无需人工标注、可扩展且可直接应用于大语言模型的系统，提供了一种实用的可解释用户表示构建方案。"
    },
    {
        "title": "Semantic Search At LinkedIn",
        "summary": "Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge, embedding-based retrieval, and a compact Small Language Model trained via multi-teacher distillation to jointly optimize relevance and engagement. A prefill-oriented inference architecture co-designed with model pruning, context compression, and text-embedding hybrid interactions boosts ranking throughput by over 75x under a fixed latency constraint while preserving near-teacher-level NDCG, enabling one of the first production LLM-based ranking systems with efficiency comparable to traditional approaches and delivering significant gains in quality and user engagement.",
        "entry_id": "http://arxiv.org/abs/2602.07309v1",
        "pub_date": "2026-02-07",
        "translated_summary": "基于大语言模型（LLM）的语义搜索突破关键词匹配，直接以“含义”召回内容，但大规模落地必须在推理效率上实现飞跃。为此，我们面向领英 AI 职位搜索与 AI 人才搜索，构建了一套 LLM 语义搜索框架：该框架由 LLM 相关性判别器、嵌入检索系统以及一个经过多教师蒸馏的微缩小型语言模型（Small Language Model, SoLM）共同组成，在训练阶段将相关性和用户参与度作为联合优化目标。我们同时设计了一种面向预填充的推理架构，将模型剪枝、上下文压缩和文本-嵌入混合交互协同整合；在固定延迟约束下，排序吞吐率提升超过 75 倍，NDCG 仍与教师模型几乎持平。该系统成为首批在生产环境落地的 LLM 级排序系统，其推理效率可与传统方法比肩，并在相关性和用户互动两个维度实现了显著提高。"
    },
    {
        "title": "LIT-GRAPH: Evaluating Deep vs. Shallow Graph Embeddings for High-Quality Text Recommendation in Domain-Specific Knowledge Graphs",
        "summary": "This study presents LIT-GRAPH (Literature Graph for Recommendation and Pedagogical Heuristics), a novel knowledge graph-based recommendation system designed to scaffold high school English teachers in selecting diverse, pedagogically aligned instructional literature. The system is built upon an ontology for English literature, addressing the challenge of curriculum stagnation, where we compare four graph embedding paradigms: DeepWalk, Biased Random Walk (BRW), Hybrid (concatenated DeepWalk and BRW vectors), and the deep model Relational Graph Convolutional Network (R-GCN). Results reveal a critical divergence: while shallow models excelled in structural link prediction, R-GCN dominated semantic ranking. By leveraging relation-specific message passing, the deep model prioritizes pedagogical relevance over raw connectivity, resulting in superior, high-quality, domain-specific recommendations.",
        "entry_id": "http://arxiv.org/abs/2602.07307v1",
        "pub_date": "2026-02-07",
        "translated_summary": "本研究提出 LIT-GRAPH（Literature Graph for Recommendation and Pedagogical Heuristics），一种基于知识图谱的新型推荐系统，旨在帮助高中英语教师选择既具多样性又契合教学需求的作品，从而解决课程僵化难题。系统依托英语文学本体构建，并比较了四种图嵌入范式：DeepWalk、偏向随机游走（BRW）、混合法（DeepWalk 与 BRW 向量的串联）以及深度模型关系图卷积网络（R-GCN）。研究结果呈现关键差异：浅层模型在结构性链路预测上表现优异，而 R-GCN 则在语义排序中占据绝对优势。凭借关系感知的消息传递机制，深度模型能够把“教学适切性”置于原始连接之上，生成更优质、领域专精的推荐。"
    },
    {
        "title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation",
        "summary": "Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.",
        "entry_id": "http://arxiv.org/abs/2602.07298v1",
        "pub_date": "2026-02-07",
        "translated_summary": "大型语言模型（LLM）为推荐系统带来了诱人前景，但其发展受阻于缺乏可预测的扩展律，而可预测的扩展律对于指导研究与优化资源配置至关重要。我们推测，这一困境可能源于先前连续预训练工作所使用的原始用户交互数据中固有的噪声、偏差与不完整性。为此，本文提出了一种新颖的分层框架，用以生成高质量合成数据：为LLM精心打造一套经策划的教学课程，从而回避上述弊端。我们通过直接而有力的实验证明课程的价值：使用经典序列模型时，在我们的合成数据上训练的模型在下游排序任务中的表现显著优于在真实数据上训练的模型（例如，SasRec在recall@100上提高约130%），说明合成数据在可泛化的用户偏好模式学习上更具优势。在此基础上，我们首次实证发现了一条稳健的幂律扩展定律——当LLM基于我们高质量、专为推荐设计的合成数据进行持续预训练时，其性能的改善呈现可预期、可重复的困惑度下降。我们的实验揭示了多种合成数据模态间的一致性缩放规律。这些发现为在推荐领域内可靠地扩展LLM能力奠定了方法学基础，使研究重心从“弥补数据缺陷”转向了“充分利用高质量、结构化信息”。"
    },
    {
        "title": "Progressive Searching for Retrieval in RAG",
        "summary": "Retrieval Augmented Generation (RAG) is a promising technique for mitigating two key limitations of large language models (LLMs): outdated information and hallucinations. RAG system stores documents as embedding vectors in a database. Given a query, search is executed to find the most related documents. Then, the topmost matching documents are inserted into LLMs' prompt to generate a response. Efficient and accurate searching is critical for RAG to get relevant information. We propose a cost-effective searching algorithm for retrieval process. Our progressive searching algorithm incrementally refines the candidate set through a hierarchy of searches, starting from low-dimensional embeddings and progressing into a higher, target-dimensionality. This multi-stage approach reduces retrieval time while preserving the desired accuracy. Our findings demonstrate that progressive search in RAG systems achieves a balance between dimensionality, speed, and accuracy, enabling scalable and high-performance retrieval even for large databases.",
        "entry_id": "http://arxiv.org/abs/2602.07297v1",
        "pub_date": "2026-02-07",
        "translated_summary": "检索增强生成（RAG）是一种前景广阔的技术，旨在缓解大语言模型（LLM）的两大核心问题：信息过时与幻觉现象。RAG 系统将文档以嵌入向量形式保存在数据库中。给定查询后，系统先搜索最相关的文档，再将匹配度最高的若干文档嵌入 LLM 的提示，从而生成回答。因此，高效且精准的搜索对 RAG 获取相关信息至关重要。本文提出一种经济高效的检索算法。该渐进式搜索算法以层次化方式逐步缩小候选集，先从低维嵌入开始，逐步过渡到更高、最终的目标维度。通过这一多阶段流程，系统在保持所需精度的同时显著缩短检索时间。实验结果表明，在 RAG 系统中采用渐进搜索能够有效权衡维度、速度与精度，即使在超大规模数据库中也能实现可扩展的高性能检索。"
    },
    {
        "title": "Sequences as Nodes for Contrastive Multimodal Graph Recommendation",
        "summary": "To tackle cold-start and data sparsity issues in recommender systems, numerous multimodal, sequential, and contrastive techniques have been proposed. While these augmentations can boost recommendation performance, they tend to add noise and disrupt useful semantics. To address this, we propose MuSICRec (Multimodal Sequence-Item Contrastive Recommender), a multi-view graph-based recommender that combines collaborative, sequential, and multimodal signals. We build a sequence-item (SI) view by attention pooling over the user's interacted items to form sequence nodes. We propagate over the SI graph, obtaining a second view organically as an alternative to artificial data augmentation, while simultaneously injecting sequential context signals. Additionally, to mitigate modality noise and align the multimodal information, the contribution of text and visual features is modulated according to an ID-guided gate.\n  We evaluate under a strict leave-two-out split against a broad range of sequential, multimodal, and contrastive baselines. On the Amazon Baby, Sports, and Electronics datasets, MuSICRec outperforms state-of-the-art baselines across all model types. We observe the largest gains for short-history users, mitigating sparsity and cold-start challenges. Our code is available at https://anonymous.4open.science/r/MuSICRec-3CEE/ and will be made publicly available.",
        "entry_id": "http://arxiv.org/abs/2602.07208v1",
        "pub_date": "2026-02-06",
        "translated_summary": "为缓解推荐系统中的冷启动与数据稀疏问题，研究者们已提出诸多多模态、序列化和对比学习技术。尽管这些增强手段能提升推荐性能，但也会带来噪声、破坏有效语义。为此，我们提出 MuSICRec（Multimodal Sequence-Item Contrastive Recommender）。该方法利用多视角图网络，将协同、序列和多模态信号有机融合。其思想是将用户已交互商品通过注意力池化生成序列节点，构建“序列-商品”（SI）视角；在此基础上，我们在 SI 图上进行信息传播，自然而然地得到第二个视角，从而避免人工数据增强，并同步注入序列上下文信号。此外，为抑制多模态噪声并实现模态对齐，我们在 ID 引导下设计门控机制，动态调节文本与视觉特征的贡献程度。  \n\n我们在严格的 leave-two-out 设定下，与大量序列、多模态及对比基线模型进行了对比实验。在 Amazon Baby、Sports 和 Electronics 数据集上，MuSICRec 全面超越各类当前最优基线，且在历史行为较短的用户上提升最为显著，有力缓解了数据稀疏与冷启动问题。  \n代码与模型已开源：https://anonymous.4open.science/r/MuSICRec-3CEE/，后续将正式对外发布。"
    },
    {
        "title": "Multimodal Enhancement of Sequential Recommendation",
        "summary": "We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted text and visual features. A frequency-based self-attention module additionally captures the short- and long-term user preferences. Across multiple Amazon datasets, MuSTRec demonstrates superior performance (up to 33.5% improvement) over multimodal and sequential state-of-the-art baselines. Finally, we detail some interesting facets of this new recommendation paradigm. These include the need for a new data partitioning regime, and a demonstration of how integrating user embeddings into sequential recommendation leads to drastically increased short-term metrics (up to 200% improvement) on smaller datasets. Our code is availabe at https://anonymous.4open.science/r/MuSTRec-D32B/ and will be made publicly available.",
        "entry_id": "http://arxiv.org/abs/2602.07207v1",
        "pub_date": "2026-02-06",
        "translated_summary": "我们提出一种新的推荐框架——MuSTRec（Multimodal and Sequential Transformer-based Recommendation），将多模态与序列化推荐范式统一于一体。MuSTRec 通过从文本和视觉特征中构建项目-项目图，同时捕获跨项目的相似性与协同过滤信号；一个基于频率的自注意模块进一步挖掘用户的短期与长期偏好。在多个 Amazon 数据集上的实验表明，MuSTRec 相比多模态与序列化两类最新基线最多可提升 33.5%。最后，我们对这一新范式下的若干关键现象进行了剖析：一方面，需要一种全新的数据划分策略；另一方面，通过将用户嵌入整合到序列化推荐中，在小数据集上的短期指标可大幅提升（最高达 200%）。代码已开源于 https://anonymous.4open.science/r/MuSTRec-D32B/，并将公开发布。"
    },
    {
        "title": "Reasoning-Augmented Representations for Multimodal Retrieval",
        "summary": "Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.",
        "entry_id": "http://arxiv.org/abs/2602.07125v1",
        "pub_date": "2026-02-06",
        "translated_summary": "通用多模态检索（UMR）旨在实现文本与视觉内容之间的任意到任意搜索，然而现有嵌入模型在面对潜在推理需求的查询时（例如消歧指代或组合约束匹配）表现极其脆弱。我们认为，这种脆弱性往往源于数据本身：当图像包含“沉默”证据而查询又将关键语义隐式表达时，单次嵌入过程必须同时完成推理与压缩，导致模型倾向于捕捉虚假特征。为此，我们提出以数据为中心的框架，通过在外部完成推理来解耦这两个任务。借助强大的视觉-语言模型，我们通过对语料中的视觉证据进行密集描述、消歧查询中的多模态指代，以及将冗长指令改写为简明的检索约束，使隐式语义显性化。仅依靠推理阶段增强无法奏效；必须在这些语义密集的表示上训练检索器，才能避免分布偏移并充分发挥新增信号的价值。在 M-BEIR 数据集上的实验表明，我们的推理增强训练方法在多个强大基线上均实现一致提升。消融实验显示，语料增强主要惠及知识密集型查询，而查询增强对组合修改请求尤为关键。代码已开源：https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval"
    },
    {
        "title": "Overview of the TREC 2025 RAGTIME Track",
        "summary": "The principal goal of the RAG TREC Instrument for Multilingual Evaluation (RAGTIME) track at TREC is to study report generation from multilingual source documents. The track has created a document collection containing Arabic, Chinese, English, and Russian news stories. RAGTIME includes three task types: Multilingual Report Generation, English Report Generation, and Multilingual Information Retrieval (MLIR). A total of 125 runs were submitted by 13 participating teams (and as baselines by the track coordinators) for three tasks. This overview describes these three tasks and presents the available results.",
        "entry_id": "http://arxiv.org/abs/2602.10024v1",
        "pub_date": "2026-02-10",
        "translated_summary": "RAGTIME（RAG TREC Instrument for Multilingual Evaluation）赛道在 TREC 中的首要目标是从多语种源文档研究生成式报告。赛道已构建了一个包含阿拉伯语、中文、英语和俄语新闻故事的文档集。RAGTIME 共设有三项任务：多语种报告生成、英语报告生成和多语种信息检索（MLIR）。来自 13 支参赛团队的 125 次运行（包括赛道组织者提供的基线系统）被提交参与这三项任务。本文概述了这三项任务的设计，并展示了目前可获得的评测结果。"
    },
    {
        "title": "Kunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design",
        "summary": "Deriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify poor scaling efficiency as the main barrier to predictable power-law scaling, stemming from inefficient modules with low Model FLOPs Utilization (MFU) and suboptimal resource allocation. We introduce Kunlun, a scalable architecture that systematically improves model efficiency and resource allocation. Our low-level optimizations include Generalized Dot-Product Attention (GDPA), Hierarchical Seed Pooling (HSP), and Sliding Window Attention. Our high-level innovations feature Computation Skip (CompSkip) and Event-level Personalization. These advances increase MFU from 17% to 37% on NVIDIA B200 GPUs and double scaling efficiency over state-of-the-art methods. Kunlun is now deployed in major Meta Ads models, delivering significant production impact.",
        "entry_id": "http://arxiv.org/abs/2602.10016v1",
        "pub_date": "2026-02-10",
        "translated_summary": "为大规模推荐系统设计和分配资源的关键一步，是能够推导出可预测的性能—算力幂律关系。尽管大语言模型已建立相应的幂律规律，但该问题在兼顾用户历史与上下文特征的推荐系统中仍属空白。我们指出，低效模块导致的 Model FLOPs Utilization（MFU）畸低、以及资源分配不当是阻碍幂律可扩展性的核心原因。为此，我们提出 Kunlun——一套可扩展的系统架构，从底层到高层全链路重塑模型效率与算力分配。底层优化包含通用点积注意力（GDPA）、层次 Seed 池化（HSP）及滑动窗口注意力；高层创新则引入计算跳帧（CompSkip）与事件级个性化。这些技术把 NVIDIA B200 的 MFU 从 17% 提升至 37%，并使扩展效率翻倍，显著优于现有方案。Kunlun 已大规模应用于 Meta Ads，带来可观的线上收益。"
    },
    {
        "title": "Efficient Learning of Sparse Representations from Interactions",
        "summary": "Behavioral patterns captured in embeddings learned from interaction data are pivotal across various stages of production recommender systems. However, in the initial retrieval stage, practitioners face an inherent tradeoff between embedding expressiveness and the scalability and latency of serving components, resulting in the need for representations that are both compact and expressive. To address this challenge, we propose a training strategy for learning high-dimensional sparse embedding layers in place of conventional dense ones, balancing efficiency, representational expressiveness, and interpretability. To demonstrate our approach, we modified the production-grade collaborative filtering autoencoder ELSA, achieving up to 10x reduction in embedding size with no loss of recommendation accuracy, and up to 100x reduction with only a 2.5% loss. Moreover, the active embedding dimensions reveal an interpretable inverted-index structure that segments items in a way directly aligned with the model's latent space, thereby enabling integration of segment-level recommendation functionality (e.g., 2D homepage layouts) within the candidate retrieval model itself. Source codes, additional results, as well as a live demo are available at https://github.com/zombak79/compressed_elsa",
        "entry_id": "http://arxiv.org/abs/2602.09935v1",
        "pub_date": "2026-02-10",
        "translated_summary": "在推荐系统生产流程的各个环节中，从交互数据中学得的嵌入向量所捕获的行为模式均扮演着关键角色。然而在首轮检索阶段，从业者必须在表示能力、可扩展性和在线延迟之间权衡：既要嵌入足够紧凑，又要保持强大的表达能力。为化解这一矛盾，我们提出一种训练策略，通过“高维稀疏嵌入层”替换传统稠密嵌入，兼顾效率、表达与可解释性。我们在工业级协同过滤自编码器 ELSA 上进行验证：将嵌入维数压缩 10 倍而推荐精度无下降；压缩 100 倍仅损失 2.5%。此外，活跃嵌入维度自然呈现一种可解释的倒排索引结构，使物品在模型隐空间中被有效分段。由此可直接在候选检索模型内部实现“分段推荐”功能（例如二维首页布局）。代码、额外实验结果与在线 demo 见：https://github.com/zombak79/compressed_elsa"
    },
    {
        "title": "AmharicIR+Instr: A Two-Dataset Resource for Neural Retrieval and Instruction Tuning",
        "summary": "Neural retrieval and GPT-style generative models rely on large, high-quality supervised data, which is still scarce for low-resource languages such as Amharic. We release an Amharic data resource consisting of two datasets that supports research on (i) neural retrieval-ranking and (ii) instruction-following text generation. The retrieval-ranking dataset contains 1,091 manually verified query-positive-negative document triplets drawn from diverse Amharic sources and constructed to support contrastive training and benchmarking of neural retrievers (e.g., DPR, ColBERT-style late interaction and SPLADE-style sparse neural retrieval). Triplets are created through a combination of expert-curated queries, web-derived queries, and LLM-assisted generation, with positive/negative documents selected from the web or synthesized by LLMs and then validated by native speakers. The instruction prompt-response dataset comprises 6,285 Amharic prompt-response pairs spanning multiple domains and instruction types, generated with several LLMs and refined through manual review and correction for grammaticality, relevance, fluency, and factual plausibility. We release both datasets with standardized splits and formats (CSV,JSON,JSONL) to enable reproducible work on Amharic retrieval, ranking, and generative modelling. These datasets also come with a methodology that can be generalized to other low-resource languages.",
        "entry_id": "http://arxiv.org/abs/2602.09914v1",
        "pub_date": "2026-02-10",
        "translated_summary": "神经检索与 GPT 风格的生成模型依赖大量、高质量的监督数据，而这类数据在诸如阿姆哈拉语等低资源语种中仍然稀缺。我们发布了一份阿姆哈拉语数据资源，包含两个数据集，分别支持（i）神经检索-排序与（ii）遵循指令的文本生成研究。\n\n检索-排序数据集包含 1 091 条经过人工核验的查询–正例–负例文档三元组，源自多种阿姆哈拉语文本，专门用于支持神经检索器的对比训练与基准评测（如 DPR、ColBERT 风格的后期交互模型和 SPLADE 风格的稀疏神经检索模型）。三元组通过专家策划查询、网络采集查询与 LLM 辅助生成相结合构建；正/负例文档从网络选取或由 LLM 合成后，均由母语者验证真实性。\n\n指令提示-回应对数据集包含 6 285 条阿姆哈拉语 prompt-response 对，涵盖多种领域与指令类型，最初由多款 LLM 生成，后经人工审核与修改，对语法、相关性、流畅性和事实可信度进行了校正。\n\n我们为这两个数据集提供标准化的划分与格式（CSV、JSON、JSONL），便于在阿姆哈拉语检索、排序及生成建模研究中实现可重复实验。数据集及其构建方法可推广至其他低资源语言。"
    },
    {
        "title": "QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search",
        "summary": "Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.",
        "entry_id": "http://arxiv.org/abs/2602.09901v1",
        "pub_date": "2026-02-10",
        "translated_summary": "查询处理（QP）在大规模社交网络服务（SNS）搜索引擎中连接用户意图与内容供给。传统QP系统采用彼此独立的判别式模型流水线（如BERT），存在语义理解受限和维护成本高企双重缺陷。尽管大语言模型（LLM）被寄予厚望，现有方法往往孤立地优化各子任务，忽视了任务间固有的语义协同，并需各自迭代。此外，标准生成式方法缺乏针对SNS场景的深度落地，难以弥合开放领域语料与碎片化口语表达之间的鸿沟，也较难满足严谨的业务定义。本文提出QP-OneModel——面向SNS领域的统一生成式多任务查询理解大模型。我们将异构子任务重塑为统一的序列生成范式，通过三段式渐进对齐策略，最终以多重奖励强化学习收官。尤为关键的是，QP-OneModel创新性地生成意图描述作为高保真语义信号，有效增强下游任务（如查询改写与排序）。离线实验显示，QP-OneModel较判别式基线整体提升7.35%，其中命名实体识别（NER）F1提高9.01%，查询词权重估计F1提升9.31%。模型泛化能力突出，在未见过的新任务上比32B模型高出7.60%的准确率。在小红书全面上线后，线上A/B测试验证其工业价值：检索相关性（DCG）提升0.21%，用户留存上升0.044%。"
    },
    {
        "title": "Internalizing Multi-Agent Reasoning for Accurate and Efficient LLM-based Recommendation",
        "summary": "Large Language Models (LLMs) are reshaping recommender systems by leveraging extensive world knowledge and semantic reasoning to interpret user intent. However, effectively integrating these capabilities with collaborative signals while avoiding prohibitive inference latency remains a critical bottleneck. To address this, we propose a trajectory-driven internalization framework to develop a Single-agent Trajectory-Aligned Recommender (STAR). Specifically, to internalize complex reasoning capabilities into a single efficient model, we first design a multi-agent teacher system capable of multi-turn tool usage and reflection. This teacher utilizes a Collaborative Signal Translation mechanism to explicitly convert latent behavioral patterns into descriptive natural language evidence to enhance reasoning accuracy. Subsequently, a trajectory-driven distillation pipeline transfers this agentic logic, including planning, tool usage, and self-reflection, into the compact STAR model. Extensive experiments demonstrate that STAR surpasses its teacher by 8.7% to 39.5% while eliminating iterative latency, paving the way for real-time, reasoning-enhanced recommendation.",
        "entry_id": "http://arxiv.org/abs/2602.09829v1",
        "pub_date": "2026-02-10",
        "translated_summary": "大语言模型正凭借其广博的世界知识和语义推理能力重塑推荐系统，使其能够更精准地解读用户意图。然而，如何在不引入高昂推理延迟的前提下，将这些能力与协同信号有效融合，仍是关键瓶颈。为此，我们提出一种基于轨迹驱动的内化框架，用以训练单智能体轨迹对齐推荐器（STAR）。具体而言，为将复杂推理能力高效内化至单一模型，我们首先构建一个多智能体教师系统，支持多轮工具调用与自省；该教师通过“协同信号翻译”机制，把隐含行为模式显式转换为自然语言证据，从而提升推理精度。随后，依托轨迹驱动的蒸馏流程，将包括规划、工具调用与自我反思在内的完整代理逻辑迁移到轻量级 STAR 模型中。大量实验表明，STAR 在消除迭代延迟的同时，其效果较教师系统提升 8.7%–39.5%，为实现实时、推理增强的推荐开辟了可行路径。"
    },
    {
        "title": "Self-Supervised Learning as Discrete Communication",
        "summary": "Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.",
        "entry_id": "http://arxiv.org/abs/2602.09764v1",
        "pub_date": "2026-02-10",
        "translated_summary": "多数自监督学习（SSL）方法通过匹配同一输入的不同视角，学习连续视觉表示；然而，这种方式对表示维度中信息结构的控制能力较为有限。本文将视觉自监督学习重新描述为一个离散的通信过程：由教师网络与学生网络通过一条固定容量的二进制信道传送语义信息。与连续特征的匹配不同，学生网络被要求预测由教师网络生成的多标签二进制消息。我们采用逐元素的二元交叉熵损失来强制离散一致性，同时引入“编码率”正则项，以充分利用受限信道的容量，从而促进结构化的表示。进一步地，我们证明定期重新初始化投影头能够强化这一效果，使嵌入特征在多种离散编码下依旧保持可预测性。大量实验表明，相比仅追求连续一致性的基线，本文方法在图像分类、检索和密集视觉预测等任务上一致提升性能，并且在通过自监督适应解决域差问题时也呈现出优势。除主干特征外，我们还对学到的二进制码进行了分析，发现这些码字构成了一种紧凑且信息丰富的离散“语言”，它们捕捉了可跨类别复用的语义因子。"
    },
    {
        "title": "DiffuReason: Bridging Latent Reasoning and Generative Refinement for Sequential Recommendation",
        "summary": "Latent reasoning has emerged as a promising paradigm for sequential recommendation, enabling models to capture complex user intent through multi-step deliberation. Yet existing approaches often rely on deterministic latent chains that accumulate noise and overlook the uncertainty inherent in user intent, and they are typically trained in staged pipelines that hinder joint optimization and exploration. To address these challenges, we propose DiffuReason, a unified \"Think-then-Diffuse\" framework for sequential recommendation. It integrates multi-step Thinking Tokens for latent reasoning, diffusion-based refinement for denoising intermediate representations, and end-to-end Group Relative Policy Optimization (GRPO) alignment to optimize for ranking performance. In the Think stage, the model generates Thinking Tokens that reason over user history to form an initial intent hypothesis. In the Diffuse stage, rather than treating this hypothesis as the final output, we refine it through a diffusion process that models user intent as a probabilistic distribution, providing iterative denoising against reasoning noise. Finally, GRPO-based reinforcement learning enables the reasoning and refinement modules to co-evolve throughout training, without the constraints of staged optimization. Extensive experiments on four benchmarks demonstrate that DiffuReason consistently improves diverse backbone architectures. Online A/B tests on a large-scale industrial platform further validate its practical effectiveness.",
        "entry_id": "http://arxiv.org/abs/2602.09744v1",
        "pub_date": "2026-02-10",
        "translated_summary": "潜在推理已成为序列式推荐中一种颇具前景的范式，它通过多步权衡来捕获复杂的用户意图。然而，现有方法往往采用确定性的潜在链，会累积噪声并忽略用户意图固有的不确定性，且它们通常以分阶段流水线方式进行训练，阻碍了联合优化与探索。为应对上述挑战，我们提出DiffuReason，一个统一「先思考后扩散」的序列推荐框架。该框架将多步思考 Token 用于潜在推理，以扩散机制对中间表征进行降噪，并采用端到端的组相对策略优化（GRPO）校准来优化排序性能。\n\n在思考阶段，模型生成一系列 Thinking Tokens，对用户历史进行推理，得出初始意图假设。在扩散阶段，我们不再将此假设作为最终输出，而是通过扩散过程将其作为用户意图的概率分布予以精细化——通过迭代降噪来消除推理过程中的噪声。最后，基于 GRPO 的强化学习使得推理模块与精炼模块在训练过程中可以协同进化，不再受分阶段优化的束缚。\n\n在四个基准数据集上的大量实验表明，DiffuReason 能稳定提升多种骨干架构的性能；在某大规模工业场景上的在线 A/B 测试进一步验证了其现实有效性。"
    },
    {
        "title": "With Argus Eyes: Assessing Retrieval Gaps via Uncertainty Scoring to Detect and Remedy Retrieval Blind Spots",
        "summary": "Reliable retrieval-augmented generation (RAG) systems depend fundamentally on the retriever's ability to find relevant information. We show that neural retrievers used in RAG systems have blind spots, which we define as the failure to retrieve entities that are relevant to the query, but have low similarity to the query embedding. We investigate the training-induced biases that cause such blind spot entities to be mapped to inaccessible parts of the embedding space, resulting in low retrievability. Using a large-scale dataset constructed from Wikidata relations and first paragraphs of Wikipedia, and our proposed Retrieval Probability Score (RPS), we show that blind spot risk in standard retrievers (e.g., CONTRIEVER, REASONIR) can be predicted pre-index from entity embedding geometry, avoiding expensive retrieval evaluations. To address these blind spots, we introduce ARGUS, a pipeline that enables the retrievability of high-risk (low-RPS) entities through targeted document augmentation from a knowledge base (KB), first paragraphs of Wikipedia, in our case. Extensive experiments on BRIGHT, IMPLIRET, and RAR-B show that ARGUS achieves consistent improvements across all evaluated retrievers (averaging +3.4 nDCG@5 and +4.5 nDCG@10 absolute points), with substantially larger gains in challenging subsets. These results establish that preemptively remedying blind spots is critical for building robust and trustworthy RAG systems (Code and Data).",
        "entry_id": "http://arxiv.org/abs/2602.09616v1",
        "pub_date": "2026-02-10",
        "translated_summary": "可靠的检索增强生成（RAG）系统的核心在于检索器能否准确找到相关信息。本文发现，RAG 系统使用的神经检索器存在“盲区”，即无法检索到与查询高度相关、但与查询嵌入之间相似度较低的实体。我们证明，训练过程中引入的偏差将这些盲区实体映射到了嵌入空间中的不可达区域，导致它们检索概率极低。\n\n基于从 Wikidata 关系及维基百科首段文本构建的大规模数据集，并借助我们提出的检索概率评分（RPS），我们可在索引前，仅通过实体在嵌入空间中的几何位置，就预测标准检索器（如 CONTRIEVER、REASONIR）的盲区风险，省去昂贵的检索评估。\n\n为消除这些盲区，我们推出 ARGUS 流程：首先识别高盲区风险（低 RPS）实体，随后从知识库（本文使用维基百科首段文本）中有针对性地增补文档，从而提升其可检索性。在 BRIGHT、IMPLIRET 和 RAR-B 上的大量实验表明，ARGUS 在所有基准检索器上均带来持续提升（nDCG@5 平均+3.4，nDCG@10 平均+4.5 绝对点数），在困难子集上增益更为显著。这些结果证实：预先修复盲区对于构建稳健可信的 RAG 系统至关重要。（代码与数据已开源）"
    },
    {
        "title": "LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval",
        "summary": "Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\\footnote{\\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\\footnote{\\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.",
        "entry_id": "http://arxiv.org/abs/2602.09570v1",
        "pub_date": "2026-02-10",
        "translated_summary": "大型语言模型（LLM）正日益成为获取法律信息的重要工具。然而，在多语种法律环境中，它们的部署仍受限于不可靠的检索性能以及缺乏经过领域适配的开放嵌入模型。具体而言，现有跨语种法律语料库并未针对语义检索精心构建，而基于 PDF 的立法文本由于提取不完美，会引入大量噪声。为应对上述挑战，我们推出了 LEMUR——一个从 24,953 份欧盟环境立法官方 EUR-Lex PDF 文件汇总而来、涵盖 25 种语言的大规模多语种法律语料库。我们利用“词汇内容评分”（Lexical Content Score，LCS），通过与权威 HTML 版本进行词汇一致性比对，来量化 PDF‐文本转换的保真度。基于 LEMUR，我们在单语及双语场景下，采用对比学习目标对三个先进的跨语种嵌入模型进行了微调，以贴合真实的法律检索需求。实验在资源稀缺和资源丰富语言上同时开展，结果显示，法律域微调较强劲基线能持续提升 Top-k 检索准确率，且低资源语言的提升尤为显著。跨语言评估进一步表明，这些改进可迁移至未见语言，说明微调主要增强了语言无关、内容层面的法律表示，而非特定语言的线索。本研究的代码和数据已开源发布。"
    },
    {
        "title": "Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA",
        "summary": "Conversational question answering increasingly relies on retrieval-augmented generation (RAG) to ground large language models (LLMs) in external knowledge. Yet, most existing studies evaluate RAG methods in isolation and primarily focus on single-turn settings. This paper addresses the lack of a systematic comparison of RAG methods for multi-turn conversational QA, where dialogue history, coreference, and shifting user intent substantially complicate retrieval. We present a comprehensive empirical study of vanilla and advanced RAG methods across eight diverse conversational QA datasets spanning multiple domains. Using a unified experimental setup, we evaluate retrieval quality and answer generation using generator and retrieval metrics, and analyze how performance evolves across conversation turns. Our results show that robust yet straightforward methods, such as reranking, hybrid BM25, and HyDE, consistently outperform vanilla RAG. In contrast, several advanced techniques fail to yield gains and can even degrade performance below the No-RAG baseline. We further demonstrate that dataset characteristics and dialogue length strongly influence retrieval effectiveness, explaining why no single RAG strategy dominates across settings. Overall, our findings indicate that effective conversational RAG depends less on method complexity than on alignment between the retrieval strategy and the dataset structure. We publish the code used.\\footnote{\\href{https://github.com/Klejda-A/exp-rag.git}{GitHub Repository}}",
        "entry_id": "http://arxiv.org/abs/2602.09552v1",
        "pub_date": "2026-02-10",
        "translated_summary": "对话式问答日益依赖检索增强生成（RAG）来将大语言模型（LLM）与外部知识对齐。然而，现有研究大多孤立地评估RAG方法，且主要集中于单轮场景。本文针对多轮对话式问答中RAG方法缺乏系统比较的现状展开研究——在该场景下，对话历史、指代关系及不断演变的用户意图使检索任务变得尤为复杂。  \n我们在八个跨多个领域、类型各异的对话式问答数据集上，对“普通版”与多种先进RAG方法进行了全面的实证研究。依托统一的实验框架，我们分别从检索质量和答案生成两个角度，采用生成器指标和检索器指标进行综合评估，并分析指标在多轮对话中的演化。  \n结果表明：轻量但鲁棒的方法——重排序、Hybrid BM25 与 HyDE——在绝大多数情况下一致地超越了普通RAG；而部分先进技术不仅没有带来提升，甚至导致性能低于无RAG基线。进一步分析指出，数据集特性与对话长度对检索效果影响显著，因此不存在能在所有场景下全面领先的单一策略。  \n综上所述，有效的对话式RAG核心不在于方法复杂，而在于检索策略与数据集结构的匹配程度。实验代码已开源。"
    },
    {
        "title": "The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training",
        "summary": "Prior work reports conflicting results on query diversity in synthetic data generation for dense retrieval. We identify this conflict and design Q-D metrics to quantify diversity's impact, making the problem measurable. Through experiments on 4 benchmark types (31 datasets), we find query diversity especially benefits multi-hop retrieval. Deep analysis on multi-hop data reveals that diversity benefit correlates strongly with query complexity ($r$$\\geq$0.95, $p$$<$0.05 in 12/14 conditions), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides actionable thresholds (CW$>$10: use diversity; CW$<$7: avoid it). Guided by CDP, we propose zero-shot multi-query synthesis for multi-hop tasks, achieving state-of-the-art performance.",
        "entry_id": "http://arxiv.org/abs/2602.09448v1",
        "pub_date": "2026-02-10",
        "translated_summary": "先前研究就合成数据生成中的查询多样性对稠密检索的作用报告了相互冲突的结论。本文首先指出这一矛盾，并提出Q-D指标以量化多样性产生的影响，使该问题可度量。在4种基准类型、共31个数据集上的实验表明，查询多样性尤其能提升多跳检索性能。通过对多跳数据的深入分析发现，多样性收益与查询复杂度（以内容词CW衡量）高度相关（12/14种情况下 $r$$\\geq$0.95，$p$$<$0.05）。我们将其形式化为“复杂度–多样性原理”（CDP）：查询复杂度决定最优多样性。该原理给出可操作的阈值：CW$>$10时使用多样化查询，CW$<$7时则避免。基于CDP，我们提出零样本多查询合成方法，专门为多跳任务设计，达到了当前最佳性能。"
    },
    {
        "title": "Personalized Parameter-Efficient Fine-Tuning of Foundation Models for Multimodal Recommendation",
        "summary": "In recent years, substantial research has integrated multimodal item metadata into recommender systems, often by using pre-trained multimodal foundation models to encode such data. Since these models are not originally trained for recommendation tasks, recent works efficiently adapt them via parameter-efficient fine-tuning (PEFT). However, even with PEFT, item embeddings from multimodal foundation models remain user-blind: item embeddings are not conditioned on user interests, despite the fact that users with diverse interests attend to different item aspects. To address this limitation, we propose PerPEFT, a personalized PEFT strategy for multimodal recommendation. Specifically, PerPEFT groups users by interest and assigns a distinct PEFT module to each group, enabling each module to capture the fine-grained item aspects most predictive of that group`s purchase decisions. We further introduce a specialized training technique that strengthens this user-group conditioning. Notably, PerPEFT is PEFT-agnostic and can be paired with any PEFT method applicable to multimodal foundation models. Through extensive experiments, we show that (1) PerPEFT outperforms the strongest baseline by up to 15.3% (NDCG@20) and (2) delivers consistent gains across diverse PEFT variants. It is noteworthy that, even with personalization, PEFT remains lightweight, adding only 1.3% of the parameter count of the foundation model. We provide our code and datasets at https://github.com/kswoo97/PerPEFT.",
        "entry_id": "http://arxiv.org/abs/2602.09445v1",
        "pub_date": "2026-02-10",
        "translated_summary": "近年来，大量研究将多模态商品元数据整合到推荐系统中，常用的做法是利用预训练的多模态基础模型对这些数据进行编码。然而，由于这些模型并非专为推荐任务训练，近期工作借助参数高效微调（PEFT）来对其进行针对性适配。即便如此，经由多模态基础模型生成的商品嵌入依旧是“用户盲”的：它们不会因用户兴趣而异，而实际场景中含不同兴趣的用户往往关注商品的差异化方面。为解决该局限，本文提出PerPEFT，一种面向多模态推荐系统的个性化PEFT策略。具体而言，PerPEFT首先按兴趣对用户聚类，并为每组分配一个独立的PEFT模块，使各模块能够捕获最能预测该组购买决策的细粒度商品特征。我们进一步设计了一种专门的训练技术，以增强这种用户群体条件化。值得注意的是，PerPEFT与PEFT方法本身解耦，可灵活适配任何可用于多模态基础模型的PEFT变体。大量实验证明：（1）PerPEFT在NDCG@20指标上较最佳基线提升达15.3%；（2）在不同PEFT变体上均带来一致增益。更关键的是，即使引入个性化，PEFT依旧轻量，新增参数仅占基础模型的1.3%。代码与数据集已开源：https://github.com/kswoo97/PerPEFT。"
    },
    {
        "title": "SARM: LLM-Augmented Semantic Anchor for End-to-End Live-Streaming Ranking",
        "summary": "Large-scale live-streaming recommendation requires precise modeling of non-stationary content semantics under strict real-time serving constraints. In industrial deployment, two common approaches exhibit fundamental limitations: discrete semantic abstractions sacrifice descriptive precision through clustering, while dense multimodal embeddings are extracted independently and remain weakly aligned with ranking optimization, limiting fine-grained content-aware ranking. To address these limitations, we propose \\textbf{SARM}, an end-to-end ranking architecture that integrates natural-language semantic anchors directly into ranking optimization, enabling fine-grained author representations conditioned on multimodal content. Each semantic anchor is represented as learnable text tokens jointly optimized with ranking features, allowing the model to adapt content descriptions to ranking objectives. A lightweight dual-token gated design captures domain-specific live-streaming semantics, while an asymmetric deployment strategy preserves low-latency online training and serving. Extensive offline evaluation and large-scale A/B tests show consistent improvements over production baselines. SARM is fully deployed and serves over 400 million users daily.",
        "entry_id": "http://arxiv.org/abs/2602.09401v1",
        "pub_date": "2026-02-10",
        "translated_summary": "大规模直播推荐需在严格实时约束下，对非稳态内容语义进行精准建模。工业界现行两类方案均存在根本缺陷：离散语义抽象利用聚类牺牲了描述精度；稠密多模态嵌入独立提取，仅与排序目标微弱对齐，因而无法实现细粒度的内容感知排序。为克服上述局限，我们提出 **SARM**，一种端到端排序架构，将自然语言语义锚点直接融入排序优化过程，使作者表示能够在多模态内容条件下达到细粒度刻画。每一个语义锚点以可学习的文本词元形式表示，与排序特征联合优化，令内容描述随排序目标动态适配。轻量级双词门控结构可捕获直播领域专属语义，而非对称部署策略保障了低延迟的在线训练与推理。全面离线评估与大规模 A/B 测试均表明，SARM 相对生产基线持续带来显著提升；目前该架构已全量上线，日服务用户超 4 亿人次。"
    },
    {
        "title": "Query-Mixed Interest Extraction and Heterogeneous Interaction: A Scalable CTR Model for Industrial Recommender Systems",
        "summary": "Learning effective feature interactions is central to modern recommender systems, yet remains challenging in industrial settings due to sparse multi-field inputs and ultra-long user behavior sequences. While recent scaling efforts have improved model capacity, they often fail to construct both context-aware and context-independent user intent from the long-term and real-time behavior sequence. Meanwhile, recent work also suffers from inefficient and homogeneous interaction mechanisms, leading to suboptimal prediction performance. To address these limitations, we propose HeMix, a scalable ranking model that unifies adaptive sequence tokenization and heterogeneous interaction structure. Specifically, HeMix introduces a Query-Mixed Interest Extraction module that jointly models context-aware and context-independent user interests via dynamic and fixed queries over global and real-time behavior sequences. For interaction, we replace self-attention with the HeteroMixer block, enabling efficient, multi-granularity cross-feature interactions that adopt the multi-head token fusion, heterogeneous interaction and group-aligned reconstruction pipelines. HeMix demonstrates favorable scaling behavior, driven by the HeteroMixer block, where increasing model scale via parameter expansion leads to steady improvements in recommendation accuracy. Experiments on industrial-scale datasets show that HeMix scales effectively and consistently outperforms strong baselines. Most importantly, HeMix has been deployed on the AMAP platform, delivering significant online gains: +0.61% GMV, +2.32% PV_CTR, and +0.81% UV_CVR.",
        "entry_id": "http://arxiv.org/abs/2602.09387v1",
        "pub_date": "2026-02-10",
        "translated_summary": "学习有效的特征交互是现代推荐系统的核心任务，但在工业场景中，由于输入稀疏的多字段和超长用户行为序列，这一挑战尤为突出。尽管近期模型扩容努力提升了容量，却仍难以同时从长期和实时行为序列中构建出上下文感知与上下文无关并存的完整用户意图；同时，现行方法因交互机制低效且单一，导致预测性能次优。为此，我们提出 HeMix——一种通过统一自适应序列分词与异构交互结构的可扩展排序模型。具体而言，HeMix 引入 Query-Mixed Interest Extractor 模块，借助动态与固定查询，在全局和实时行为序列上联合建模上下文感知与上下文无关的用户兴趣。在交互层面，我们用 HeteroMixer 模块替代自注意力，实现高效的多粒度跨特征交互：其核心包括多头 Token 融合、异构交互和组对齐重建三条流水线。得益于 HeteroMixer，HeMix 展现出优异的扩展性——通过参数扩张持续提高推荐准确率。工业规模数据集的实验表明，HeMix 不仅能有效扩展，还持续优于强大的基线模型。最重要的是，HeMix 已在 AMAP 平台全量上线，带来显著线上收益：GMV+0.61%，PV_CTR+2.32%，UV_CVR+0.81%。"
    },
    {
        "title": "SMES: Towards Scalable Multi-Task Recommendation via Expert Sparsity",
        "summary": "Industrial recommender systems typically rely on multi-task learning to estimate diverse user feedback signals and aggregate them for ranking. Recent advances in model scaling have shown promising gains in recommendation. However, naively increasing model capacity imposes prohibitive online inference costs and often yields diminishing returns for sparse tasks with skewed label distributions. This mismatch between uniform parameter scaling and heterogeneous task capacity demands poses a fundamental challenge for scalable multi-task recommendation. In this work, we investigate parameter sparsification as a principled scaling paradigm and identify two critical obstacles when applying sparse Mixture-of-Experts (MoE) to multi-task recommendation: exploded expert activation that undermines instance-level sparsity and expert load skew caused by independent task-wise routing. To address these challenges, we propose SMES, a scalable sparse MoE framework with progressive expert routing. SMES decomposes expert activation into a task-shared expert subset jointly selected across tasks and task-adaptive private experts, explicitly bounding per-instance expert execution while preserving task-specific capacity. In addition, SMES introduces a global multi-gate load-balancing regularizer that stabilizes training by regulating aggregated expert utilization across all tasks. SMES has been deployed in Kuaishou large-scale short-video services, supporting over 400 million daily active users. Extensive online experiments demonstrate stable improvements, with GAUC gain of 0.29% and a 0.31% uplift in user watch time.",
        "entry_id": "http://arxiv.org/abs/2602.09386v1",
        "pub_date": "2026-02-10",
        "translated_summary": "工业级推荐系统通常依赖多任务学习来估计多样化的用户反馈信号，并将其聚合用于排序。近年来，通过模型放大带来的效果提升在推荐领域前景广阔。然而，简单地增加模型规模不仅会在线推理时带来极高的开销，对于标签分布极度倾斜的稀疏任务，还常出现收益递减的现象。这种“统一放大参数”与“各任务实际所需容量差异巨大”之间的错位为可扩展的多任务推荐带来了根本性挑战。\n\n本文将“参数稀疏化”视为一种有原则的扩展范式，研究其在多任务推荐中的应用，并在稀疏 Mixture-of-Experts（MoE）框架下发现两个关键障碍：  \n1) 专家激活数量爆炸导致实例级稀疏性被破坏；  \n2) 任务独立的路由策略造成专家负载严重倾斜。  \n\n针对以上问题，我们提出 SMES——一种具备渐进式专家路由的可扩展稀疏 MoE 框架。SMES 将专家激活分解为：  \n• 跨任务联合挑选的“共享专家子集”；  \n• 各任务私有的“自适应专家”。  \n该设计显式约束每次推理所激活的专家数量，同时保留任务专用容量。此外，SMES 引入全局多门负载均衡正则项，通过统筹所有任务的专家使用率来稳定训练。\n\nSMES 已在快手大规模短视频服务体系上线，支持超过 4 亿日活用户。大量在线实验验证其增益稳定：GAUC 提升 0.29%，用户观看时长提升 0.31%。"
    },
    {
        "title": "Beyond the Unit Hypersphere: Embedding Magnitude in Contrastive Learning",
        "summary": "Cosine similarity is prevalent in contrastive learning, yet it makes an implicit assumption: embedding magnitude is noise. Prior work occasionally found dot product and cosine similarity comparable, but left unanswered WHAT information magnitude carries, WHEN it helps, and HOW to leverage it. We conduct a systematic study through a $2 \\times 2$ ablation that independently controls input-side and output-side normalization across text and vision models. Our findings reveal three key insights. First, in text retrieval, output (document) magnitude strongly correlates with relevance (Cohen's $d$ up to 1.80), yielding the largest gains on reasoning-intensive tasks. Second, input and output magnitudes serve asymmetric roles: output magnitude directly scales similarity scores while input magnitude modulates training dynamics. Third, magnitude learning benefits asymmetric tasks (text retrieval, RAG) but harms symmetric tasks (STS, text-image alignment). These findings establish a task symmetry principle: the choice between cosine and dot product depends on whether the task has distinct input roles, enabling cost-free improvements by simply removing an unnecessary constraint.",
        "entry_id": "http://arxiv.org/abs/2602.09229v1",
        "pub_date": "2026-02-09",
        "translated_summary": "尽管余弦相似度在对比学习中无处不在，它却隐含了一个假设：嵌入向量幅值仅为噪声。先前研究偶尔指出点积与余弦相似度表现相当，却未能揭示幅值携带了“什么信息”、在“何时有用”以及“如何利用”这三个关键问题。本文通过一项 $2 \\times 2$ 系统消融实验，独立控制文本与视觉模型的输入端和输出端归一化，得出三大核心结论：首先，在文本检索场景中，输出端（文档）幅值与查询相关性强相关（Cohen's $d$ 高达 1.80），在需要推理的任务上带来的提升最大；其次，输入与输出幅值承担非对称角色——输出幅值直接放大相似度得分，而输入幅值则调节训练动态；最后，学习幅值对具有非对称性质的任务（如文本检索、RAG）有益，却损害对称性任务（STS、文本—图像对齐）。这些发现确立了“任务对称性原理”：应依据任务输入是否扮演不同角色，在余弦与点积之间做选择；只需移除不必要的约束，即可不增加任何额外成本地取得性能提升。"
    },
    {
        "title": "FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases",
        "summary": "Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.",
        "entry_id": "http://arxiv.org/abs/2602.09163v1",
        "pub_date": "2026-02-09",
        "translated_summary": "科学知识库通过将原始文献中的发现提炼整理为结构化、可供人类和新兴AI系统查询的格式，显著加快了科研发现的步伐。然而，维护这些资源需要专业策展人不断搜索相关论文、在多篇文章之间核对证据，并最终生成基于本体的注释——现有的诸多基准仅聚焦于命名实体识别或关系抽取等孤立子任务，无法覆盖这一完整流程。\n\n为此，我们提出了 FlyBench，用于评估AI代理在“端到端代理式科学文献本体策展”上的能力。代理仅凭一个基因符号，就需检索并阅读由16,898篇全文文献构成的语料，最终产出结构化注释：基因本体（Gene Ontology）功能项、表达模式项，以及数十年间名称演变的同义词链接。该基准数据取自果蝇知识库 FlyBase，涵盖 100 个基因的 7,397 条专家手工注释。\n\n我们测试了四种基线代理架构：记忆式、固定流程式、单智能体式和多智能体式。实验表明，架构选择对性能有显著影响：多智能体设计全面领先简单结构；然而，单纯放大基础模型规模，收益却逐渐递减。所有基线方法均留有巨大改进空间。\n\n进一步分析揭示了一系列未来改进的关键发现：例如，代理主要利用检索功能验证其已有参数化知识，而非真正发现新信息。我们希望 FlyBench 能够成为推动「检索增强科学推理」能力发展的催化剂，这一能力在广泛科学领域均极具应用潜力。"
    },
    {
        "title": "An Interactive Metrics Dashboard for the Keck Observatory Archive",
        "summary": "Since 2004, the Keck Observatory Archive (KOA) has operated as a NASA-funded collaboration between the NASA Exoplanet Science Institute ( NExScI) and the W.M. Keck Observatory. It ingests and serves all data acquired by the twin 10-meter Keck telescopes on Mauna Kea, Hawaii. In the past three years, KOA has begun a modernization program to replace the architecture and systems used since the archive's creation with a new modern Python-based infrastructure. This infrastructure will position KOA to respond to the rapid growth of new and complex data sets that will be acquired by new instruments now in development, and enable follow-up to identify the deluge of alerts of transient sources expected by new survey telescopes such as the Vera C. Rubin Observatory. Since 2022, KOA has ingested new data in near-real time, generally within one minute of creation, and has made them immediately accessible to observers through a dedicated web interface. The archive is now deploying a new, scalable, Python-based, VO-compliant query infrastructure built with the Plotly-Dash framework and R-tree indices to speed-up queries by a factor of 20.\n  The project described here exploits the new query infrastructure to develop a dashboard that will return live metrics on the performance and growth of the archive. These metrics assess the current health of the archive and guide planning future hardware and software upgrades. This single dashboard will enable, for example, monitoring of real-time ingestion, as well as studying the long-term growth of the archive. Current methods of gathering metrics that have been in place since the archive opened will not support the archive as it continues to scale. These methods suffer from high latency, are not optimized for on-demand metrics, are scattered among various tools, and are cumbersome to use.",
        "entry_id": "http://arxiv.org/abs/2602.09126v1",
        "pub_date": "2026-02-09",
        "translated_summary": "自2004年以来，Keck台档案库（KOA）一直作为NASA系外行星科学研究所（NExScI）与夏威夷莫纳克亚凯克天文台之间的合作项目，由美国宇航局资助。该档案库接收并发布两台10米凯克望远镜所采集的全部数据。在过去三年中，KOA启动了一项现代化工程，旨在用一套全新的以Python为基础架构替代自档案库建立之初沿用至今的体系与系统。新的基础架构使KOA能够应对正在研发中的新仪器即将带来、规模与复杂度迅速增长的大量数据，并为应对如Vera C. Rubin天文台等新一代巡天望远镜所产生的瞬变源海量警报，提供后续证认所需的快速响应能力。自2022年起，KOA几乎实时地接收新数据，通常在一分钟内完成并立即通过专用网页接口供观测者访问。目前，档案库正在部署一种全新、可扩展、基于Python并符合虚拟天文台（VO）标准的查询基础架构；该架构采用 Plotly-Dash 框架和 R-tree 索引，可将查询速度提升20倍。\n\n本文介绍的项目利用这一全新查询基础架构，开发一套仪表盘（dashboard），实时返回档案库运行性能与数据增长的各项指标。这些指标既评估档案库的即时健康状况，也为未来的软硬件升级提供规划依据。通过该单一仪表盘，用户可在同一界面对实时数据注入进行监控，并分析档案库的长期增长情况。自档案库建立以来采用的既有指标收集方法已无法满足其继续扩展的需求，这些方法存在延迟高、未能针对即需指标优化、分散于多种工具、使用繁琐等问题。"
    },
    {
        "title": "Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion",
        "summary": "Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.",
        "entry_id": "http://arxiv.org/abs/2602.08917v1",
        "pub_date": "2026-02-09",
        "translated_summary": "利用大语言模型（LLM）进行查询扩展前景广阔，但现有方法通常依赖手工设计的提示、人工挑选的示例或单一 LLM，既难以规模化，又对领域迁移敏感。我们提出了一种自动且可领域自适应的查询扩展（QE）框架。该框架利用 BM25-MonoT5 管道采集伪相关文档，构建领域内示例库；并在无需训练的情况下，采用基于聚类的多样化示例选择策略，在上下文中实现稳健、稳定的无监督 QE。为进一步利用不同模型的互补性，我们设计了双 LLM 集成方案：由两个异构 LLM 独立生成扩展后，再由精炼 LLM 将其整合为统一、连贯的查询扩展。在 TREC DL20、DBPedia 和 SciFact 三个基准上的实验表明，经过精炼的集成方案相比 BM25、Rocchio、零样本以及固定少样本基线，都实现了持续且统计显著的提升。整套框架为示例选择与多 LLM 生成提供了可重复测试平台，并为真实场景下的无标签 QE 提供了落地解决方案。"
    },
    {
        "title": "Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval",
        "summary": "Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference for text generated by large language models (LLMs). This bias is called \"source bias\", and it has been hypothesized that lower perplexity contributes to this effect. In this study, we revisit this claim by conducting a controlled evaluation to trace the emergence of such preferences across training stages and data sources. Using parallel human- and LLM-generated counterparts of the SciFact and Natural Questions (NQ320K) datasets, we compare unsupervised checkpoints with models fine-tuned using in-domain human text, in-domain LLM-generated text, and MS MARCO. Our results show the following: 1) Unsupervised retrievers do not exhibit a uniform pro-LLM preference. The direction and magnitude depend on the dataset. 2) Across the settings tested, supervised fine-tuning on MS MARCO consistently shifts the rankings toward LLM-generated text. 3) In-domain fine-tuning produces dataset-specific and inconsistent shifts in preference. 4) Fine-tuning on LLM-generated corpora induces a pronounced pro-LLM bias. Finally, a retriever-centric perplexity probe involving the reattachment of a language modeling head to the fine-tuned dense retriever encoder indicates agreement with relevance near chance, thereby weakening the explanatory power of perplexity. Our study demonstrates that source bias is a training-induced phenomenon rather than an inherent property of dense retrievers.",
        "entry_id": "http://arxiv.org/abs/2602.10833v1",
        "pub_date": "2026-02-11",
        "translated_summary": "稠密检索是在开放域自然语言处理任务中获取相关语境或世界知识的有前途方法，现已广泛应用于信息检索系统。然而，近期研究发现，人们对大语言模型（LLM）生成文本存在广泛偏好，这一现象被称为“来源偏差”（source bias），并被假设与更低的困惑度（perplexity）有关。本文通过控制评估重新审视这一假设，追踪该偏好如何随训练阶段和数据来源逐步显现。我们构建了 SciFact 和 Natural Questions（NQ320K）两个数据集的并行人工与 LLM 生成版本，并在以下四类设置上比较检索器表现：无监督检查点、以域内人工文本微调的模型、以域内 LLM 文本微调的模型，以及以 MS MARCO 微调的模型。实验结果表明：1) 无监督检索器并未表现出一致的偏 LLM 倾向，偏好方向和强度因数据集而异；2) 在所有测试条件下，以 MS MARCO 监督微调都会稳定地让 LLM 生成文本排位上升；3) 域内微调带来的偏好变化具有数据集特异性且表现不一致；4) 直接在 LLM 生成语料上微调会显著加剧来源偏差。最后，我们将语言建模头重新附加至已微调的稠密检索器编码器，以“检索器中心困惑度探测”评估困惑度与相关性的关系，结果显示二者几乎随机一致，从而削弱了困惑度作为来源偏差的解释力。本研究表明，来源偏差是训练过程中诱发出的现象，而非稠密检索器固有的属性。"
    },
    {
        "title": "EST: Towards Efficient Scaling Laws in Click-Through Rate Prediction via Unified Modeling",
        "summary": "Efficiently scaling industrial Click-Through Rate (CTR) prediction has recently attracted significant research attention. Existing approaches typically employ early aggregation of user behaviors to maintain efficiency. However, such non-unified or partially unified modeling creates an information bottleneck by discarding fine-grained, token-level signals essential for unlocking scaling gains. In this work, we revisit the fundamental distinctions between CTR prediction and Large Language Models (LLMs), identifying two critical properties: the asymmetry in information density between behavioral and non-behavioral features, and the modality-specific priors of content-rich signals. Accordingly, we propose the Efficiently Scalable Transformer (EST), which achieves fully unified modeling by processing all raw inputs in a single sequence without lossy aggregation. EST integrates two modules: Lightweight Cross-Attention (LCA), which prunes redundant self-interactions to focus on high-impact cross-feature dependencies, and Content Sparse Attention (CSA), which utilizes content similarity to dynamically select high-signal behaviors. Extensive experiments show that EST exhibits a stable and efficient power-law scaling relationship, enabling predictable performance gains with model scale. Deployed on Taobao's display advertising platform, EST significantly outperforms production baselines, delivering a 3.27\\% RPM (Revenue Per Mile) increase and a 1.22\\% CTR lift, establishing a practical pathway for scalable industrial CTR prediction models.",
        "entry_id": "http://arxiv.org/abs/2602.10811v1",
        "pub_date": "2026-02-11",
        "translated_summary": "高效的工业点击率（CTR）预测规模化扩展问题近来受到了广泛关注。现有方法通常对用户行为进行过早聚合，以维持计算效率。然而，这种非统一或仅部分统一的建模方式带来信息瓶颈，丢失了粒度级的、token 级信号，而这些信号正是实现规模收益的关键。本文重新审视 CTR 预测与大语言模型（LLM）的根本差异，识别出两大关键特性：其一，行为特征与非行为特征在信息密度上的不对称性；其二，内容丰富的信号本身具有模态先验。基于此，我们提出高效可扩展 Transformer（EST）框架，它将所有原始输入不加损失地视为单一序列，实现完全统一建模。EST 由两大模块组成：轻量级交叉注意力（LCA）通过剪除冗余的自相互作用，聚焦于高影响力的跨特征依赖；内容稀疏注意力（CSA）则依据内容相似度动态选取高信号行为。大量实验表明，EST 展现出稳定且高效的幂律扩展关系，能通过模型规模预测性提升性能。在淘宝展示广告平台上线后，EST 显著优于生产基线，RPM（每千次展示收入）提升 3.27%，CTR 提升 1.22%，为工业 CTR 预测的规模化扩展提供了可靠实践路径。"
    },
    {
        "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories",
        "summary": "Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.",
        "entry_id": "http://arxiv.org/abs/2602.10809v1",
        "pub_date": "2026-02-11",
        "translated_summary": "已有的多模态检索系统在语义匹配方面表现出色，但它们默认查询与图像之间的相关度可以独立于上下文被衡量。该范式忽略了现实视觉流中广泛存在的依赖关系：信息弥散在时序序列中，而并非局限于单张快照。为弥合这一缺口，我们提出 DeepImageSearch——一种全新的“代理式”框架，将图像检索重新定义为自主探索任务。模型必须基于原始视觉历史进行多步规划与推理，依据隐含的上下文线索锁定目标。我们构建 DISBench，一套依托关联视觉数据的严苛评测基准。针对创建依赖上下文的查询所面临的规模化难题，我们设计人机协同流水线：先由视觉-语言模型挖掘潜在的时空关联，再交由人类验证，从而将繁琐的上下文发现任务前移。此外，我们基于模块化代理框架构建鲁棒基线，配备细粒度工具和双记忆系统，以实现长时域导航。大量实验表明，DISBench 对当前标杆模型构成严峻挑战，凸显了在下一代检索系统中引入代理式推理的必要性。"
    },
    {
        "title": "VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection",
        "summary": "Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.",
        "entry_id": "http://arxiv.org/abs/2602.10787v1",
        "pub_date": "2026-02-11",
        "translated_summary": "软件漏洞检测（SVD）是现代系统面临的一项关键挑战。大型语言模型（LLM）能够在给出预测的同时提供自然语言解释，但现有研究大多聚焦于二分类评估，且产生的解释与通用缺陷枚举（CWE）类别的语义往往不一致。本文提出 VulReaD——一种基于知识图谱引导的漏洞推理与检测框架，使任务从单纯的二分类升维到 CWE 级细粒度推理。VulReaD 以安全知识图谱（KG）为语义主干，利用高性能教师 LLM 自动生成与 CWE 一致的对比式推理监督信号，从而无需人工标注即可训练学生模型。学生模型进一步经奇率偏好优化（ORPO）微调，鼓励同分类体系对齐的推理过程，同时抑制未经支持的错误解释。在三个真实世界数据集上的实验表明，与最佳基线相比，VulReaD 使二分类 F1 提升 8–10%，多分类 Macro-F1 提升 30%，Micro-F1 提升 18%。结果还显示，LLM 在二分类检测上已超越深度学习基线，而知识图谱引导的推理显著增强了 CWE 覆盖度与可解释性。"
    },
    {
        "title": "Equity by Design: Fairness-Driven Recommendation in Heterogeneous Two-Sided Markets",
        "summary": "Two-sided marketplaces embody heterogeneity in incentives: producers seek exposure while consumers seek relevance, and balancing these competing objectives through constrained optimization is now a standard practice. Yet real platforms face finer-grained complexity: consumers differ in preferences and engagement patterns, producers vary in catalog value and capacity, and business objectives impose additional constraints beyond raw relevance. We formalize two-sided fairness under these realistic conditions, extending prior work from soft single-item allocations to discrete multi-item recommendations. We introduce Conditional Value-at-Risk (CVaR) as a consumer-side objective that compresses group-level utility disparities, and integrate business constraints directly into the optimization. Our experiments reveal that the \"free fairness\" regime, where producer constraints impose no consumer cost, disappears in multi item settings. Strikingly, moderate fairness constraints can improve business metrics by diversifying exposure away from saturated producers. Scalable solvers match exact solutions at a fraction of the runtime, making fairness-aware allocation practical at scale. These findings reframe fairness not as a tax on platform efficiency but as a lever for sustainable marketplace health.",
        "entry_id": "http://arxiv.org/abs/2602.10739v1",
        "pub_date": "2026-02-11",
        "translated_summary": "双边市场具有激励异质性：生产者渴望获得更多曝光，消费者则追求内容相关度，借助约束优化在二者之间寻求平衡已经成为通行的做法。然而，真实平台面临更细粒度的复杂性：消费者偏好与参与模式各不相同；生产商的商品价值、交付容量千差万别；而平台还必须满足超出“纯粹相关度”的其他业务约束。本文在这些现实条件下对两边公平进行了形式化，将先前基于软约束单物件分配的研究扩展到离散的多物品推荐场景。\n\n我们提出将条件风险价值（CVaR）作为消费者侧目标函数，以压缩群体间效用差异；同时把业务约束直接融入优化模型。实验表明，在单物品场景下出现的“无代价公平”区间——即对生产者的公平约束几乎不影响消费者效用的情况——在多物品设定中消失。更显著的是，适度的公平约束可通过把曝光从已饱和生产者分散出去，反而提升关键业务指标。我们所设计的可扩展求解器以极小运行时间即可逼近精确解，使大规模公平感知分配切实可行。\n\n这些发现将“公平”从对平台效率的“赋税”重新定位为促进市场长期健康的一种杠杆。"
    },
    {
        "title": "A Cognitive Distribution and Behavior-Consistent Framework for Black-Box Attacks on Recommender Systems",
        "summary": "With the growing deployment of sequential recommender systems in e-commerce and other fields, their black-box interfaces raise security concerns: models are vulnerable to extraction and subsequent adversarial manipulation. Existing black-box extraction attacks primarily rely on hard labels or pairwise learning, often ignoring the importance of ranking positions, which results in incomplete knowledge transfer. Moreover, adversarial sequences generated via pure gradient methods lack semantic consistency with real user behavior, making them easily detectable. To overcome these limitations, this paper proposes a dual-enhanced attack framework. First, drawing on primacy effects and position bias, we introduce a cognitive distribution-driven extraction mechanism that maps discrete rankings into continuous value distributions with position-aware decay, thereby advancing from order alignment to cognitive distribution alignment. Second, we design a behavior-aware noisy item generation strategy that jointly optimizes collaborative signals and gradient signals. This ensures both semantic coherence and statistical stealth while effectively promoting target item rankings. Extensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods in both attack success rate and evasion rate, validating the value of integrating cognitive modeling and behavioral consistency for secure recommender systems.",
        "entry_id": "http://arxiv.org/abs/2602.10633v1",
        "pub_date": "2026-02-11",
        "translated_summary": "随着序列化推荐系统在电商及其他领域的广泛部署，其黑箱接口带来了安全隐患：模型易被窃取并遭对手操控。现有的黑箱窃取攻击大多依赖硬标签或成对学习，往往忽视排序位置的重要性，导致知识迁移不完整。此外，纯梯度方法生成的对抗序列缺乏与真实用户行为的语义一致性，易于被检测。为克服这些局限，本文提出了一种双重增强的攻击框架。首先，借鉴首因效应与位置偏差，我们引入一种认知分布驱动的抽取机制，将离散排名映射为带位置衰减的连续值分布，从而由序列对齐升级为认知分布对齐。其次，我们设计了行为感知噪声商品生成策略，协同优化协同信号与梯度信号，在确保语义连贯与统计隐形的同时，有效提升目标商品的排名。在多个数据集上的大量实验表明，我们的方法在攻击成功率与逃逸率上均显著优于现有方法，验证了将认知建模与行为一致性融入推荐安全框架的价值。"
    },
    {
        "title": "S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage",
        "summary": "Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\\% lift in GMV in online A/B tests, without requiring real-time LLM inference.",
        "entry_id": "http://arxiv.org/abs/2602.10606v1",
        "pub_date": "2026-02-11",
        "translated_summary": "生成式推荐模型通过序列生成端到端地输出物品，然而，仅依赖行为日志进行训练往往只能提供与用户真实意图弱相关的监督信号。大型语言模型（LLM）蕴含丰富的语义先验，能够弥补这一不足，但直接将其应用于工业级推荐面临两大障碍：一是语义信号可能与平台商业目标相冲突，二是大规模使用 LLM 推理成本过高。\n\n本文提出语义感知框架 S-GRec，其核心思想是在训练阶段将轻量级在线生成器与离线 LLM 语义判别器解耦。S-GRec 设计了双阶段个性化语义判别器（PSJ）：首先为每条候选结果提供可解释的细粒度理由，再通过成对反馈学习与用户条件相关的语义聚合方式，输出稳定的语义奖励，从而缓解仅由日志监督带来的弱监督问题。\n\n为避免语义监督与商业目标偏离，我们进一步提出非对称优势策略优化（A2PO）。该方法以商业价值（例如 eCPM）为锚定目标，在 LLM-产生的语义优势与商业奖励方向一致时才进行注入，实现“双赢”指导。\n\n在公开基准和实际大规模生产环境中的大量实验表明，S-GRec 不仅有效而且具备良好可扩展性：线下评估指标显著提升，线上 A/B 测试取得 CTR 显著提高以及 1.19 % GMV 增长，且无需在线实时调用 LLM。"
    },
    {
        "title": "Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking",
        "summary": "E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present \\textbf{Campaign-2-PT-RAG}, a scalable label generation framework that constructs user--campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign--PT mappings, show that our LLM-assisted approach generates high-quality labels with 78--90% precision while maintaining over 99% recall.",
        "entry_id": "http://arxiv.org/abs/2602.10577v1",
        "pub_date": "2026-02-11",
        "translated_summary": "电子商务中的营销活动排名模型需要大规模的显式训练标签 —— 用于标识哪些用户是因为该场活动而下单的。然而，生成这些标签极为困难：营销活动通常采用富有创意、强调主题的语言，文字与最终购买的商品之间缺乏直接对应关系。在缺少清晰商品归属信息的情况下，用于活动优化的监督学习效果受限。本文提出 **Campaign-2-PT-RAG**，一个可扩展的标签生成框架，它通过推断每场活动推广的产品类型（Product Types, PTs）来构建“用户-活动”级购买正标签。框架首先利用大语言模型（LLMs）解析活动创意内容，捕获其隐含意图；随后通过语义搜索在平台商品分类体系中检索候选 PTs；再由结构化的 LLM 分类器评估每个 PT 与活动的语义相关度，从而产出每场活动独有的“产品覆盖集合”。当用户的后续购买落于此集合对应的 PT 时，即生成下游排序模型可用的大规模正向训练标签。通过将模糊的归因问题重新定义为可解的语义对齐任务，该方法能够为工业生产环境提供可扩展且高度一致的训练监督。在内部真实数据与合成数据集上的实验，并以人工标注的活动-PT 映射为基准，结果显示：LLM 辅助方法可产出高质量标签，精确度达 78–90%，召回率保持在 99% 以上。"
    },
    {
        "title": "Boundary-Aware Multi-Behavior Dynamic Graph Transformer for Sequential Recommendation",
        "summary": "In the landscape of contemporary recommender systems, user-item interactions are inherently dynamic and sequential, often characterized by various behaviors. Prior research has explored the modeling of user preferences through sequential interactions and the user-item interaction graph, utilizing advanced techniques such as graph neural networks and transformer-based architectures. However, these methods typically fall short in simultaneously accounting for the dynamic nature of graph topologies and the sequential pattern of interactions in user preference models. Moreover, they often fail to adequately capture the multiple user behavior boundaries during model optimization. To tackle these challenges, we introduce a boundary-aware Multi-Behavioral Dynamic Graph Transformer (MB-DGT) model that dynamically refines the graph structure to reflect the evolving patterns of user behaviors and interactions. Our model involves a transformer-based dynamic graph aggregator for user preference modeling, which assimilates the changing graph structure and the sequence of user behaviors. This integration yields a more comprehensive and dynamic representation of user preferences. For model optimization, we implement a user-specific multi-behavior loss function that delineates the interest boundaries among different behaviors, thereby enriching the personalized learning of user preferences. Comprehensive experiments across three datasets indicate that our model consistently delivers remarkable recommendation performance.",
        "entry_id": "http://arxiv.org/abs/2602.10493v1",
        "pub_date": "2026-02-11",
        "translated_summary": "在当代推荐系统领域中，用户与项目的交互天然呈现动态与序列特征，且伴随多样化的用户行为。已有研究尝试利用序列交互与用户-项目关系图对用户偏好建模，并广泛采用图神经网络与 Transformer 架构等先进技术。然而，这些方法往往难以在同一框架内同时刻画图拓扑结构的动态演进以及交互序列模式，且在模型优化阶段未能有效区分用户多种行为之间的边界。为应对上述挑战，我们提出了一种边界感知的多行为动态图 Transformer（MB-DGT）模型。该模型能够依用户行为与交互的演化过程动态精炼图结构：借助基于 Transformer 的动态图聚合器，将改变的图结构与用户行为序列进行融合，使偏好表征更全面、动态。针对优化过程，我们设计用户专属的多行为损失函数，明确划分不同行为间的兴趣边界，进一步提升个性化偏好学习。在三个数据集的充分实验表明，MB-DGT 始终实现卓越的推荐性能。"
    },
    {
        "title": "ChainRec: An Agentic Recommender Learning to Route Tool Chains for Diverse and Evolving Interests",
        "summary": "Large language models (LLMs) are increasingly integrated into recommender systems, motivating recent interest in agentic and reasoning-based recommendation. However, most existing approaches still rely on fixed workflows, applying the same reasoning procedure across diverse recommendation scenarios. In practice, user contexts vary substantially-for example, in cold-start settings or during interest shifts, so an agent should adaptively decide what evidence to gather next rather than following a scripted process. To address this, we propose ChainRec, an agentic recommender that uses a planner to dynamically select reasoning tools. ChainRec builds a standardized Tool Agent Library from expert trajectories. It then trains a planner using supervised fine-tuning and preference optimization to dynamically select tools, decide their order, and determine when to stop. Experiments on AgentRecBench across Amazon, Yelp, and Goodreads show that ChainRec consistently improves Avg HR@{1,3,5} over strong baselines, with especially notable gains in cold-start and evolving-interest scenarios. Ablation studies further validate the importance of tool standardization and preference-optimized planning.",
        "entry_id": "http://arxiv.org/abs/2602.10490v1",
        "pub_date": "2026-02-11",
        "translated_summary": "大语言模型（LLM）正日益深入推荐系统，催生了学界和业界对“基于智能体与推理性推荐”的兴趣。然而，现有方法大多仍固守固定工作流，将所有场景强行塞进统一推理流程；真实场景中，用户情境千差万别——冷启动或兴趣迁移发生时尤为明显——因此智能体理应动态决定“接下来收集何种证据”，而非死板遵循脚本。\n\n针对这一问题，我们提出 ChainRec：一个由规划器驱动的智能体推荐系统，可实时选择所需推理工具。ChainRec 首先从专家轨迹中提炼并构建一套标准化的“工具—智能体”库；随后以监督式微调与偏好优化的方式训练规划器，使其动态决定：  \n1. 调用哪些工具；  \n2. 工具调用顺序；  \n3. 何时停止推理。\n\n在 AgentRecBench 上，基于 Amazon、Yelp 与 Goodreads 的实验表明，ChainRec 在 Avg HR@{1,3,5} 指标上显著优于一众强基线，尤其在冷启动与兴趣演化的场景下提升更为突出。系列消融实验进一步证实：工具标准化与偏好优化式规划均为关键成功因子。"
    },
    {
        "title": "Compute Only Once: UG-Separation for Efficient Large Recommendation Models",
        "summary": "Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems.",
        "entry_id": "http://arxiv.org/abs/2602.10455v1",
        "pub_date": "2026-02-11",
        "translated_summary": "受规模定律驱动，推荐系统日益依赖大规模模型来捕捉复杂特征交互与用户行为，但这一趋势也带来了高昂的训练与推理成本。尽管长序列模型（如 LONGER）可以利用 KV 缓存重用用户端计算，此类重用对于深度耦合物征与用户特征信息的稠密特征交互架构（如 RankMixer）却十分困难。本文提出一种新颖框架——用户-群组解耦（User-Group Separation，UG-Sep），首次在稠密交互模型中实现用户端计算的可复用。UG-Sep 引入掩码机制，在 token-mixing 层内显式分离用户端与商品端的信息流，确保部分 token 在各层中始终保持纯用户表征，从而其对应计算可在多个样本间复用，大幅降低冗余推理开销。为弥补掩码可能带来的表达力损失，我们进一步设计了信息补偿策略，能够自适应地重建被压制的用户-商品交互。此外，UG-Sep 显著减少用户端 FLOPs，使推理瓶颈由计算变为内存带宽；我们集成 W8A16（权重 8 位、激活 16 位）的仅权重量化，进一步缓解带宽瓶颈并加速推理。我们在字节跳动进行大量离线评测和线上长期 A/B 实验，结果显示 UG-Sep 在不损害线上用户体验与商业指标的前提下，在包括信息流推荐与广告系统在内的多个业务场景中将推理延迟最高降低 20%。"
    },
    {
        "title": "End-to-End Semantic ID Generation for Generative Advertisement Recommendation",
        "summary": "Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline.",
        "entry_id": "http://arxiv.org/abs/2602.10445v1",
        "pub_date": "2026-02-11",
        "translated_summary": "生成式推荐（GR）通过“下一 token 预测”范式取得了卓越表现，其关键是借助语义标识符（SID）将大规模物品离散化为序列。现有方法普遍采用残差量化（RQ）生成 SID：先将物品编码为嵌入，再量化为离散 SID。然而，该范式存在两大固有限制：1）由两阶段级联压缩导致的目标失配与语义退化；2）RQ 结构固有的误差累积。\n\n为突破上述瓶颈，本文提出 UniSID——一种面向广告场景的统一 SID 生成框架。其核心思想是在原始广告数据上端到端地联合优化嵌入与 SID，让语义信息直接流入离散空间，从而根本解决级联压缩缺陷。为细粒度语义，我们进一步采用多粒度对比学习，在不同 SID 层级内对齐不同广告；并通过“摘要式广告重构”机制，引导 SID 挖掘上下文未显式呈现的高层语义。大量实验表明，UniSID 在所有指标上均持续超越现有最佳 SID 生成方法，在下游广告场景中相较于最强基线最大提升 4.62% 的点击率（Hit Rate）。"
    },
    {
        "title": "Chamfer-Linkage for Hierarchical Agglomerative Clustering",
        "summary": "Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.\n  In this paper, we propose \\emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.",
        "entry_id": "http://arxiv.org/abs/2602.10444v1",
        "pub_date": "2026-02-11",
        "translated_summary": "层级聚合聚类（HAC）是一种广泛使用的聚类方法，通过反复合并距离最近的两簇来实现，簇间距离由某个链接函数决定。与众多聚类方法不同，HAC 并不针对单一显式全局目标进行优化，因此聚类质量主要依赖经验评估，链接函数的选择在实际应用中尤为关键。然而，经典的单链接、平均链接及 Ward 方法等在实际数据集上表现出高度不稳定，难以始终产生高质量的聚类结果。\n\n本文提出 **Chamfer-linkage**：一种新的链接函数，利用在机器学习和计算机视觉领域广受采用的 Chamfer 距离来衡量两个簇之间的距离。我们论证 Chamfer-linkage 满足了几种理想的概念表征性质，而现有流行测度往往难以同时兼顾。理论上，我们证明了基于 Chamfer-linkage 的 HAC 可在 O(n²) 时间内实现，与经典链接函数的计算复杂度相同。实验上，在多样化的数据集中，Chamfer-linkage 均比平均链接、Ward 方法等经典方案稳定地获得更高质量的聚类效果。综上，Chamfer-linkage 可作为一种“即插即用”的实用替代方案，为层级聚类在理论和实践层面扩展了工具集。"
    },
    {
        "title": "GeoGR: A Generative Retrieval Framework for Spatio-Temporal Aware POI Recommendation",
        "summary": "Next Point-of-Interest (POI) prediction is a fundamental task in location-based services, especially critical for large-scale navigation platforms like AMAP that serve billions of users across diverse lifestyle scenarios. While recent POI recommendation approaches based on SIDs have achieved promising, they struggle in complex, sparse real-world environments due to two key limitations: (1) inadequate modeling of high-quality SIDs that capture cross-category spatio-temporal collaborative relationships, and (2) poor alignment between large language models (LLMs) and the POI recommendation task. To this end, we propose GeoGR, a geographic generative recommendation framework tailored for navigation-based LBS like AMAP, which perceives users' contextual state changes and enables intent-aware POI recommendation. GeoGR features a two-stage design: (i) a geo-aware SID tokenization pipeline that explicitly learns spatio-temporal collaborative semantic representations via geographically constrained co-visited POI pairs, contrastive learning, and iterative refinement; and (ii) a multi-stage LLM training strategy that aligns non-native SID tokens through multiple template-based continued pre-training(CPT) and enables autoregressive POI generation via supervised fine-tuning(SFT). Extensive experiments on multiple real-world datasets demonstrate GeoGR's superiority over state-of-the-art baselines. Moreover, deployment on the AMAP platform, serving millions of users with multiple online metrics boosting, confirms its practical effectiveness and scalability in production.",
        "entry_id": "http://arxiv.org/abs/2602.10411v1",
        "pub_date": "2026-02-11",
        "translated_summary": "下一个兴趣点（POI）预测是位置服务中的核心任务，对 AMAP 这类服务于十亿级用户、覆盖多元生活场景的大型导航平台尤为重要。尽管近期基于 POI 序列（SID, session-based interaction data）的 POI 推荐方法取得了不错效果，但在复杂且稀疏的实际场景下面临两大瓶颈：① 难以生成高质量 SID，以捕获跨类别的时空协同关系；② 大语言模型（LLM）与 POI 推荐任务之间存在严重的对齐不足。为此，我们提出 GeoGR——面向 AMAP 等导航型 LBS 的地理生成式推荐框架，可感知用户上下文状态的动态变化并实现意图感知的 POI 推荐。\n\nGeoGR 采用双阶段设计：\n(i) 地理感知 SID 分词管线：通过“在地理约束下共访问 POI 对 → 对比学习 → 迭代精修”的机制，显式学习时空协同语义表征；  \n(ii) 多阶段 LLM 训练策略：借助多套模板化的继续预训练（CPT）将非原生 SID token 对齐，再通过监督微调（SFT）实现自回归式 POI 生成。\n\n在多个真实数据集的实验表明，GeoGR 显著优于当前最佳基线。将其部署在 AMAP 平台、服务千万级用户的线上验证进一步证明：该方法在实际生产环境中兼具有效性与可扩展性，多项核心指标均获得显著提升。"
    },
    {
        "title": "Single-Turn LLM Reformulation Powered Multi-Stage Hybrid Re-Ranking for Tip-of-the-Tongue Known-Item Retrieval",
        "summary": "Retrieving known items from vague descriptions, Tip-of-the-Tongue (ToT) retrieval, remains a significant challenge. We propose using a single call to a generic 8B-parameter LLM for query reformulation, bridging the gap between ill-formed ToT queries and specific information needs. This method is particularly effective where standard Pseudo-Relevance Feedback fails due to poor initial recall. Crucially, our LLM is not fine-tuned for ToT or specific domains, demonstrating that gains stem from our prompting strategy rather than model specialization. Rewritten queries feed a multi-stage pipeline: sparse retrieval (BM25), dense/late-interaction reranking (Contriever, E5-large-v2, ColBERTv2), monoT5 cross-encoding, and list-wise reranking (Qwen 2.5 72B). Experiments on 2025 TREC-ToT datasets show that while raw queries yield poor performance, our lightweight pre-retrieval transformation improves Recall by 20.61%. Subsequent reranking improves nDCG@10 by 33.88%, MRR by 29.92%, and MAP@10 by 29.98%, offering a cost-effective intervention that unlocks the potential of downstream rankers. Code and data: https://github.com/debayan1405/TREC-TOT-2025",
        "entry_id": "http://arxiv.org/abs/2602.10321v1",
        "pub_date": "2026-02-10",
        "translated_summary": "从含糊描述中检索已知目标（即“舌尖现象” Tip-of-the-Tongue, ToT）依旧是一项艰巨任务。我们提出仅用一次普通八百亿参数 LLM 调用来实时改写查询，弥合粗陋的 ToT 问句与确切信息需求之间的鸿沟。当标准伪相关反馈因初召回极低而失效时，该方法尤显成效。关键亮点在于，LLM 既未针对 ToT 任务也未针对特定领域微调，所有提升完全归功于提示策略，而非模型专精。优化后的查询注入多阶段流水线：稀疏检索 (BM25)、稠密交互及迟交互重排 (Contriever、E5-large-v2、ColBERTv2)、monoT5 交叉编码，以及基于 Qwen 2.5 72B 的列表级重排序。在 2025 TREC-ToT 数据集上的实验表明：未经改写的查询表现极差，而我们的轻量级「检索前」重述即可使召回率提升 20.61%；后续重排环节进一步将 nDCG@10 提高 33.88%，MRR 提升 29.92%，MAP@10 提升 29.98%，为下游排序器解锁性能潜力的同时，保持了低成本优势。代码及数据：https://github.com/debayan1405/TREC-TOT-2025"
    },
    {
        "title": "ECHO: An Open Research Platform for Evaluation of Chat, Human Behavior, and Outcomes",
        "summary": "ECHO (Evaluation of Chat, Human behavior, and Outcomes) is an open research platform designed to support reproducible, mixed-method studies of human interaction with both conversational AI systems and Web search engines. It enables researchers from varying disciplines to orchestrate end-to-end experimental workflows that integrate consent and background surveys, chat-based and search-based information-seeking sessions, writing or judgment tasks, and pre- and post-task evaluations within a unified, low-coding-load framework. ECHO logs fine-grained interaction traces and participant responses, and exports structured datasets for downstream analysis. By supporting both chat and search alongside flexible evaluation instruments, ECHO lowers technical barriers for studying learning, decision making, and user experience across different information access paradigms, empowering researchers from information retrieval, HCI, and the social sciences to conduct scalable and reproducible human-centered AI evaluations.",
        "entry_id": "http://arxiv.org/abs/2602.10295v1",
        "pub_date": "2026-02-10",
        "translated_summary": "ECHO（Chat、人类行为与结果评估平台）是一个开放的研究平台，专门用于支持可重复、混合方法的人机对话 AI 与 Web 搜索引擎交互研究。它以统一、低代码框架整合了以下端到端实验流程：知情同意与背景问卷调查、基于聊天或搜索的信息获取会话、写作或判断类任务，以及任务前后评估。ECHO 可细粒度记录交互轨迹与参与者回应，并能导出结构化数据供后续分析。通过同时在对话和搜索两种信息获取范式下提供灵活的评估工具，ECHO 显著降低了信息检索、人机交互与社会科学等领域研究者在探讨学习、决策与用户体验过程中的技术门槛，助力其开展可扩展、可复现、以人为本的 AI 评估。"
    },
    {
        "title": "MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation",
        "summary": "Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is challenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning to aggregate dispersed evidence across pages. To address these challenges, we are motivated to adopt a query-centric formulation that projects cross-modal and cross-page information into a unified query representation space, with queries acting as abstract semantic surrogates for heterogeneous multimodal content. In this paper, we propose a Multimodal Long-Context Document Retrieval Augmented Generation (MLDocRAG) framework that leverages a Multimodal Chunk-Query Graph (MCQG) to organize multimodal document content around semantically rich, answerable queries. MCQG is constructed via a multimodal document expansion process that generates fine-grained queries from heterogeneous document chunks and links them to their corresponding content across modalities and pages. This graph-based structure enables selective, query-centric retrieval and structured evidence aggregation, thereby enhancing grounding and coherence in long-context multimodal question answering. Experiments on datasets MMLongBench-Doc and LongDocURL demonstrate that MLDocRAG consistently improves retrieval quality and answer accuracy, demonstrating its effectiveness for long-context multimodal understanding.",
        "entry_id": "http://arxiv.org/abs/2602.10271v1",
        "pub_date": "2026-02-10",
        "translated_summary": "理解包含段落、图表和表格等多模态片段的长上下文文件颇具挑战，主要体现在两个维度：(1) 不同模态内容之间存在跨模态异构性，难以定位跨模态关联信息；(2) 需进行跨页推理以整合分散于多页的证据。为了解决这些难题，我们提出一种以查询为中心的范式，将跨模态、跨页的信息映射到一个统一的查询表示空间，并以查询作为异构多模态内容的抽象语义代理。本文设计了“多模态长文本文档检索增强生成框架”（MLDocRAG），利用“多模态片段-查询图”（MCQG）将多模态文档内容围绕具有丰富语义且可被回答的查询进行组织。通过多模态文档扩展流程，系统从异构片段中生成细粒度查询，并以图形式将查询与其在跨模态、跨页中的对应内容进行关联。该图结构支持基于查询的选择性检索与结构化证据聚合，从而增强长上下文多模态问答的上下文支撑和连贯性。在 MMLongBench-Doc 和 LongDocURL 数据集的实验结果显示，MLDocRAG 显著提升了检索质量和答案准确率，充分验证了其在长上下文多模态理解中的有效性。"
    },
    {
        "title": "JAG: Joint Attribute Graphs for Filtered Nearest Neighbor Search",
        "summary": "Despite filtered nearest neighbor search being a fundamental task in modern vector search systems, the performance of existing algorithms is highly sensitive to query selectivity and filter type. In particular, existing solutions excel either at specific filter categories (e.g., label equality) or within narrow selectivity bands (e.g., pre-filtering for low selectivity) and are therefore a poor fit for practical deployments that demand generalization to new filter types and unknown query selectivities. In this paper, we propose JAG (Joint Attribute Graphs), a graph-based algorithm designed to deliver robust performance across the entire selectivity spectrum and support diverse filter types. Our key innovation is the introduction of attribute and filter distances, which transform binary filter constraints into continuous navigational guidance. By constructing a proximity graph that jointly optimizes for both vector similarity and attribute proximity, JAG prevents navigational dead-ends and allows JAG to consistently outperform prior graph-based filtered nearest neighbor search methods. Our experimental results across five datasets and four filter types (Label, Range, Subset, Boolean) demonstrate that JAG significantly outperforms existing state-of-the-art baselines in both throughput and recall robustness.",
        "entry_id": "http://arxiv.org/abs/2602.10258v1",
        "pub_date": "2026-02-10",
        "translated_summary": "尽管带过滤条件的最近邻搜索是现代向量检索系统中的基础任务，但现有算法的性能对查询选择度和过滤类型极为敏感：它们要么仅对特定类型的过滤（如标签等值）表现优异，要么只在狭窄的选择度区间内（如低选择度下的前置过滤）领先，难以满足实战场景中对新型过滤类型和未知选择度的泛化需求。\n\n本文提出 JAG（Joint Attribute Graphs），一种图算法，旨在在完整选择度范围内稳健运行，并支持多样化的过滤类型。其核心创新在于引入“属性距离”与“过滤距离”，将二元的过滤约束转化为连续的导航指引。JAG 通过构建一幅同时优化向量相似性与属性邻近性的联合邻近图，避免导航死胡同，从而持续超越此前基于图的带过滤最近邻搜索方法。\n\n在五个数据集、四类过滤器（标签、范围、子集、布尔）上的实验表明，无论在吞吐量还是召回率稳定性方面，JAG 均显著优于现有最优基线。"
    },
    {
        "title": "Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems",
        "summary": "Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.",
        "entry_id": "http://arxiv.org/abs/2602.12041v1",
        "pub_date": "2026-02-12",
        "translated_summary": "高效建模高阶特征交互是点击率／转化率预测的核心挑战。现代工业推荐系统大多以深度推荐模型为基础，其中交互网络在提升预测性能与系统效率方面起决定性作用。然而，现有交互模块往往难以同时兼顾强大的表达能力、高计算效率与良好可扩展性，导致在严格的生产约束下扩张模型时投入产出比有限。\n\n本文提出 MLCC，一种结构化的特征交互架构，通过「层次压缩 + 动态组合」的方式组织特征交叉，既能高效捕获高阶特征依赖，又保持可接受的计算复杂度。进一步，我们引入 MC-MLCC（多通道扩展版），将特征交互拆分到并行子空间，实现高效的横向扩展：在提升表达能力的同时显著抑制了参数量增长。\n\n在三个公开基准与一套大规模工业数据集上的实验显示，所提模型相较强劲的 DLRM 风格基线在 AUC 上最高提升 0.52，并在性能可比的情况下最多将参数量与计算量（FLOPs）减少达 26 倍。全面的扩展性分析表明，模型在嵌入维度、头数以及通道数上的扩展行为稳定且可预测，其中基于通道的扩展方式在实际效率上远超传统的「嵌入维度膨胀」。最后，真实广告平台上的在线 A/B 测试验证了方法的落地效果，该架构已在 B 站广告系统中在严格的时延与资源约束下大规模部署。"
    },
    {
        "title": "IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval",
        "summary": "Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \\textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.",
        "entry_id": "http://arxiv.org/abs/2602.11941v1",
        "pub_date": "2026-02-12",
        "translated_summary": "近年来，多模态信息检索依托深度预训练模型日益增强的跨模态表征能力取得了显著进展。音乐信息检索（MIR）领域的质量尤为大幅提升，其神经表征甚至被集成到了日常产品中。然而，目前仍缺乏高质量的基准数据集来系统评估音乐检索性能。为解决这一问题，本文推出 **IncompeBench**——一套经精细标注的基准。该基准包含 1,574 段经授权、高质量的音乐片段，500 条多样化的查询示例，以及超过 125,000 条单独人工相关性判断。标注过程通过多级流水线完成，确保了人类标注者与生成数据的高度一致性。这些数据集已公开发布：https://huggingface.co/datasets/mixedbread-ai/incompebench-strict 与 https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient；标注提示词则存放在 https://github.com/mixedbread-ai/incompebench-programs。"
    },
    {
        "title": "Efficient Crawling for Scalable Web Data Acquisition (Extended Version)",
        "summary": "Journalistic fact-checking, as well as social or economic research, require analyzing high-quality statistics datasets (SDs, in short). However, retrieving SD corpora at scale may be hard, inefficient, or impossible, depending on how they are published online. To improve open statistics data accessibility, we present a focused Web crawling algorithm that retrieves as many targets, i.e., resources of certain types, as possible, from a given website, in an efficient and scalable way, by crawling (much) less than the full website. We show that optimally solving this problem is intractable, and propose an approach based on reinforcement learning, namely using sleeping bandits. We propose SB-CLASSIFIER, a crawler that efficiently learns which hyperlinks lead to pages that link to many targets, based on the paths leading to the links in their enclosing webpages. Our experiments on websites with millions of webpages show that our crawler is highly efficient, delivering high fractions of a site's targets while crawling only a small part.",
        "entry_id": "http://arxiv.org/abs/2602.11874v1",
        "pub_date": "2026-02-12",
        "translated_summary": "新闻事实核查及社会/经济学研究均依赖于高质量统计数据集（简称SD）。然而，依据其在线发布方式的不同，在大规模获取SD语料库时往往会遭遇困难、低效甚至完全不可行。为提升开放统计数据的可访问性，本文提出一种聚焦式网络爬虫算法，可在给定的整个网站中，以高效且可扩展的方式检索尽可能多的目标（即特定类型的资源），且爬取量远低于全站规模。\n\n我们证明了最优解决该问题是不可行的，并基于强化学习提出一种解决方案——借助sleeping bandits模型。我们设计了SB-CLASSIFIER爬虫，该爬虫依据网页内部通往链接的路径，自主学习哪些超链接指向包含大量目标的页面。在拥有数百万页面的真实网站上的实验表明，该爬虫效率极高，仅爬取了网站的一小部分即可获得极高比例的目标。"
    },
    {
        "title": "Improving Neural Retrieval with Attribution-Guided Query Rewriting",
        "summary": "Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.",
        "entry_id": "http://arxiv.org/abs/2602.11841v1",
        "pub_date": "2026-02-12",
        "translated_summary": "神经检索器虽然有效，却也十分脆弱：只要查询表达含糊或存在歧义，即使存在相关文档，排名也会出错。现有方法仅部分缓解了这种脆弱性：大型语言模型（LLM）在改写查询时不获得检索器的反馈；可解释性模型能指出具有误导性的词元，但仅用于事后分析。本文将二者闭环整合，提出一种基于归因指导的查询改写方法：利用粒度为词元的解释信息指导重写过程。具体而言，我们为每个原查询计算来自检索器的梯度词元归因得分；随后，将该分数作为软提示，输入到结构化的 LLM prompt 中，提示模型在保持原意图的前提下修正弱化或误导性的查询成分。在 BEIR 基准上的实验显示，该方法生成的改写显著优于所有强力基线，且在具有隐含或歧义信息需求的场景下收益更为突出。"
    },
    {
        "title": "ULTRA:Urdu Language Transformer-based Recommendation Architecture",
        "summary": "Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.",
        "entry_id": "http://arxiv.org/abs/2602.11836v1",
        "pub_date": "2026-02-12",
        "translated_summary": "作为一名低资源语言，乌尔都语缺乏高效的语义内容推荐系统，尤其在个性化新闻检索领域表现明显。现有方法多依赖词汇匹配或与语言无关的技术，难以捕捉用户真实语义意图，并在查询长度及信息需求各异的情况下表现不佳。这一缺陷导致乌尔都语内容推荐的相关性降低、自适应能力受限。本文提出 ULTRA（Urdu Language Transformer-based Recommendation Architecture），一种旨在克服这些挑战的自适应语义推荐框架。ULTRA 引入双嵌入架构，并配有查询长度感知路由机制，可动态区分简短的意图型查询与较长的上下文丰富查询。依据阈值驱动的决策过程，用户查询被路由至专为标题/标题级表示或全内容/文档级语义表示优化的两条语义流水线，在检索时保证语义粒度的恰当性。该系统利用基于 Transformer 的嵌入与优化后的池化策略，突破表层关键词匹配限制，实现上下文感知的相似度搜索。大规模乌尔都语新闻语料上的大量实验表明，该架构在多种查询类型上均持续提升了推荐相关性；与单流水线基线相比，精确率提升逾 90%，凸显了查询自适应语义对齐在低资源语言中的有效性。研究结果将 ULTRA 确立为一种稳健且可泛化的内容推荐架构，并为低资源语境下的语义检索系统提供了可实践的设计借鉴。"
    },
    {
        "title": "Reliable and Private Anonymous Routing for Satellite Constellations",
        "summary": "Shared, dynamic network infrastructures, such as dual-use LEO satellite constellations, pose critical threats to metadata privacy, particularly for state actors operating in mixed-trust environments. This work proposes an enhanced anonymity architecture, evolving the Loopix mix-network, to provide robust security and reliability in these volatile topologies. We introduce three primary contributions: (1) A multi-path transport protocol utilizing $(n, k)$ erasure codes, which is demonstrated to counteract the high link volatility and intermittent connectivity that renders standard mix-networks unreliable. (2) The integration of a computationally efficient Private Information Retrieval (PIR) protocol during route discovery. (3) The introduction of adaptive, centrality-based delay strategies that efficiently mitigate the inherent topological bias of LEO networks, providing a superior anonymity-to-latency trade-off. This mechanism provably prevents metadata leakage at the user-provider directory, mitigating profiling and correlation attacks. We validate this architecture via high-fidelity, packet-level simulations of a LEO constellation. Empirical results show our multi-path transport achieves near-zero message loss, establishing a quantifiable trade-off between reliability and bandwidth overhead. Furthermore, microbenchmarks of the PIR protocol quantify its computational and latency overheads, confirming its feasibility for practical deployment. This work provides a validated blueprint for deployable high-anonymity communication systems, demonstrating the viability of securely multiplexing sensitive operations within large-scale commercial network infrastructures.",
        "entry_id": "http://arxiv.org/abs/2602.11764v1",
        "pub_date": "2026-02-12",
        "translated_summary": "共享的动态网络基础设施（如两用 LEO 卫星星座）已对元数据隐私构成关键威胁，尤其是在混合信任环境中运作的国家行为者尤为脆弱。本文提出一种增强型匿名架构，通过演化 Loopix 混合网络，以在如此不稳定的拓扑中提供强健的安全与可靠性保障。我们的三大核心贡献如下：\n\n(1) 一个采用 $(n, k)$ 纠删码的多路径传输协议，可通过冗余抵消高链路波动与频繁断线导致标准混合网络不可靠的问题，实验验证其几乎零报文丢失。\n\n(2) 在路由发现阶段集成计算高效的私有信息检索（PIR）协议。\n\n(3) 引入基于中心性的自适应延迟策略，显著缓解 LEO 网络的固有拓扑偏差，实现更优的匿名—延迟权衡；该机制被证明可在用户—服务商目录端防止元数据泄露，抵御画像与流量关联攻击。\n\n我们通过对 LEO 星座的高保真数据包级仿真验证该架构。实证结果表明，多路径传输的报文丢失率逼近零，并形成可靠性 vs. 带宽开销的可量化权衡；对 PIR 协议的微基准测试则量化其计算与延迟开销，确认实际部署的可行性。本工作为可部署的高匿名通信系统提供经验证的蓝图，展示在大型商用网络基础设施内安全地多路复用敏感操作的可行性。"
    },
    {
        "title": "Uncertainty-aware Generative Recommendation",
        "summary": "Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.",
        "entry_id": "http://arxiv.org/abs/2602.11719v1",
        "pub_date": "2026-02-12",
        "translated_summary": "生成式推荐已成为一种变革性新范式，它将推荐任务重新表述为端到端的自回归序列生成问题。然而，现有偏好优化方法几乎一成不变地依赖二元结果正确性，陷入系统性局限——我们称之为“不确定性失盲”。其核心在于忽视模型自身的生成置信度、样本学习难度的差异，以及缺乏显式置信表达能力，直接导致训练动态不稳，决策风险难以量化。本文提出“不确定性感知的生成式推荐”（UGR），一个统一框架，将不确定性视为自适应优化的关键信号。UGR 融合三大机制：1) 不确定性加权奖励，惩罚高置信错误；2) 难度感知优化动态，避免过早收敛；3) 显式置信对齐，使模型具备置信表达能力。大量实验表明，UGR 不仅带来更优的推荐性能，还能从根源上稳定训练，避免常规方法常见的大幅性能下滑；学到的置信值进一步支持下游可风险感知的可靠应用。"
    },
    {
        "title": "EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation",
        "summary": "Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.",
        "entry_id": "http://arxiv.org/abs/2602.11680v1",
        "pub_date": "2026-02-12",
        "translated_summary": "捆绑推荐的目的是为用户推送一整套商品，以便整体消费。现有模型主要依赖已观察到的用户–捆绑交互，难以探索不断出现的新捆绑，这对当前方法提出了严峻表征挑战：它们通常把每个捆绑视为孤立实例，而忽略了借助热门商品对用户–商品（UI）与捆绑–商品（BI）关系的深度融合。为缓解这一问题，本文提出面向冷启动捆绑推荐的多视角对比学习框架 EpicCBR。具体而言，该框架精准挖掘并利用商品关系构建用户画像，识别可能关注捆绑的潜在用户；同时设计一种基于流行度的方法，通过历史捆绑信息与用户偏好刻画新捆绑特征。为确保模型在冷启动与热启动场景下皆具鲁棒性，我们又构造了一个可融合上述情境的多视图图对比学习框架，提升泛化能力。在三个主流基准数据集上的大量实验表明，EpicCBR 相比当前最优方法性能提升最高达 387%，充分验证了其在冷启动场景下的显著优势。代码与数据集已开源：https://github.com/alexlovecoding/EpicCBR。"
    },
    {
        "title": "IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation",
        "summary": "Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: \"when to depart\", \"how to travel\", \"where to go\", and \"what needs arise via the route\". However, current research is limited by fragmented datasets that focus merely on next POI recommendation (\"where to go\"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.",
        "entry_id": "http://arxiv.org/abs/2602.11664v1",
        "pub_date": "2026-02-12",
        "translated_summary": "下一站兴趣点（POI）推荐对现代出行与基于位置的服务至关重要。为实现流畅的用户体验，模型必须全面理解出行全程的四大要素——“何时出发”“如何出行”“去往何处”以及“沿途需求”。然而，受限于现有数据集的碎片化，研究仅聚焦于“去哪里”，忽视了出发时间、出行方式以及行程中的情境需求；且数据规模有限，难以准确评估模型性能。  \n\n为此，我们推出首个大规模公开综合出行推荐数据集 IntTravel，涵盖 1.63 亿用户在 730 万 POI 上的 41 亿次交互记录。依托该数据集，我们进一步提出端到端、仅解码器的生成式多任务推荐框架。该框架通过信息保持、信息选择与因子化设计，在任务协作与专业化区分间取得平衡，带来显著性能提升。其通用性也得到验证——在 IntTravel 与非出行基准线上均达到 SOTA 效果。  \n\n目前，IntTravel 已部署于高德地图，服务数亿用户，CTR 提升达 1.09%。数据与代码已开源：https://github.com/AMAP-ML/IntTravel"
    },
    {
        "title": "Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts",
        "summary": "Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.",
        "entry_id": "http://arxiv.org/abs/2602.11622v1",
        "pub_date": "2026-02-12",
        "translated_summary": "零样本图异常检测（GAD）近年来备受关注，然而不同图在结构、特征及异常模式上的异质性，使得现有单一图神经网络方法难以充分刻画多样化的异常机制。为此，专家混合（MoE）架构通过融合具有互补归纳偏置的多种 GNN 专家，为这一问题提供了有前景的新范式；但在零样本 GAD 中，分布偏移严重限制了其路由能力，带来两大关键挑战。首先，不同图中的节点语义差异极大，仅凭原始特征执行路由易导致有偏或亚优的专家分配。其次，异常图往往存在显著的分布差异，现有路由设计难以捕捉跨越训练图的领域不变路由准则。\n\n针对上述挑战，本文提出面向零样本 GAD 的 EvoFG-MoE 框架，集成进化路由特征生成（EvoFG）模块。为增强 MoE 路由，EvoFG 通过基于大语言模型的生成器与 Shapley 引导的评估机制，迭代地构建并筛选信息丰富的结构特征。同时，我们设计了记忆增强路由，结合不变学习损失，在分布变迁中捕获可迁移的路由模式。在六个基准数据集上的广泛实验表明，EvoFG 始终优于当前最佳基线，并表现出强大、稳定的零样本 GAD 性能。"
    },
    {
        "title": "Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation",
        "summary": "Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.",
        "entry_id": "http://arxiv.org/abs/2602.11605v1",
        "pub_date": "2026-02-12",
        "translated_summary": "摘要  \n现有生成式推荐（GenRec）模型通常借助全注意力机制来刻画用户行为，但在终身序列场景下，过长的交互历史既带来高昂的计算开销，又导致随机交互噪声的累积，阻碍系统规模化应用。针对这两重难题，本文提出 Rec2PM，框架通过在序列级别将漫长的用户交互历史压缩为少量“偏好记忆”记号（Preference Memory tokens）来解决。与需在序列上单步串行进度的传统循环式方法不同，Rec2PM 引入一种自指 Teacher-Forcing 策略：首先从全局视角一次性提炼参考记忆，再将其作为监督目标并行化地更新循环组件，从而使训练阶段具备完全并行能力，同时在推理阶段仍能支持迭代式增量更新。此外，由于记忆以嵌入形式存储而非占用巨大的 KV-cache，Rec2PM 在存储效率方面实现大幅压缩。在多个大规模基准上的实验表明，Rec2PM 不仅显著降低推测延迟与内存占用，其准确性也超越端到端的全序列模型。进一步分析发现，偏好记忆充当了去噪信息瓶颈，有效过滤交互噪声，保留并强化用户的长期稳定兴趣。"
    },
    {
        "title": "Analytical Search",
        "summary": "Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.\n  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.",
        "entry_id": "http://arxiv.org/abs/2602.11581v1",
        "pub_date": "2026-02-12",
        "translated_summary": "分析型信息需求（如趋势分析与因果影响评估）广泛存在于法律、金融、科学等诸多领域。然而，现有信息检索范式——无论是以相关性为导向的文档排序，还是借助大语言模型（LLM）的检索增强生成（RAG）——在大规模语料层面都难以为此类任务提供端到端支撑。它们要么聚焦于信息查找而非全链路问题解决，要么简单地把一切视为“问答”，且对推理过程、证据利用和可验证性的控制极为有限，难以满足效用概念多样、问责要求极高的分析型查询。\n\n为此，本文提出将“分析搜索”视为一种崭新且独立的搜索范式，以回应上述分析型信息需求。分析搜索将搜索重构思为“受证据约束、面向流程”的分析工作流：系统显式建模分析意图，检索证据并融合，再通过结构化多步推理输出可验证结论。我们在概念上将分析搜索与现有范式进行鲜明对比，提出一个涵盖查询理解、召回导向检索、推理式融合及自适应验证的统一系统框架，并探讨构建分析搜索引擎的潜在研究方向。本文旨在凸显分析搜索的重要概念意义与实际价值，呼吁业界携手打造面向分析型信息需求的下一代搜索引擎。"
    },
    {
        "title": "LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling",
        "summary": "Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict \"Latency Wall\", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.",
        "entry_id": "http://arxiv.org/abs/2602.11562v1",
        "pub_date": "2026-02-12",
        "translated_summary": "摘要：  \n在现代推荐系统里，对超长用户行为序列建模对于捕捉演化且终身的兴趣至关重要。然而，若要在实时工业环境中部署此类模型，将面临严格的“延迟墙”，其受两大瓶颈制约：海量用户历史的高 I/O 检索延迟，以及标准注意力机制的二次计算复杂度。为突破这两大瓶颈，我们提出 LASER——一套端到端优化框架，已在小红书（RedNote）生产规模部署。该框架通过两项互补创新解决问题：\n\n1. 系统效率：我们设计了 SeqVault——面向长用户历史的统一模式感知服务架构。借助混合 DRAM-SSD 索引策略，SeqVault 将检索延迟降低 50%，CPU 使用率降低 75%，实现毫秒级访问完整的实时与生命周期用户历史。\n\n2. 算法效率：我们提出“分段目标注意力”（STA）机制以解决计算瓶颈。受用户兴趣本质稀疏的启发，STA 采用基于 sigmoid 的门控策略，相当于“静默机制”，过滤噪声项；随后，轻量级“全局堆叠目标注意力”（GSTA）模块在压缩后片段上进一步捕获片段间依赖关系，却不会带来高昂计算开销。该设计有效实现序列压缩，在降低长序列建模复杂度的同时保留关键信号。\n\n大量离线实验表明，LASER 始终优于当前最强基线。在覆盖 1 亿日活用户的大规模线上 A/B 测试中，LASER 实现了 ADVV 提升 2.36%，营收提升 2.08%，展示了其卓越的可扩展性与显著的商业价值。"
    },
    {
        "title": "KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance",
        "summary": "E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.",
        "entry_id": "http://arxiv.org/abs/2602.11518v1",
        "pub_date": "2026-02-12",
        "translated_summary": "电商搜索作为连接用户需求与海量商品库存的核心界面，在我们的日常生活中发挥着至关重要的作用。然而，在实际应用中，它仍面临诸多挑战，如查询高度模糊、商品文本语义贫乏且充满噪声，以及用户偏好多样化等，这些因素共同导致难以精准捕捉用户意图和细粒度的商品语义。\n\n近年来，大型语言模型（LLMs）在语义表示与上下文推理方面的显著突破，为克服上述挑战提供了新的契机。然而，现有的电商搜索数据集仍存在显著局限：查询常通过启发式方法构造，冷启动用户和长尾商品被滤除，查询和商品文本均被匿名化，且多数数据集仅覆盖搜索流程中的单一阶段。这些问题共同阻碍了基于 LLM 的电商搜索研究。\n\n为应对这些挑战，我们构建并发布了 KuaiSearch。据我们所知，这是目前规模最大的电商搜索数据集。该数据集基于 Kuaishou 平台真实用户搜索交互构建，保留了原始用户查询和自然语言商品文本，涵盖了冷启动用户与长尾商品，并系统地覆盖了搜索流程中的召回、排序和相关性评估三个关键阶段。\n\n我们从产品、用户和查询等多个维度对 KuaiSearch 进行了深入分析，并在多项代表性搜索任务上建立了基准实验。实验结果表明，KuaiSearch 为现实电商搜索研究提供了坚实可靠的数据基础。"
    },
    {
        "title": "From Noise to Order: Learning to Rank via Denoising Diffusion",
        "summary": "In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.",
        "entry_id": "http://arxiv.org/abs/2602.11453v1",
        "pub_date": "2026-02-12",
        "translated_summary": "在信息检索（IR）领域，传统的“学习排序”（LTR）方法几乎都局限于判别式机器学习，仅对给定查询-文档对特征后“文档是否相关”的条件概率进行建模。与这一思路不同，本文提出一种基于去噪扩散模型的深度生成式LTR框架，转而刻画特征向量与相关性标签的完整联合概率分布。在判别式设置下，过度参数化的排序模型可能以多种方式拟合训练数据；据此我们猜想，在生成式设置下，能够解释整个数据分布的候选解会更稳健。受这一直觉驱使，本文提出 DiffusionRank：把已有的去噪扩散型表格数据生成模型 TabDiff 扩展到 LTR 场景，构造出与经典点式和配对判别式目标的生成式对应体。实验结果显示，DiffusionRank 相比其判别式基线取得了显著提升。我们的工作为此后如何持续利用深度生成式方法（如扩散模型）在 IR 中推进排序学习，开辟了广阔的研究空间。"
    },
    {
        "title": "Filtered Approximate Nearest Neighbor Search in Vector Databases: System Design and Performance Analysis",
        "summary": "Retrieval-Augmented Generation (RAG) applications increasingly rely on Filtered Approximate Nearest Neighbor Search (FANNS) to combine semantic retrieval with metadata constraints. While algorithmic innovations for FANNS have been proposed, there remains a lack of understanding regarding how generic filtering strategies perform within Vector Databases. In this work, we systematize the taxonomy of filtering strategies and evaluate their integration into FAISS, Milvus, and pgvector. To provide a robust benchmarking framework, we introduce a new relational dataset, \\textit{MoReVec}, consisting of two tables, featuring 768-dimensional text embeddings and a rich schema of metadata attributes. We further propose the \\textit{Global-Local Selectivity (GLS)} correlation metric to quantify the relationship between filters and query vectors.\n  Our experiments reveal that algorithmic adaptations within the engine often override raw index performance. Specifically, we find that: (1) \\textit{Milvus} achieves superior recall stability through hybrid approximate/exact execution; (2) \\textit{pgvector}'s cost-based query optimizer frequently selects suboptimal execution plans, favoring approximate index scans even when exact sequential scans would yield perfect recall at comparable latency; and (3) partition-based indexes (IVFFlat) outperform graph-based indexes (HNSW) for low-selectivity queries. To facilitate this analysis, we extend the widely-used \\textit{ANN-Benchmarks} to support filtered vector search and make it available online. Finally, we synthesize our findings into a set of practical guidelines for selecting index types and configuring query optimizers for hybrid search workloads.",
        "entry_id": "http://arxiv.org/abs/2602.11443v1",
        "pub_date": "2026-02-11",
        "translated_summary": "基于检索增强的生成（RAG）应用愈发依赖于“带过滤条件的近似最近邻搜索（FANNS）”，以实现语义检索与元数据约束的结合。虽然已有不少针对 FANNS 的算法创新，但在向量数据库中，通用过滤策略的实际表现仍缺乏系统认知。本工作对过滤策略进行了系统分类，并在 FAISS、Milvus 与 pgvector 上评估其集成效果。为此，我们发布了新的关系型基准数据集 MoReVec：它包含两个表，拥有 768 维文本向量以及丰富的元数据属性。为量化过滤器与查询向量间的关联，我们提出 “全局-局部选择性（GLS）” 相关性指标。\n\n实验结果表明，引擎内部的算法适配往往掩盖了原始索引性能：（1） Milvus 通过近似/精确执行的混合策略获得更高、更稳定的召回率；（2） pgvector 的成本型查询优化器经常选择次优执行计划，对低选择性查询仍偏爱近似索引扫描，而顺序扫描可在相近延迟下实现完美召回；（3） 对于低选择性查询，基于分区的索引 IVFFlat 性能优于基于图的 HNSW。为支撑本研究，我们将主流基准 ANN-Benchmarks 扩展至支持过滤向量搜索并开源发布。最终，我们将发现凝练为一套实用指南，帮助针对混合查询负载选择索引类型并配置查询优化器。"
    },
    {
        "title": "Diffusion-Pretrained Dense and Contextual Embeddings",
        "summary": "In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.",
        "entry_id": "http://arxiv.org/abs/2602.11151v1",
        "pub_date": "2026-02-11",
        "translated_summary": "本报告提出 pplx-embed——一系列面向网络规模检索的多语言嵌入模型，其核心在扩散预训练语言模型骨干之上采用多阶段对比学习。凭借基于扩散的预训练所带来的双向注意力机制，模型可在文档段落中捕获全面的双向上下文，从而支持均值池化和延迟分块策略，在超长文本中更好地保持全局语境。我们开源两种模型：用于标准检索的 pplx-embed-v1，以及可将整篇文档全局上下文注入段落表示的语境化嵌入模型 pplx-embed-context-v1。\n\n在公开基准测试中，pplx-embed-v1 在 MTEB（多语言 v2）、MTEB（代码）、MIRACL、BERGEN 和 ToolRet 等检索任务上均达到当前领先水平；而 pplx-embed-context-v1 则在 ConTEB 基准中刷新多项记录。此外，在我们聚焦数千万级实际文档的大规模搜索场景的内部评测套件中，pplx-embed-v1 亦展现出强劲表现。这些结果验证了该模型族在生产环境中兼顾检索质量与效率、可真正落地的有效性。"
    },
    {
        "title": "MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation",
        "summary": "Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec's architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.",
        "entry_id": "http://arxiv.org/abs/2602.11062v1",
        "pub_date": "2026-02-11",
        "translated_summary": "图神经网络（GNN）通过在复杂用户-商品交互建模上发挥卓越效能，已彻底改变了推荐系统；然而，数据稀疏性及商品冷启动问题严重影响其性能，尤其是对那些几乎没有交互记录的新商品。尽管多模态内容被视为解决该问题的有力手段，现有方法在稀疏数据中所含噪声与纠缠的影响下，仍只能生成次优的新商品表征。针对这一缺陷，我们将多模态推荐任务转化为离散语义令牌化问题，提出“面向冷启动推荐的稀疏正则化多模态令牌化”（MoToRec）框架。该框架以“稀疏正则化残差量化变分自编码器”（RQ-VAE）为核心，可生成由离散、可解释 token 构成的组合式语义编码，从而实现解耦表征。MoToRec 通过三大协同组件进一步增强了整体架构：（1）引入稀疏正则 RQ-VAE，强制表征解耦；（2）设计全新的自适应稀有度放大机制，在训练中优先聚焦冷启商品；（3）构建层次化多源图编码器，使协同信号与多模态信息实现鲁棒融合。在三个大规模数据集上的大量实验表明，MoToRec 在整体与冷启动场景下均显著优于现有最佳方法。本研究验证：离散令牌化为缓解长期的冷启动难题提供了一种有效且可扩展的新路径。"
    },
    {
        "title": "GraphSeek: Next-Generation Graph Analytics with LLMs",
        "summary": "Graphs are foundational across domains but remain hard to use without deep expertise. LLMs promise accessible natural language (NL) graph analytics, yet they fail to process industry-scale property graphs effectively and efficiently: such datasets are large, highly heterogeneous, structurally complex, and evolve dynamically. To address this, we devise a novel abstraction for complex multi-query analytics over such graphs. Its key idea is to replace brittle generation of graph queries directly from NL with planning over a Semantic Catalog that describes both the graph schema and the graph operations. Concretely, this induces a clean separation between a Semantic Plane for LLM planning and broader reasoning, and an Execution Plane for deterministic, database-grade query execution over the full dataset and tool implementations. This design yields substantial gains in both token efficiency and task effectiveness even with small-context LLMs. We use this abstraction as the basis of the first LLM-enhanced graph analytics framework called GraphSeek. GraphSeek achieves substantially higher success rates (e.g., 86% over enhanced LangChain) and points toward the next generation of affordable and accessible graph analytics that unify LLM reasoning with database-grade execution over large and complex property graphs.",
        "entry_id": "http://arxiv.org/abs/2602.11052v1",
        "pub_date": "2026-02-11",
        "translated_summary": "图作为跨领域基础数据结构，却一直需要深厚专业知识才能驾驭。大语言模型（LLM）让人们看到了用自然语言（NL）轻松分析图数据的可能，但面对生产级属性图却力不从心：这些数据规模庞大、结构高度异构、实体关系复杂，且不断更新。为解决此难题，本文提出了一种新颖的抽象框架，用于在大型、复杂且不断演化的图数据集上执行灵活的多轮查询分析。\n\n该框架的核心思想不再让 LLM 直接“生写”脆弱的图查询，而是在“语义目录”（Semantic Catalog）之上进行规划。这个目录同时描述了图的结构模式和可操作的原语。具体而言，我们将系统清晰地划分为两层：\n• 语义层（Semantic Plane）：由 LLM 负责高层规划与逻辑推理；  \n• 执行层（Execution Plane）：以数据库级的确定性方式，在完整数据上执行生成下来的查询并调用各类工具。\n\n这种分离设计即便在小上下文窗口的 LLM 上，也能显著降低 token 消耗，并在任务成功率上取得质的飞跃。基于该抽象，我们实现了首个 LLM 增强的图分析框架 GraphSeek。在基准测试中，GraphSeek 的成功率远超强化版 LangChain（86% vs. 54%），开启了将大语言模型推理与数据库级执行无缝融合的下一代经济、普适图分析时代。"
    },
    {
        "title": "MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan",
        "summary": "Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data.",
        "entry_id": "http://arxiv.org/abs/2602.11235v1",
        "pub_date": "2026-02-11",
        "translated_summary": "工业级推荐系统常涵盖多个场景，但现有的跨域（CDR）与多场景（MSR）方法通常依赖高昂的资源与严苛的输入对齐，制约了可扩展性。本文提出基于 Transformer 的 MTFM（Meituan Foundation Model for Recommendation）以解决上述痛点。该方法不再预先对齐输入，而是将来自不同域的数据转化为异构 Token，以无对齐方式获取多场景知识。为了提高效率，我们首先构建“多场景用户级样本聚合”机制，显著减少训练实例总量，从而提升训练吞吐；随后，引入分组查询注意力（Grouped-Query Attention）和定制混合目标注意力（Hybrid Target Attention），大幅降低显存占用与计算开销。此外，系统级层面的优化（如算子融合、消除 CPU-GPU 阻塞）进一步加速了训练与推理。离线与在线实验均验证了 MTFM 的有效性，并表明通过扩充模型容量与多场景数据的规模可带来显著性能提升。"
    },
    {
        "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
        "summary": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
        "entry_id": "http://arxiv.org/abs/2602.12278v1",
        "pub_date": "2026-02-12",
        "translated_summary": "检索增强生成（RAG）已被广泛应用于协助大语言模型处理涉及长文档的任务。然而，现有的检索模型并非专为长文档设计，无法有效应对长文档检索所面临的三大核心挑战：上下文感知性、因果依赖性以及检索范围界定。本文提出 AttentionRetriever，一种新型长文档检索模型，利用注意力机制与基于实体的检索来构建上下文感知的长文档嵌入，并动态确定检索范围。通过大规模实验，我们发现 AttentionRetriever 在长文档检索基准上显著超越现有模型，同时仍保持与稠密检索模型相当的效率。"
    },
    {
        "title": "SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization",
        "summary": "Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.",
        "entry_id": "http://arxiv.org/abs/2602.12187v1",
        "pub_date": "2026-02-12",
        "translated_summary": "检索增强生成引擎（SAGE）已成为信息获取的新范式，它将大规模网络检索与生成能力融为一体，为用户提供综合答案。这一转变彻底改变了网络内容在线曝光的方式，催生了“检索增强式生成引擎优化”（SAGEO）——即通过优化网页文档来提升其在 AI 生成回答中的可见度。\n\n尽管关注度与日俱增，目前尚无评估环境能够全面支持 SAGEO 的研究。现有基准大多无法端到端地评估优化策略的效果：它们只能在预先选定的候选文档集上操作，完全忽略了生成之前的检索与重排序环节。此外，这些基准往往会丢弃真实网页中的结构化信息（例如 schema 标记），而搜索引擎在实践中恰恰依赖这些丰富信号来排序和展现内容。\n\n为填补这些空白，我们推出了 SAGEO Arena——首个真实且可复现的分阶段 SAGEO 分析环境。它同时关注面向搜索的优化（SEO）和面向生成的优化（GEO）。为此，我们在包含丰富结构化信息的大规模网页语料上，搭建了完整版生成式搜索管道。\n\n实验结果显示：在真实设置下，现有方法大多难以落地，甚至会在检索和重排序阶段带来性能下降。不过，我们发现引入结构化信息可有效缓解这些缺陷；真正有效的 SAGEO 必须针对不同管道阶段量身定制优化策略。总而言之，我们的基准为超越简化设定、实现真实场景的 SAGEO 评估与优化铺平了道路。"
    },
    {
        "title": "Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset",
        "summary": "Personalized book recommendation in Bangla literature has been constrained by the lack of structured, large-scale, and publicly available datasets. This work introduces RokomariBG, a large-scale, multi-entity heterogeneous book graph dataset designed to support research on personalized recommendation in a low-resource language setting. The dataset comprises 127,302 books, 63,723 users, 16,601 authors, 1,515 categories, 2,757 publishers, and 209,602 reviews, connected through eight relation types and organized as a comprehensive knowledge graph.\n  To demonstrate the utility of the dataset, we provide a systematic benchmarking study on the Top-N recommendation task, evaluating a diverse set of representative recommendation models, including classical collaborative filtering methods, matrix factorization models, content-based approaches, graph neural networks, a hybrid matrix factorization model with side information, and a neural two-tower retrieval architecture. The benchmarking results highlight the importance of leveraging multi-relational structure and textual side information, with neural retrieval models achieving the strongest performance (NDCG@10 = 0.204). Overall, this work establishes a foundational benchmark and a publicly available resource for Bangla book recommendation research, enabling reproducible evaluation and future studies on recommendation in low-resource cultural domains. The dataset and code are publicly available at https://github.com/backlashblitz/Bangla-Book-Recommendation-Dataset",
        "entry_id": "http://arxiv.org/abs/2602.12129v1",
        "pub_date": "2026-02-12",
        "translated_summary": "This paper introduces BanglaLitRec-the first large-scale, multi-entity heterogeneous book graph for personalized recommendation in the low-resource Bengali domain.  \nBanglaLitRec contains：  \n\n• 127,302 本书  \n• 63,723 位用户  \n• 16,601 位作者  \n• 551 个类别  \n• 209,602 条评价，并通过 8 类关系互联，形成知识图谱。  \n\n为系统评估其有效性，作者在 Top-N 推荐任务上对多种代表性模型进行了基准测试，包括：  \n传统协同过滤、矩阵分解、基于内容的方法、图神经网络、混合矩阵分解+辅助信息，以及神经双塔检索架构。  \n结果显示，充分利用多关系结构和文本侧信息可获得最佳性能（Neural 双塔在 NDCG@10 = 0.204）。  \n\n该数据集与代码已在 GitHub 开源：https://github.com/backslash-bangla/Bangla-Book-Recommendation-Dataset，为低资源文化场景下的可重复推荐研究提供了公共基准。"
    }
]